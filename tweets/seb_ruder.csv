id,created_at,text
1259979192130842600,Mon May 11 22:50:23 +0000 2020,RT @CLTLVU: PhD Position Computational Linguistics on Diversifying News Recommendation. @CLTLVU @VU_FGW @antske  @suzan @vanatteveldt @nhel‚Ä¶
1259910296220680200,Mon May 11 18:16:37 +0000 2020,RT @barbara_plank: Are you looking for a faculty position? ITU is hiring - several assistant and associate professor positions are availabl‚Ä¶
1258506241132748800,Thu May 07 21:17:24 +0000 2020,RT @paperswithcode: üéâ A huge update to Papers with Code: now with 2500+ leaderboards and 20,000+ results. Plus, results now link directly t‚Ä¶
1258378014859157500,Thu May 07 12:47:52 +0000 2020,@dennybritz That's one reason I really like the theme track at this year's ACL conference: It encourages people to‚Ä¶ https://t.co/jsIcw0f3oV
1256345241898156000,Fri May 01 22:10:22 +0000 2020,RT @PfeiffJo: I'm really excited to announce our two new papers: ‚ÄúAdapterFusion: Non-Destructive Task Composition for Transfer Learning‚Äù an‚Ä¶
1256344957943722000,Fri May 01 22:09:14 +0000 2020,@karmacondon @PfeiffJo @licwu Depending how much overlap in terms of symbols there is with the existing multilingua‚Ä¶ https://t.co/IwgauDUr5F
1256343482853187600,Fri May 01 22:03:22 +0000 2020,@PfeiffJo @licwu If we apply this framework to XLM-R, a state-of-the-art multilingual model we achieve strong gains‚Ä¶ https://t.co/gcggdFprHk
1256343022381469700,Fri May 01 22:01:32 +0000 2020,@PfeiffJo @licwu Using adapters‚Äîsmall bottleneck layers‚Äîwe can learn task and language-specific parameters to add a‚Ä¶ https://t.co/Ewb72K3TQx
1256342421966934000,Fri May 01 21:59:09 +0000 2020,@PfeiffJo @licwu Pre-trained multilingual models are great but they can only cover so many languages. For instance,‚Ä¶ https://t.co/Zvh7piSwrT
1256342290483810300,Fri May 01 21:58:38 +0000 2020,I'm really excited about our new paper with @PfeiffJo, @licwu &amp; IGurevych.  We propose MAD-X, a new adapter-based f‚Ä¶ https://t.co/5b4Csu9aZp
1256235329062940700,Fri May 01 14:53:36 +0000 2020,RT @artetxem: Check out our new position paper: "A Call for More Rigor in Unsupervised Cross-lingual Learning" (w/ @seb_ruder, @DaniYogatam‚Ä¶
1255111709834043400,Tue Apr 28 12:28:45 +0000 2020,@JerodMeagher @naacl Nice! Hope you're finding it useful!
1255100806778499000,Tue Apr 28 11:45:25 +0000 2020,RT @DeepMind: This Thursday at 5pm BST, our researchers will be hosting the #ICLR2020 social for ‚ÄòTopics in Language Research'. We hope to‚Ä¶
1254708659172638700,Mon Apr 27 09:47:10 +0000 2020,@srush_nlp We can find smaller clusters about text generation, reasoning, machine translation, and word embeddings.
1254708321405284400,Mon Apr 27 09:45:49 +0000 2020,I'm a big fan of the visualization of #ICLR2020 papers by @srush_nlp. https://t.co/qwrf8pqJGX  Searching for "nlp",‚Ä¶ https://t.co/T86liZ8tKr
1254699712835444700,Mon Apr 27 09:11:37 +0000 2020,RT @daniel_hers: I am looking for a PhD candidate! Interested in linguistic meaning representations for #NLProc? Contact me and apply here:‚Ä¶
1254475296117489700,Sun Apr 26 18:19:52 +0000 2020,RT @aidanematzadeh: Our cognition (and language capacities) are highly dependent on our memory. AI systems also require similar memory func‚Ä¶
1254400667910844400,Sun Apr 26 13:23:19 +0000 2020,If you're at #ICLR2020, consider attending the @black_in_ai meetup on research and entrepreneurship opportunities i‚Ä¶ https://t.co/91fDD1Q3Ok
1254109669670428700,Sat Apr 25 18:07:00 +0000 2020,RT @996roma: this paper on memory in intelligent systems from @aidanematzadeh, @seb_ruder, @DaniYogatama at #BAICS2020 was a fascinating re‚Ä¶
1254074248349565000,Sat Apr 25 15:46:15 +0000 2020,RT @alienelf: 8/ Would also like to announce our last minute emergency panel who will be joining us at #AfricaNLP2020 tomorrow! ‚ú®‚ô•Ô∏èüëèüèæ  - @n‚Ä¶
1254043615254458400,Sat Apr 25 13:44:31 +0000 2020,RT @alienelf: 1/ Tomorrow @iclr_conf begins. Those of you attending please do join us at TOMORROW the AfricaNLP üåç workshop, where we'll be‚Ä¶
1254008875734401000,Sat Apr 25 11:26:28 +0000 2020,RT @aggielaz: One thing I'm really looking forward during conferences is to randomly meet other researchers when queuing for the coffee bre‚Ä¶
1253690754280624000,Fri Apr 24 14:22:22 +0000 2020,If you didn't originally plan to attend #ICLR2020, potentially consider attending the virtual conference now. You w‚Ä¶ https://t.co/lNc1mGF2Xj
1253689680345825300,Fri Apr 24 14:18:06 +0000 2020,You can find all the #ICLR2020 socials here: https://t.co/zOk7OS559W Lots of fun ones, from using generative ML for‚Ä¶ https://t.co/UtMRGcp0iJ
1253688972758388700,Fri Apr 24 14:15:18 +0000 2020,My colleagues and I are organizing an #ICLR2020 social on 'Topics in Language Research' on 30/4 at 5pm BST. Please‚Ä¶ https://t.co/veZ6tLqHPR
1253254670736384000,Thu Apr 23 09:29:32 +0000 2020,@bayethiernodiop @baamtusarl @iclr_conf @galsenai Congrats! Looking forward to this! üéâ
1252623945821573000,Tue Apr 21 15:43:15 +0000 2020,@laure_delisle @Caltech Huge congrats, Laure!! üëèüéâ
1252529275426799600,Tue Apr 21 09:27:04 +0000 2020,RT @alienelf: I'm humbled &amp; excited to be presenting the progress of @MasakhaneMt at @iclr_conf  AfricaNLP workshop next week ‚ô•Ô∏è‚ú®   üåç Machi‚Ä¶
1252263611675881500,Mon Apr 20 15:51:25 +0000 2020,@anas_ant @VolgenauSchool Congrats! üéâ
1252197242590711800,Mon Apr 20 11:27:41 +0000 2020,@ionandrou @Thiagogm Thanks for the pointer, Ion! :)
1251907341441794000,Sun Apr 19 16:15:44 +0000 2020,@Thiagogm Thanks for the pointer! :)
1251907226949824500,Sun Apr 19 16:15:16 +0000 2020,@jganseman @ml6team Glad you enjoyed it. As far as I'm aware, the role of punctuation in sentence embeddings has no‚Ä¶ https://t.co/ObR8GsGeNm
1251904411846291500,Sun Apr 19 16:04:05 +0000 2020,@carbadol @oeg_upm Thanks for the pointer, Carlos! :)
1251093556510548000,Fri Apr 17 10:22:02 +0000 2020,@joshlk100 Thanks! I'm not sure I'll have enough time to update the post. I hope someone else will write a comprehensive up-to-date one. :)
1251090730946306000,Fri Apr 17 10:10:49 +0000 2020,@adnancagri ‚ò∫Ô∏è
1250414924402049000,Wed Apr 15 13:25:24 +0000 2020,RT @artetxem: We have seen impressive progress on unsupervised MT under ideal conditions (large high-quality corpora, similar domains, rela‚Ä¶
1250348187560235000,Wed Apr 15 09:00:12 +0000 2020,New special edition of NLP News on how NLP and ML can be used to help with the COVID-19 pandemic üò∑ https://t.co/ZA3Pdr3G3Z
1250089235866140700,Tue Apr 14 15:51:14 +0000 2020,@tallinzen @NYUDataScience @nyuling Congrats, Tal! üéâ
1250089029057613800,Tue Apr 14 15:50:24 +0000 2020,@LeonDerczynski @licwu Those studies have mostly focused on related languages AFAIK. I'd expect contextuality to he‚Ä¶ https://t.co/ZWRXRlCvHi
1250088218499977200,Tue Apr 14 15:47:11 +0000 2020,@LeonDerczynski @licwu Based on recent papers (https://t.co/n4imyj9wJK, https://t.co/RGnkHK3e5v) it seems that cont‚Ä¶ https://t.co/pn9ittDeaI
1250070461427396600,Tue Apr 14 14:36:37 +0000 2020,RT @anas_ant: @bittlingmayer Another factor that we didn't study is the quality of the original embeddings due to the size of the available‚Ä¶
1250059977307426800,Tue Apr 14 13:54:58 +0000 2020,@boknilev Agreed. That's on my wish list / todo list. :)
1250023881307553800,Tue Apr 14 11:31:32 +0000 2020,@EdwardDixon3 Thanks for the pointer! ‚ò∫Ô∏è
1250006465282179000,Tue Apr 14 10:22:19 +0000 2020,@boknilev For the other datasets, the difference could be larger due to differences in the data (e.g. see the human‚Ä¶ https://t.co/BakfQyAXwq
1250005552589148200,Tue Apr 14 10:18:42 +0000 2020,@boknilev Yes. For all but TyDiQA-GoldP, human performance is based on English. That comes with the obvious caveat‚Ä¶ https://t.co/85RDgu0YKk
1249975680445698000,Tue Apr 14 08:20:00 +0000 2020,RT @JeffDean: "The languages in XTREME are selected to maximize language diversity, coverage in existing tasks, &amp; availability of training‚Ä¶
1249845093223334000,Mon Apr 13 23:41:05 +0000 2020,@nlpnoah I'm sorry you did not find our effort helpful, Noah. Evaluation in diverse settings (particularly low-reso‚Ä¶ https://t.co/ywVtldLjNX
1249787752050294800,Mon Apr 13 19:53:14 +0000 2020,RT @WWRob: Hoping this dataset becomes one of the most popular #NLProc benchmarks! 40 languages from 13 or 14 families (depending on your t‚Ä¶
1249784393100669000,Mon Apr 13 19:39:53 +0000 2020,@gena_d Argh. The last letter of the website got lost during copy-pasting. üòÖ Here's the correct link: https://t.co/S8RdKmhCsE
1249780316140638200,Mon Apr 13 19:23:41 +0000 2020,This work would have not been possible without the contributions of many amazing people including @JunjieHu12‚Ä¶ https://t.co/zHDpwIv0I6
1249780143582785500,Mon Apr 13 19:23:00 +0000 2020,Overall, we find that there is a large gap to English and human performance and a lot of potential for improvement,‚Ä¶ https://t.co/i7pK74hUjM
1249780015958577200,Mon Apr 13 19:22:30 +0000 2020,XTREME evaluates models on their capability to do zero-shot cross-lingual transfer when fine-tuned on English. In t‚Ä¶ https://t.co/QjdMUq0nB6
1249779748961767400,Mon Apr 13 19:21:26 +0000 2020,I'm excited to announce XTREME, a new benchmark that covers 9 tasks and 40 typologically diverse languages.  Paper:‚Ä¶ https://t.co/5b34tRWZGv
1249775331734626300,Mon Apr 13 19:03:53 +0000 2020,RT @GoogleAI: Announcing XTREME, a new #NaturalLanguageProcessing benchmark for cross-lingual generalization, which covers 40 typologically‚Ä¶
1248982730005786600,Sat Apr 11 14:34:22 +0000 2020,RT @karlmoritz: One thing I am looking at is how to reduce admin pain in all of our lives. If you spare four minutes, I would love your inp‚Ä¶
1248932885572325400,Sat Apr 11 11:16:18 +0000 2020,Overall, we come to the conclusion that besides improving alignment algorithms for distant languages, we should als‚Ä¶ https://t.co/UZGBC4s6TX
1248932775673188400,Sat Apr 11 11:15:52 +0000 2020,@licwu We challenge this assumption and through controlled experiments in typologically diverse languages, we find‚Ä¶ https://t.co/Crvvwx00BL
1248932641451229200,Sat Apr 11 11:15:20 +0000 2020,Are All Good Word Vector Spaces Isomorphic?  That's the question we ask in recent work w/ @licwu &amp; Anders S√∏gaard.‚Ä¶ https://t.co/520gRTurcv
1248583968808714200,Fri Apr 10 12:09:50 +0000 2020,@JasonPunyon @jeremyphoward @fastdotai @StackOverflow üòçThis is super cool! Thanks a lot for writing about this! It'‚Ä¶ https://t.co/82ukbQ4MJB
1248583291860594700,Fri Apr 10 12:07:08 +0000 2020,RT @JasonPunyon: Just Blogged "The Unfriendly Robot: Automatically flagging unwelcoming comments" https://t.co/ayhpjDQQlk
1248305751593975800,Thu Apr 09 17:44:18 +0000 2020,@rasbt @stataholic Stanford's CS224d gives a nice overview: https://t.co/ZCISHucNuT Also definitely read @ch402's b‚Ä¶ https://t.co/AvW5FNhXqO
1247496825432653800,Tue Apr 07 12:09:55 +0000 2020,RT @annalkorhonen: Come and join us at @CambridgeLTL! We have a fully funded PhD position in ¬†#NLProc in the exciting area of multilingual‚Ä¶
1246357371615613000,Sat Apr 04 08:42:08 +0000 2020,RT @xrysoflhs: Join my @turinginst AI fellowship project and do an PhD in Longitudinal #NLProc methods from user generated content with app‚Ä¶
1246014074183463000,Fri Apr 03 09:57:59 +0000 2020,RT @CamachoCollados: Given the current situation, @tpilehvar and I have decided to openly release the first draft of our book ‚ÄúEmbeddings i‚Ä¶
1245417983553540000,Wed Apr 01 18:29:20 +0000 2020,RT @WWRob: Are you a Data Scientist that wants to help respond to #COVID19? Here are 5 ways that you can help and 5 actions to avoid: https‚Ä¶
1244972385046061000,Tue Mar 31 12:58:41 +0000 2020,RT @GretaFranzini: PhD position @univgroningen on "Understanding Ancient Greek gods through #NLProc". Application deadline: 21/04/2020; Pro‚Ä¶
1244605478858109000,Mon Mar 30 12:40:44 +0000 2020,RT @Quantum_Stat: [UPDATE] Big Bad NLP Database  We've updated the database with 38 new datasets! Thanks again for contributing: @pasini_t,‚Ä¶
1243847534038179800,Sat Mar 28 10:28:56 +0000 2020,@omarsar0 Thanks for writing these summaries, @omarsar0! üôè
1243611495306494000,Fri Mar 27 18:51:00 +0000 2020,NLP Research Highlights ‚Äî Issue #1  An extensive summary of recent NLP papers related to language models, transfer‚Ä¶ https://t.co/WnvKvM7gyw
1243572978375368700,Fri Mar 27 16:17:57 +0000 2020,@xabi_soto Great! Please share if you have anything written up or a demo available.
1243556894809239600,Fri Mar 27 15:14:02 +0000 2020,RT @alsuhr: Organizing a virtual NLP house party with Daphne Ippolito and @RishiBommasani this Saturday evening (EDT). We‚Äôll have themed Zo‚Ä¶
1243528159951237000,Fri Mar 27 13:19:51 +0000 2020,RT @srush_nlp: Looking for a community volunteer to help with web dev for Virtual ICLR. Pretty fun, collaborative, and indoor project to he‚Ä¶
1243513002483159000,Fri Mar 27 12:19:37 +0000 2020,RT @alienelf: Our first community publication for @MasakhaneMt is finally outüíúüå∫  "Masakhane - Machine Translation for Africa"   üåç‚ú®üå∫
1243477938630164500,Fri Mar 27 10:00:17 +0000 2020,New NLP Newsletter: COVID-19, Hutter Prize, Compression = AGI?, BERT, Green AI https://t.co/qjVPBnd4G6
1243316830401450000,Thu Mar 26 23:20:06 +0000 2020,RT @FelixHill84: Apologies for work-based stuff in the current situation - please RT if you may know anyone interested   We (DeepMind) are‚Ä¶
1243300328302403600,Thu Mar 26 22:14:32 +0000 2020,RT @sleepinyourhat: I had heard mumblings of this earlier, and I'm excited that it has come out: XTREME, Google/DeepMind/CMU's new benchmar‚Ä¶
1243230636342722600,Thu Mar 26 17:37:36 +0000 2020,@LeonDerczynski Yay! Congrats! üéâ
1243217537942331400,Thu Mar 26 16:45:33 +0000 2020,RT @gneubig: We are working with Translators Without Borders and several large tech companies/orgs to get trustworthy COVID19 information t‚Ä¶
1243207767491596300,Thu Mar 26 16:06:44 +0000 2020,@ThomasScialom Thanks for the comments, Thomas! I spent quite a bit of time doing hyper-parameter tuning for XLM an‚Ä¶ https://t.co/yF27pf0pWd
1242217752951128000,Mon Mar 23 22:32:46 +0000 2020,RT @sleepinyourhat: If you're a domain expert working on a short-term pro-bono project, and that project would benefit significantly from a‚Ä¶
1239177825292345300,Sun Mar 15 13:13:11 +0000 2020,@jethroksy Thanks for the note. Should be fixed now. The RSS feed should be available here: https://t.co/QJ35Hqoidy
1238496320786239500,Fri Mar 13 16:05:07 +0000 2020,@andrey_kurenkov @ethayarajh Thanks for reading, Andrey! :)
1238492307567100000,Fri Mar 13 15:49:10 +0000 2020,RT @BrandenChan3: @seb_ruder As @timomo1234 mentioned, we've been working on this overview of the SQuADs in other languages! It's a compari‚Ä¶
1238472607684583400,Fri Mar 13 14:30:54 +0000 2020,@dk21 @omarsar0 Sure! ‚ò∫Ô∏è Stay tuned for the next edition.
1237450430503944200,Tue Mar 10 18:49:08 +0000 2020,This is a super extensive article by @Tim_Dettmers on how to choose a grad school.  Its goal: to "help you make a b‚Ä¶ https://t.co/N7MtCxnEwe
1237405226434470000,Tue Mar 10 15:49:30 +0000 2020,RT @huggingface: üî•Manu Romero (@mrm8488) released three new models for **multilingual question answering**, fine-tuned on XQuAD, a cross-li‚Ä¶
1237387000799318000,Tue Mar 10 14:37:05 +0000 2020,The Coronavirus Drug Discovery Competition is a great example of creative ML approaches (RNN trained on molecules,‚Ä¶ https://t.co/XgPnoPKVCQ
1236952888527724500,Mon Mar 09 09:52:04 +0000 2020,RT @AriannaBisazza: We're looking for a talented PhD student to work on the Responsible Processing of Text Data, at the intersection of #NL‚Ä¶
1235985833687158800,Fri Mar 06 17:49:20 +0000 2020,RT @annabelle_cs: üôã‚Äç‚ôÄÔ∏èLOOKING FOR A JOBüôã‚Äç‚ôÄÔ∏è Former developer and software architect looking to apply her skills to supporting theoretical M‚Ä¶
1235925238048854000,Fri Mar 06 13:48:33 +0000 2020,@zaidalyafeai @omarsar0 Seconding what @omarsar0 said. I've included what I was aware of in my PhD thesis (‚Ä¶ https://t.co/Su0eOOnPcR
1235923942948339700,Fri Mar 06 13:43:25 +0000 2020,RT @Quantum_Stat: [UPDATE] Big Bad NLP Database   We've updated the database with 28 new datasets! Thanks again for contributing: @pasini_t‚Ä¶
1235556070464147500,Thu Mar 05 13:21:37 +0000 2020,RT @DeepMind: Today we're sharing structure predictions for six proteins associated with the virus that causes COVID-19, generated by the m‚Ä¶
1234942654854652000,Tue Mar 03 20:44:07 +0000 2020,RT @MilaNLProc: @seb_ruder Following up on this, here is an overview containing links and comparisons of language-specific BERT models! Cur‚Ä¶
1234922239218331600,Tue Mar 03 19:23:00 +0000 2020,Barbara is a great mentor! If you're interested in transfer learning (cross-lingual, cross-domain) and have thought‚Ä¶ https://t.co/wahAAeH7y4
1234912170225238000,Tue Mar 03 18:42:59 +0000 2020,RT @lspecia: We are hiring 2 full-time or part-time research associates or assistants to work on multimodal machine learning at Imperial Co‚Ä¶
1234911444908617700,Tue Mar 03 18:40:06 +0000 2020,@emnlp2020 The correct profile is: @nlp_evaluation
1234910411272708000,Tue Mar 03 18:36:00 +0000 2020,Good experimental methodology is soo important but often not does receive the attention it deserves.  If you are pa‚Ä¶ https://t.co/VvV8r0d1nf
1234870361893802000,Tue Mar 03 15:56:51 +0000 2020,RT @omarsar0: Inspired by works such as the NLP Progress by @seb_ruder  and @rbstojnic's Papers with Code, I would like to introduce NLP Pa‚Ä¶
1234839587392049200,Tue Mar 03 13:54:34 +0000 2020,@etiene_d @sandropezzelle @amoreupf Awesome!! Go for it, Etiene!! üëè
1234822926970556400,Tue Mar 03 12:48:22 +0000 2020,RT @sandropezzelle: Two weeks left to apply for a great #PhD position at @amoreupf in #Barcelona! Requisites: 1) interest in #reference com‚Ä¶
1234178756413137000,Sun Mar 01 18:08:40 +0000 2020,RT @igezeani: Black-in-AI @black_in_ai is organizing an event at the ICLR'20 conference. If you'd like to attend and/or wish to help to org‚Ä¶
1233415088582905900,Fri Feb 28 15:34:07 +0000 2020,@lucie_nlp This is super cool! Congrats, Lucie!! üòäüéâ
1233334084895694800,Fri Feb 28 10:12:14 +0000 2020,This is a *really* extensive repo containing ~380 BERT-related papers sorted into downstream tasks, modifications,‚Ä¶ https://t.co/3bdBC2OWnR
1233328249633112000,Fri Feb 28 09:49:03 +0000 2020,@dennybritz @hardmaru üéâ
1233037145234985000,Thu Feb 27 14:32:18 +0000 2020,RT @vlachos_nlp: Postdoc position in automated fact checking  https://t.co/FMxZz3Vlc5 working with me @cambridgenlp @Cambridge_CL in collab‚Ä¶
1232670408098435000,Wed Feb 26 14:15:01 +0000 2020,RT @kroscoo: New paper: https://t.co/gL3t3O03kd. @DaniYogatama and I investigate multi-task learning for natural language generation. We fi‚Ä¶
1232348296401866800,Tue Feb 25 16:55:04 +0000 2020,@_ryancallihan @SchmidhuberAI The transcription is actually even the officially approved one ‚ò∫Ô∏è‚Ä¶ https://t.co/EN0JdHIqfe
1232228806695497700,Tue Feb 25 09:00:15 +0000 2020,New NLP News: Accelerating science, memorizing vs learning to look things up, Schmidhuber's 2010s, Greek BERT, ARC,‚Ä¶ https://t.co/sVlxuMp7iQ
1231714423317946400,Sun Feb 23 22:56:17 +0000 2020,- üåçXQuAD (ours): https://t.co/DjfdZkQhvu TyDiQA: https://t.co/NlvNQjUEQe MLQA: https://t.co/BiyvCL5ocU  Are there a‚Ä¶ https://t.co/gARPN5FdOl
1231713840502657000,Sun Feb 23 22:53:58 +0000 2020,There have been many cool datasets recently, which will hopefully spur QA research in other languages: - üá®üá≥DuReader‚Ä¶ https://t.co/xlmeBVrCpk
1231215087999946800,Sat Feb 22 13:52:06 +0000 2020,RT @GhanaNLP: There is no ‚ÄúGoogle Translate‚Äù for any Ghanaian language (in the year 2020) Please help us start building one by providing a‚Ä¶
1230550160066801700,Thu Feb 20 17:49:55 +0000 2020,RT @SStymne: Come join us in Uppsala! I'm looking for a postdoc working on domain-sensitive cross-lingual parsing. https://t.co/DBdEcyIG1m
1230100396477231000,Wed Feb 19 12:02:43 +0000 2020,@kamwithk_ Thanks! Glad you liked it. :)
1229787341474222000,Tue Feb 18 15:18:45 +0000 2020,RT @natschluter: Come do your PhD in #NLProc with me in lovely Copenhagen.  Deadline March 10! https://t.co/EL18zSJFM9
1229772748966318000,Tue Feb 18 14:20:46 +0000 2020,RT @alienelf: ‚ú®‚ùóÔ∏èUrgently seeking reviewers for AfricaNLP workshop at @iclr_conf ‚ùóÔ∏è‚ú®  If you've published in NLP &amp; want to support diversit‚Ä¶
1229384142447665200,Mon Feb 17 12:36:35 +0000 2020,RT @omarsar0: üóûÔ∏èNLP Newsletter #4: PyTorch3D, DeepSpeed, T-NLG, Question Answering Benchmarks, Hydra,...üóûÔ∏è  Featuring: @mdlhx, @suzatweet,‚Ä¶
1229368669190393900,Mon Feb 17 11:35:05 +0000 2020,RT @AthensNlp: It's official! The 2nd AthNLP - Athens Natural Language Processing Summer School will take place from 26 August - 1 Septembe‚Ä¶
1228619866116104200,Sat Feb 15 09:59:37 +0000 2020,@pazunre @davlanade @alabi_jesujoba @GhanaNLP That's awesome! Thanks for the pointer. :)
1227945841626927000,Thu Feb 13 13:21:17 +0000 2020,RT @vlachos_nlp: Deadline for applications for the NLP faculty position in @cambridgenlp @Cambridge_CL coming up this Sunday! https://t.co/‚Ä¶
1227923762668089300,Thu Feb 13 11:53:33 +0000 2020,@beeonaposy @joecrobak @lonriesberg @ModeAnalytics Thanks for the shout-out! ‚ò∫Ô∏è
1227923692690313200,Thu Feb 13 11:53:16 +0000 2020,RT @beeonaposy: @joecrobak @lonriesberg @ModeAnalytics Best for NLP: NLP News  @seb_ruder provides a one-stop-shop for keeping up with what‚Ä¶
1227561936214208500,Wed Feb 12 11:55:47 +0000 2020,RT @alienelf: Reminder to those working in African Languages (&amp; relevant NLP techniques) to submit to the  @iclr_conf workshop in Addis: "A‚Ä¶
1227526223154352000,Wed Feb 12 09:33:52 +0000 2020,RT @RishirajSahaRoy: I am looking for one PhD position in my research group on question answering: https://t.co/DJ77LhQCVQ . If you are int‚Ä¶
1227323895918813200,Tue Feb 11 20:09:54 +0000 2020,RT @wzuidema: We're hiring! 5 new PhD positions in AI at ILLC.  Top-notch science! Salary! Benefits! Amsterdam! ILLC!  (1) deadline 10/3: a‚Ä¶
1227299490857660400,Tue Feb 11 18:32:55 +0000 2020,This is great work that collects corpora and evaluates models for two extremely low-resource languages spoken in Af‚Ä¶ https://t.co/2inVxL4c8W
1227296792502526000,Tue Feb 11 18:22:12 +0000 2020,RT @davlanade: Super excited to inform you that our paper on FastText and BERT embeddings for Twi and Yor√πb√° got accepted to #LREC2020. We‚Ä¶
1227296766413897700,Tue Feb 11 18:22:05 +0000 2020,@davlanade @alabi_jesujoba This is fantastic! Congrats! üëèIs the link to the paper already available?
1227249804327182300,Tue Feb 11 15:15:29 +0000 2020,@d_aumiller @StatNLP_HD Hey! You can find slides from a similar talk with a lot of overlap online here: https://t.co/gO68OJF8mx
1226864234128199700,Mon Feb 10 13:43:22 +0000 2020,RT @uppsala_nlp: Interested in doing a PhD in computational linguistics at Uppsala? We have an open position! More information here: https:‚Ä¶
1226422015332094000,Sun Feb 09 08:26:08 +0000 2020,RT @mgalle: Starting the year feeling that there are too many things going on in #nlproc and you can't catch up?  2021 can be different  @U‚Ä¶
1225836768446075000,Fri Feb 07 17:40:35 +0000 2020,@StatNLP_HD Thanks a lot for having me! üòä
1225703485661950000,Fri Feb 07 08:50:58 +0000 2020,RT @MarekRei: Analysis of ML and NLP publication statistics from 2019. https://t.co/IYN3ecNSlc #machinelearning #NLProc https://t.co/sUFjx4‚Ä¶
1224969968699236400,Wed Feb 05 08:16:14 +0000 2020,@RANTordaas Hi Yuvraj, please send me an email to continue the conversation.
1224969710753722400,Wed Feb 05 08:15:12 +0000 2020,RT @nasrinmmm: If only every serious deadline reminder email we receive on a daily basis had some dose of humor and creativity in it, our b‚Ä¶
1223971964559282200,Sun Feb 02 14:10:31 +0000 2020,RT @omarsar0: üì∞ NLP Newsletter #3: Flax, Thinc, Language-specific BERT models, Meena, Flyte, LaserTagger,‚Ä¶üì∞  featuring: @AnimaAnandkumar, @‚Ä¶
1223614479440720000,Sat Feb 01 14:30:00 +0000 2020,Curriculum for Reinforcement Learning   "Learning is probably the best superpower we humans have." @lilianweng expl‚Ä¶ https://t.co/Ok4V1EfxuN
1223362181158273000,Fri Jan 31 21:47:27 +0000 2020,@lmthang Seems like Meena is a fan of metal: https://t.co/D0yadgaHi1
1223276790145343500,Fri Jan 31 16:08:08 +0000 2020,@mohitban47 @DARPA @Microsoft @uncnlp @MSFTResearch @UNC @unccollege Super cool! Congrats!! üëè
1223240014777266200,Fri Jan 31 13:42:00 +0000 2020,RT @PontiEdoardo: Given the paucity of annotated data, how can we perform sample-efficient generalization on unseen task-language combinati‚Ä¶
1222920467712966700,Thu Jan 30 16:32:14 +0000 2020,RT @lilianweng: Humans learn from curriculum since birth. We can learn complicated math problems because we have accumulated enough prior k‚Ä¶
1222883645846892500,Thu Jan 30 14:05:55 +0000 2020,RT @StatNLP_HD: We are happy to welcome @seb_ruder back to Heidelberg! He will give a talk on 6th Feb about "Cross-lingual transfer learnin‚Ä¶
1222840088008282000,Thu Jan 30 11:12:50 +0000 2020,RT @omarsar0: This year I am keen on featuring ML and NLP tools and projects in the NLP Newsletter. They help to inspire other developers a‚Ä¶
1222829789075198000,Thu Jan 30 10:31:55 +0000 2020,RT @GaborMelis: When you apply a prototype Transmogrifier to language modelling, you get the Mogrifier LSTM https://t.co/ilXlv7PK8w and a c‚Ä¶
1222828253146120200,Thu Jan 30 10:25:49 +0000 2020,RT @MLSS_Tuebingen: Machine Learning Summer School 2020 is in Tuebingen, Germany! Please apply. Deadline: 11 Feb 2020. https://t.co/5dFNXAz‚Ä¶
1222530615641366500,Wed Jan 29 14:43:06 +0000 2020,RT @CamachoCollados: At @CompScienceCU we run a "Data and Knowledge Engineering" seminar every Monday. Check out the amazing list of speake‚Ä¶
1222174776548044800,Tue Jan 28 15:09:08 +0000 2020,@mormontre I'm glad you're finding it helpful. ‚ò∫Ô∏è
1222169572712878000,Tue Jan 28 14:48:27 +0000 2020,@jeremyphoward @Smerity The other dimension here is monolingual vs multilingual models. I think monolingual models‚Ä¶ https://t.co/43Gp00BVha
1222167166801281000,Tue Jan 28 14:38:53 +0000 2020,@KreutzerJulia @StatNLP_HD @GoogleAI Me too!! ‚ò∫Ô∏è
1222150114267095000,Tue Jan 28 13:31:08 +0000 2020,@KreutzerJulia @StatNLP_HD @GoogleAI Congrats!! üéâ
1222134594499162000,Tue Jan 28 12:29:28 +0000 2020,@lena_voita Cool! Is it being recorded or is anyone live tweeting?
1222072286544527400,Tue Jan 28 08:21:52 +0000 2020,@PeterMartigny @Twitter @feedly @TheSherylKlein Congrats, Peter! Awesome that you'll be continuing your NLP journey! üéâ
1222071322601230300,Tue Jan 28 08:18:02 +0000 2020,@alfcnz Congrats! This is so so great! Anyone you mentor is lucky to have you! üéâ
1221947094690386000,Tue Jan 28 00:04:24 +0000 2020,@SalimChemlal @zaidalyafeai Sure. Maybe you two should chat. ‚ò∫Ô∏è
1221882182354030600,Mon Jan 27 19:46:28 +0000 2020,RT @suzan: @evanmiltenburg @seb_ruder We have both a Dutch ULMFiT model and a Dutch BERT-model (BERT-NL), both available on https://t.co/Up‚Ä¶
1221869824009212000,Mon Jan 27 18:57:21 +0000 2020,RT @huggingface: @seb_ruder A few more ;)  - üá∏üá™ https://t.co/nbf52IOciR https://t.co/pV6Mn921vY - üá´üáÆ https://t.co/LdUqwaliip - üáØüáµ https://t‚Ä¶
1221862122986070000,Mon Jan 27 18:26:45 +0000 2020,Looking at the replies, there are a lot of BERT models that I missed: - üá∑üá∫ RuBERT https://t.co/SOh4N3qDD7 - üá™üá∏ BETO‚Ä¶ https://t.co/XW8ti0lnvn
1221852940161425400,Mon Jan 27 17:50:16 +0000 2020,@evanmiltenburg Thanks! That one slipped under my radar.
1221852359925674000,Mon Jan 27 17:47:58 +0000 2020,RT @pfjaeger: We are excited to host @seb_ruder on Februray 6 at https://t.co/nJg10Wippf ! He will talk about "Cross-lingual Transfer Learn‚Ä¶
1221851361811128300,Mon Jan 27 17:44:00 +0000 2020,Transfer learning is increasingly going multilingual with language-specific BERT models:  - üá©üá™ German BERT‚Ä¶ https://t.co/iNsHc1ItFI
1221734644036636700,Mon Jan 27 10:00:12 +0000 2020,New NLP News: NLP Progress, Restrospectives and look ahead, New NLP courses, Independent research initiatives, Inte‚Ä¶ https://t.co/jPr38bkoe3
1221486960755146800,Sun Jan 26 17:36:00 +0000 2020,If you want to learn about privacy-preserving machine learning, then there is no better resource than this step-by-‚Ä¶ https://t.co/EOnReiE36t
1221439900710342700,Sun Jan 26 14:29:00 +0000 2020,@chipro's analyses compensation, level, and experience details of 19k tech workers (mostly FAAAM and US-based).   T‚Ä¶ https://t.co/f75mTqH17u
1221159301860880400,Sat Jan 25 19:54:00 +0000 2020,Emil‚Äôs Story as a Self-Taught AI Researcher  An interview with @EmilWallner with useful tips on structuring a curri‚Ä¶ https://t.co/4Pp4Pucmg8
1221112662811201500,Sat Jan 25 16:48:40 +0000 2020,@marian_nmt Sounds great!
1221096135479054300,Sat Jan 25 15:43:00 +0000 2020,Is MT really lexically less diverse than human translation?  TL;DR: @marian_nmt analyses WMT19 system outputs and f‚Ä¶ https://t.co/gioZ50a8dw
1221060401259065300,Sat Jan 25 13:21:00 +0000 2020,@marian_nmt Thanks for writing about this and for the interesting analyses! Maybe you could add a definition of MTL‚Ä¶ https://t.co/Dr7p537REM
1221035502146674700,Sat Jan 25 11:42:04 +0000 2020,I also really like the focus on learning, organised around a collection of top resources for each topic: https://t.co/OyIUgbOETS
1221035013501833200,Sat Jan 25 11:40:07 +0000 2020,@GokuMohandas For instance, you can follow what I'm currently reading here: https://t.co/xDh7JeDXUa
1221034934636335000,Sat Jan 25 11:39:48 +0000 2020,practicalAI is a free tool to discover &amp; organise the top-community created ML content by @GokuMohandas.   It's sup‚Ä¶ https://t.co/utP530OfUD
1220661145230594000,Fri Jan 24 10:54:30 +0000 2020,@iamtrask Thanks, Andrew! üòä
1219572139164160000,Tue Jan 21 10:47:11 +0000 2020,RT @alienelf: Excited to invite NLP researchers working in African Languages (or relevant NLP techniques) to submit to the @iclr_conf works‚Ä¶
1218164869159182300,Fri Jan 17 13:35:11 +0000 2020,RT @lawrennd: My department is hiring in NLP!  This is a fantastic opportunity for someone to work in one of the world's leading environmen‚Ä¶
1217495602533146600,Wed Jan 15 17:15:46 +0000 2020,@jeremyphoward @XinhaoLi1 @fastdotai Thanks for the pointer! I'll check them out.
1217237356195012600,Wed Jan 15 00:09:35 +0000 2020,RT @sleepinyourhat: There's a decent chance that I'll have space/$ to host a visiting PhD student this summer for work on large-scale NLU d‚Ä¶
1217121886456438800,Tue Jan 14 16:30:45 +0000 2020,RT @navatintarev: Reminder, 11 PhD positins open on explainable AI in 6 different European countries. :)
1216866643223236600,Mon Jan 13 23:36:30 +0000 2020,@omarsar0 @viglovikov @huggingface @TensorFlow @zacharylipton @timnitGebru @gradientpub Thanks for doing this! Look‚Ä¶ https://t.co/7j9QtpR88C
1216865727195623400,Mon Jan 13 23:32:52 +0000 2020,@shreydesai Thanks for the pointer! Added the reference to the post. :)
1216415937362038800,Sun Jan 12 17:45:34 +0000 2020,RT @omarsar0: üì∞ NLP Newsletter (Issue #1): Tokenizers, TensorFlow 2.1, TextVectorization, TorchIO, NLP Shortfalls,‚Ä¶üì∞  featuring: @seb_ruder‚Ä¶
1216389202700591000,Sun Jan 12 15:59:19 +0000 2020,RT @ClementDelangue: Great intro to the modern landscape of Deep Learning &amp; #nlproc by @lexfridman @MIT. Including sweet mentions of the mo‚Ä¶
1214559595512782800,Tue Jan 07 14:49:07 +0000 2020,RT @h_saggion: Deadline approaching... PhD position in Natural Language Processing in Deep Summarization  at #LaSTUS @talnupf in the contex‚Ä¶
1214539745784717300,Tue Jan 07 13:30:15 +0000 2020,@IAugenstein @DIKU_Institut Awesome!! Huge congrats, Isabelle!! üéâ
1214125895616729000,Mon Jan 06 10:05:45 +0000 2020,RT @IAugenstein: Is starting a PhD in #NLProc #ML on your list of New Year's resolutions? You can now apply for an open-topic Marie Curie T‚Ä¶
1214107906427248600,Mon Jan 06 08:54:16 +0000 2020,10 ML &amp; NLP Research Highlights of 2019  New blog post on ten ML and NLP research directions that I found exciting‚Ä¶ https://t.co/54rytXTYSa
1213822218142138400,Sun Jan 05 13:59:03 +0000 2020,NLP Year in Review ‚Äî 2019  An extensive list of interesting publications, creative and societal applications, tools‚Ä¶ https://t.co/jRunsBgwEe
1212886778321002500,Fri Jan 03 00:01:56 +0000 2020,@MaduakorFrancis Thanks for the shout-out! ‚ò∫Ô∏è
1211956830219444200,Tue Dec 31 10:26:40 +0000 2019,@sjmielke Thanks for sharing, Sabrina! I wish you all the best for what's ahead! Seems like a great start to the new year! :)
1211593309866643500,Mon Dec 30 10:22:10 +0000 2019,RT @ivrik: üëá(Thread) Some Advice for Ph.D. Students:üëá üí´ideas: the best and most interesting research ideas came to my mind when I was eithe‚Ä¶
1211245823700406300,Sun Dec 29 11:21:22 +0000 2019,@math_rachel Thanks for sharing, Rachel! üòä
1211245754519568400,Sun Dec 29 11:21:06 +0000 2019,RT @math_rachel: NLP research wishlist 2020 by @seb_ruder: - Learn from few samples instead of large data - Compact &amp; efficient models, not‚Ä¶
1210176485031583700,Thu Dec 26 12:32:12 +0000 2019,RT @AnalyticsVidhya: Just in case you missed it, here's a comprehensive look at the various development in #MachineLearning in 2019 and wha‚Ä¶
1209805607684980700,Wed Dec 25 11:58:28 +0000 2019,@ionandrou Congrats, Ion! Well deserved! üéâ
1209433089086251000,Tue Dec 24 11:18:13 +0000 2019,@iugoaoj Thanks! It should be fixed now in the online version.
1209140243565285400,Mon Dec 23 15:54:33 +0000 2019,@danielrock Thanks, Daniel! ‚ò∫Ô∏è
1209092448347152400,Mon Dec 23 12:44:38 +0000 2019,New NLP News: 2020 NLP wish lists, HuggingFace + fastai, NeurIPS 2019, GPT-2 things, Machine Learning Interviews‚Ä¶ https://t.co/g3PQVjRit8
1208378697314640000,Sat Dec 21 13:28:26 +0000 2019,@arunchaganty Thanks for sharing this summary! I really like the focus on ideas and particular parts of a paper.
1208056053960773600,Fri Dec 20 16:06:22 +0000 2019,@paarulakan Not sure. Maybe @iatitov @alsuhr @jkkummerfeld know more.
1207938836975816700,Fri Dec 20 08:20:35 +0000 2019,RT @seth_stafford: @seb_ruder @dabelcs @NeurIPSConf I went looking for the context of the L√©on Bottou quote and noticed that it‚Äôs from his‚Ä¶
1207746420842909700,Thu Dec 19 19:36:00 +0000 2019,‚ÄúNature does not shuffle the data, so we shouldn't either‚Äù‚ÄîLeon Bottou (ICML 2019 Keynote) via @dabelcs' fantastic‚Ä¶ https://t.co/eTFGahLWMf
1207403913932136400,Wed Dec 18 20:55:00 +0000 2019,Great to see VCs being excited about NLP. Recent examples:  - @Lux_Capital's investment in @huggingface:‚Ä¶ https://t.co/W7Fq0o7VPe
1207275616124452900,Wed Dec 18 12:25:11 +0000 2019,@WWRob Thanks for pointing that out! I meant Indic here as "languages spoken in India" rather than the linguistic l‚Ä¶ https://t.co/1Ra5520Hh3
1207275616124452900,Wed Dec 18 12:25:11 +0000 2019,@WWRob Thanks for pointing that out! I meant Indic here as "languages spoken in India" rather than the linguistic l‚Ä¶ https://t.co/1Ra5520Hh3
1207082700131229700,Tue Dec 17 23:38:36 +0000 2019,RT @loretoparisi: Thanks @seb_ruder. When dealing with indic languages and #nlp it‚Äôs also important to mention libindic suite and indic-tra‚Ä¶
1207074241830674400,Tue Dec 17 23:05:00 +0000 2019,Pretrained language models for 12 Indic languages in the iNLTK toolkit: https://t.co/kynTAIeep5 https://t.co/nWYc2nkscI
1207062589114589200,Tue Dec 17 22:18:42 +0000 2019,NLP internship / PhD opportunities for speakers of low-resource languages (particularly African üåç): - PhD w/‚Ä¶ https://t.co/JyODnD9orL
1207053475064012800,Tue Dec 17 21:42:29 +0000 2019,This looks like a great symposium on an important topic (integrating specific &amp; general knowledge / fast &amp; slow lea‚Ä¶ https://t.co/jqVQNZoGBY
1206983386767790000,Tue Dec 17 17:03:58 +0000 2019,RT @radamihalcea: My 2020 NLP wishlist -start paying attention to human lang acquisition, to learn more about us &amp; gain insights for NLP mo‚Ä¶
1206590352402079700,Mon Dec 16 15:02:12 +0000 2019,RT @GokuMohandas: ‚ÄúML is way easier to learn now with all the free resources‚Äù - yeah, but it‚Äôs also a lot more overwhelming (low signal/noi‚Ä¶
1204471278679318500,Tue Dec 10 18:41:45 +0000 2019,@sivareddyg @facebookai @MILAMontreal @mcgillu This is awesome, Siva! Congrats!!
1204339442883928000,Tue Dec 10 09:57:53 +0000 2019,RT @fchollet: One of my worries about current trends in AI research is the reinforcement of the long-term cultural &amp; linguistic dominance o‚Ä¶
1204131253630328800,Mon Dec 09 20:10:37 +0000 2019,RT @gregd_nlp: post-weekend ACL paper status https://t.co/vlPdq6usCk
1203613311918583800,Sun Dec 08 09:52:30 +0000 2019,If you'd like to get a sneak peek at the latest advances in NLP and have submitted papers to *ACL conferences in th‚Ä¶ https://t.co/RYOHd6VHG3
1203234292014432300,Sat Dec 07 08:46:24 +0000 2019,@AdaBeaudoin I won't make it to NeurIPS this time. Enjoy the conference!
1203095558136709000,Fri Dec 06 23:35:08 +0000 2019,@AdaBeaudoin Thanks so much for the kind words! :)
1202577499563147300,Thu Dec 05 13:16:33 +0000 2019,RT @natschluter: I'm looking for 2 PhD students in #NLProc on a #GoogleBrain funded grant.  Come join us in lovely Copenhagen!  Deadline: F‚Ä¶
1202349826203160600,Wed Dec 04 22:11:51 +0000 2019,So many great lessons in Richard Hamming's "You and Your Research" (1986): - "Continue to plant the little acorns f‚Ä¶ https://t.co/IBvVwRpWwT
1202143229392621600,Wed Dec 04 08:30:55 +0000 2019,RT @jeremyphoward: If you're interested in learning both classic and modern NLP techniques in a code first way, there's a study group for‚Ä¶
1201564573632991200,Mon Dec 02 18:11:33 +0000 2019,RT @gneubig: We are looking for an intern (Undergrad or Graduate) in Summer 2020 who speaks a less-resourced or indigenous language and wou‚Ä¶
1200571549285277700,Sat Nov 30 00:25:37 +0000 2019,RT @jeremyphoward: Oh this is cool - successfully combining our (with @seb_ruder) ULMFiT transfer learning methods with BERT to get SoTA fo‚Ä¶
1200000251920339000,Thu Nov 28 10:35:29 +0000 2019,RT @alienelf: So, today is a day for African NLP  @siminyu_kat  and team, among all the other crazy amazing things they're making happen ma‚Ä¶
1199754942912901000,Wed Nov 27 18:20:43 +0000 2019,@EdwardDixon3 Thanks for the pointer, Edward! Yes, I'm very much a fan of not going with the mainstream. Other rece‚Ä¶ https://t.co/hTKrdf9xg2
1199692102554312700,Wed Nov 27 14:11:01 +0000 2019,RT @yoavgo: This ACL features a brand new "Interpretation" track! It's about time, but we also don't have a pool of dedicated reviewers yet‚Ä¶
1199620180923633700,Wed Nov 27 09:25:13 +0000 2019,RT @julianharris: "Long term" memory is a key research challenge in #NLProc  Episodic Memory in Lifelong Language Learning: local adaptatio‚Ä¶
1199420559542509600,Tue Nov 26 20:12:00 +0000 2019,As it turns out, Wang Ling was way ahead of the curve re NLP's muppet craze (see slides from LxMLS '16 &amp; Oxford NLP‚Ä¶ https://t.co/GJmVzl57ga
1198539211575570400,Sun Nov 24 09:49:50 +0000 2019,@SmallerNNsPls Thanks! :)
1198217185774231600,Sat Nov 23 12:30:13 +0000 2019,@SmallerNNsPls Besides that, another table with an overview of approximate (reported) compression rates would be really nice to have.
1198217021256880000,Sat Nov 23 12:29:34 +0000 2019,@SmallerNNsPls Thanks so much for putting this together! Would it be possible to "compress" the table (making the f‚Ä¶ https://t.co/ujxzldtaCf
1197988057796030500,Fri Nov 22 21:19:45 +0000 2019,RT @PrabhuPradhan: NeurIPS Social Event for Budding Researchers.  (1) Brainstorming w. amazing mentors (2) Enlightening panel session (3) Q‚Ä¶
1197987597278208000,Fri Nov 22 21:17:55 +0000 2019,RT @wuningxi: The language we speak shapes our online experience, determines our access to information and affects what technologies are av‚Ä¶
1197887168204677000,Fri Nov 22 14:38:51 +0000 2019,RT @cambridgenlp: Great talk from @seb_ruder from  @DeepMindAI visiting the NLP lab today on "Unsupervised cross-lingual representation lea‚Ä¶
1197634529831399400,Thu Nov 21 21:54:57 +0000 2019,RT @alkalait: Very happy to share our event with the @NeurIPSConf community.  The "Well-being in ML" social at NeurIPS 2019 will take place‚Ä¶
1197560042670743600,Thu Nov 21 16:58:58 +0000 2019,RT @DeepMindAI: Last week, 100's of researchers gathered in Montevideo, Uruguay, for @Khipu_AI - a teaching conference aiming to strengthen‚Ä¶
1197182642019602400,Wed Nov 20 15:59:19 +0000 2019,RT @vlachos_nlp: Thanks @seb_ruder  for the shoutout to @AthensNlp ! Credit goes to our great speakers: Ryan McDonald @xavier_nlp  @vinodkp‚Ä¶
1196799446828404700,Tue Nov 19 14:36:38 +0000 2019,RT @me_datapoint: Inaccurate, non-informed, and sensational reports have led to far too much hype around #AI and often created a totally wr‚Ä¶
1196777633717072000,Tue Nov 19 13:09:57 +0000 2019,@hadyelsahar @revue Thanks! Fixed now in the online version.
1196762621195825200,Tue Nov 19 12:10:18 +0000 2019,RT @sannykimchi: Some great video playlists from @seb_ruder's newsletter: Indaba '19 (talks by @drfeifei, @RichardSocher ..): https://t.co/‚Ä¶
1196756702256861200,Tue Nov 19 11:46:47 +0000 2019,@XYOU @revue Thanks for the note! That's fixed now in the online version.
1196729895772131300,Tue Nov 19 10:00:15 +0000 2019,New NLP News: Highlights of EMNLP 2019, Ethics in NLP vol. 2, AI and Journalism https://t.co/LT02qjsIoL (via @revue)
1196103152665288700,Sun Nov 17 16:29:48 +0000 2019,RT @Khipu_AI: Khipu 2019 crowd with the Masakhane spirit, let's build together! Thanks everyone for the incredible energy and enthusiasm th‚Ä¶
1194582816752914400,Wed Nov 13 11:48:32 +0000 2019,RT @MarekRei: 74 short summaries of machine learning and NLP research papers. #NLProc #MachineLearning #DeepLearning https://t.co/j7a3xSjz3‚Ä¶
1194392004144377900,Tue Nov 12 23:10:19 +0000 2019,RT @AI4Dev: To make #AI applications more accessible, we are breaking new ground in #NLP of African languages with our new #AI4D African La‚Ä¶
1193202994323570700,Sat Nov 09 16:25:37 +0000 2019,@Audrey_Sage_ Sorry about the late reply. The slides are available here: https://t.co/AeTuuY7oRl
1192871251049615400,Fri Nov 08 18:27:23 +0000 2019,RT @eagirre: Great blog post covering the very latest on cross-lingual representation learning, including our own research at @IxaGroup wit‚Ä¶
1192863157393473500,Fri Nov 08 17:55:13 +0000 2019,RT @Khipu_AI: The countdown is on! Only 2 days until Khipu 2019 kicks off in Montevideo. If you can't attend in person, you can tune in via‚Ä¶
1192762105025454000,Fri Nov 08 11:13:40 +0000 2019,RT @cambridgenlp: Don‚Äôt Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction. #emn‚Ä¶
1192472117876609000,Thu Nov 07 16:01:22 +0000 2019,RT @_aylien: In our biggest feature rollout to date, we‚Äôre bringing a whole new dimension of investigation and discovery capabilities to ou‚Ä¶
1192440394182738000,Thu Nov 07 13:55:19 +0000 2019,RT @kbeguir: Great point! To everyone denied a visa for #NeurIPS2019 you are welcome to #Tunisia in 2020 for @DeepIndaba &amp; @AIHackTunisia!‚Ä¶
1192437504114339800,Thu Nov 07 13:43:50 +0000 2019,@GuillaumeLample @emnlp2019 Congrats!! üëèIt's great to see that work on low-resource languages is appreciated and awarded.
1192193025268695000,Wed Nov 06 21:32:21 +0000 2019,@ArthurCamara Unfortunately not as far as I know. Cc'ing @revue as I've asked for this a while ago: https://t.co/bPrIV9uNXc
1192191884975845400,Wed Nov 06 21:27:49 +0000 2019,If you are at @emnlp2019, then don't miss @eisenjulian's talk on our MultiFiT paper at 15:54 in Session 11A on Thur‚Ä¶ https://t.co/LCRSdKEIJp
1192189950730608600,Wed Nov 06 21:20:08 +0000 2019,New blog post: Unsupervised cross-lingual representation learning An overview of learning cross-lingual representat‚Ä¶ https://t.co/Gs5thkOiJR
1192185829025894400,Wed Nov 06 21:03:46 +0000 2019,@waydegilliam @eisenjulian @PiotrCzapla @GuggerSylvain @jeremyphoward @PiotrCzapla is just preparing an update to u‚Ä¶ https://t.co/Re7mNw2YXT
1192170726331224000,Wed Nov 06 20:03:45 +0000 2019,@F_Vaggi @leopd For language modelling on small datasets such as PTB and WikiText-2, LSTMs still seem to have the e‚Ä¶ https://t.co/t7Bibjstk2
1191054750885859300,Sun Nov 03 18:09:15 +0000 2019,RT @emnlp2019: #EMNLP2019 livetweeters:  @annargrs @aryamccarthy @Awasthi_A_ @daniilmagpie @HaoTan5 @jmhessel @joeddav @Lasha1608 @mayhewsw‚Ä¶
1190554462647128000,Sat Nov 02 09:01:17 +0000 2019,RT @Quantum_Stat: Tonight we had a great time joining @seb_ruder 's talk about cross-lingual representations at @nyuniversity   This word2v‚Ä¶
1190554324981682200,Sat Nov 02 09:00:45 +0000 2019,@RahelJhirad Thanks so much for having me! It was a pleasure talking about NLP with so many interested people from different backgrounds.
1190554086778818600,Sat Nov 02 08:59:48 +0000 2019,RT @RahelJhirad: Completely incredible and generous talk by @seb_ruder on cross lingual representation learning tonight. A Friday evening t‚Ä¶
1190436844007297000,Sat Nov 02 01:13:55 +0000 2019,If you are at @emnlp2019, don't forget to sign up for live tweeting. Even tweeting one session will help those of u‚Ä¶ https://t.co/EnpnqhuwxH
1190271070563176400,Fri Nov 01 14:15:12 +0000 2019,@alienelf @alkalait üòç
1190268559534022700,Fri Nov 01 14:05:13 +0000 2019,RT @ThomasScialom: On the Cross-lingual Transferability of Monolingual Representations  By @artetxem, @seb_ruder &amp; @DaniYogatama  Interesti‚Ä¶
1190028075137982500,Thu Oct 31 22:09:37 +0000 2019,RT @ashkamath20: It was great to have @seb_ruder give us an overview of cross-lingual representation learning methods and their limitations‚Ä¶
1190019965656678400,Thu Oct 31 21:37:23 +0000 2019,RT @wellecks: @seb_ruder https://t.co/N0Kzbej5zX https://t.co/KkmPV5HdP2
1190019958488612900,Thu Oct 31 21:37:22 +0000 2019,RT @alfcnz: ¬´Unsupervised cross-lingual representation learning¬ª by the incredible researcher and friend @seb_ruder. https://t.co/6WF2MWraWq
1189565741579808800,Wed Oct 30 15:32:28 +0000 2019,RT @ClementDelangue: Night Dumbo walk with @LysandreJik @seb_ruder @SanhEstPasMoi https://t.co/hkTCqsDydk
1189292634138988500,Tue Oct 29 21:27:14 +0000 2019,RT @DataScienceNIG: NLP to the next level with MultiFit - novel methods for multilingual fine-tuning of languages that outperform models tr‚Ä¶
1188890012630376400,Mon Oct 28 18:47:22 +0000 2019,RT @emnlp2019: Heading to EMNLP soon? Help us livetweet the conference!   We are looking for volunteers to livetweet each session. #nlproc‚Ä¶
1188880218699776000,Mon Oct 28 18:08:27 +0000 2019,RT @sleepinyourhat: Excited to host @seb_ruder at @NYUDataScience this Thursday for a talk on unsupervised cross-lingual representation lea‚Ä¶
1188854395812888600,Mon Oct 28 16:25:50 +0000 2019,RT @DaniYogatama: Our amazing intern @artetxem show we can transfer a monolingual model to other languages by just learning lexical embeddi‚Ä¶
1188801638930231300,Mon Oct 28 12:56:12 +0000 2019,RT @artetxem: Check out our new paper "On the Cross-lingual Transferability of Monolingual Representations" (w/ @seb_ruder &amp; @DaniYogatama)‚Ä¶
1188160449591890000,Sat Oct 26 18:28:20 +0000 2019,RT @EdwardDixon3: #BERT, #DeepLearning, #NLP and @JeffDean on the front page of the @FT , am I reading my favourite paper or @seb_ruder 's‚Ä¶
1187423419954057200,Thu Oct 24 17:39:39 +0000 2019,This is a great collection of advice with many different perspectives on applying to a PhD program in NLP. https://t.co/O7SHWLPr9D
1187409171790139400,Thu Oct 24 16:43:02 +0000 2019,RT @kdnuggets: #Economics and #BigData Meetup: Friday, Nov 1, 6pm, NYC Speaker: @seb_ruder from @DeepMindAI Talk: Unsupervised Cross-Lingua‚Ä¶
1187365816402108400,Thu Oct 24 13:50:45 +0000 2019,RT @emnlp2019: The conference app for EMNLP-IJCNLP 2019 is now live. You can download it by following the instructions here: https://t.co/z‚Ä¶
1187329828934078500,Thu Oct 24 11:27:45 +0000 2019,The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning l‚Ä¶ https://t.co/C0IlJEU7fT
1187316575185113000,Thu Oct 24 10:35:05 +0000 2019,RT @DeepMindAI: Our new work uses episodic memory to learn language tasks in a lifelong setting. The episodic memory is used in two ways: s‚Ä¶
1187070447516639200,Wed Oct 23 18:17:04 +0000 2019,RT @dramemariama20: Did you miss EurNLP or Deep Learning Indaba? No worries!!!!! @seb_ruder gives you the opportunity to make up for it. Fo‚Ä¶
1187039613300424700,Wed Oct 23 16:14:32 +0000 2019,RT @emnlp2019: The interactive schedule for the conference is now live. You can choose talks, posters, tutorials, and workshops to generate‚Ä¶
1187012836381380600,Wed Oct 23 14:28:08 +0000 2019,RT @jeremyphoward: One year ago, @seb_ruder asked the https://t.co/GEOZuodrZj community for help with multilingual language modeling.  @eis‚Ä¶
1187006229371117600,Wed Oct 23 14:01:53 +0000 2019,RT @alienelf: Just saw that the vid for my talk on "Machine Translation for African Languages" at @DeepIndaba has been uploaded. This is wh‚Ä¶
1186744388908654600,Tue Oct 22 20:41:25 +0000 2019,Most of the world‚Äôs text is not in English. We are releasing MultiFiT to train and fine-tune language models effici‚Ä¶ https://t.co/bVnBMsQPEv
1186567939232817200,Tue Oct 22 09:00:16 +0000 2019,New NLP News: Deep Learning Indaba, EurNLP, ML echo chamber, Pretrained LMs, Reproducibility papers via @revue https://t.co/tvhIGplTCj
1186562653973631000,Tue Oct 22 08:39:16 +0000 2019,RT @emnlp2019: The instructions for presenters of oral, poster, and remote presentations at the main conference are now available on the co‚Ä¶
1186399477789319200,Mon Oct 21 21:50:52 +0000 2019,RT @full_stack_dl: We've been working hard to make it easier for internationals and students from underrepresented backgrounds to participa‚Ä¶
1186286448921723000,Mon Oct 21 14:21:44 +0000 2019,@pommedeterre33 Thanks for the pointer! You'll find it in the next edition tomorrow. :)
1186250839322706000,Mon Oct 21 12:00:14 +0000 2019,RT @DaniYogatama: In our recent work led by @ikekong, we provide an information theoretic perspective that unifies classical and modern lan‚Ä¶
1185817139099394000,Sun Oct 20 07:16:52 +0000 2019,RT @emnlp2019: We are delighted to let you know that over 1,700 people have registered for the conference. If you haven‚Äôt registered yet, d‚Ä¶
1185284969733787600,Fri Oct 18 20:02:12 +0000 2019,@madhugraj Hi Madhu, please send me an email if you would like some advice.
1184918838971895800,Thu Oct 17 19:47:20 +0000 2019,RT @IAmSamFin: Nice article!  (And their updated headline: ‚ÄúMachines Beat Humans on a Reading Test. But Do They Understand?‚Äù Is waaaaaay be‚Ä¶
1184899254961266700,Thu Oct 17 18:29:31 +0000 2019,Machines‚Äô Language Skills Just Surpassed Our Own. But Is This Understanding? Nuanced take by @QuantaMagazine on the‚Ä¶ https://t.co/ztdDGMzDNi
1184778504690393000,Thu Oct 17 10:29:42 +0000 2019,RT @Andyweizhao: This post discusses the highlights of the 1st EurNLP, wrote with @ChaiHaixia. Great time at @eurnlp #EurNLP2019 https://t.‚Ä¶
1184230596085796900,Tue Oct 15 22:12:30 +0000 2019,RT @juanmirocks: Annotating text in its original context, including images and style, is important for #NLProc   More on my post: üìùhttps://‚Ä¶
1184207347314577400,Tue Oct 15 20:40:07 +0000 2019,RT @RahelJhirad: I am happy to announce that our next Meetup speaker will be @seb_ruder from @DeepMindAI   His talk:  Unsupervised Cross-Li‚Ä¶
1184207241504874500,Tue Oct 15 20:39:42 +0000 2019,@kelina1124 That's awesome!! Huge congrats, Katharina! üéâ
1184207063725027300,Tue Oct 15 20:39:00 +0000 2019,RT @kelina1124: üåüExciting news: I will be joining the CS department at CU Boulder as an assistant professor in January 2020!üåü I will hire P‚Ä¶
1183316575086100500,Sun Oct 13 09:40:31 +0000 2019,RT @max_nlp: "The future is not about bigger models, it's about bigger ideas" - Phil Blunsom @eurnlp https://t.co/NMlieRLjhe
1183313277117436000,Sun Oct 13 09:27:24 +0000 2019,RT @AndrewM_Webb: So in @sirajraval's livestream yesterday he mentioned his 'recent neural qubit paper'. I've found that huge chunks of it‚Ä¶
1183307593722282000,Sun Oct 13 09:04:49 +0000 2019,@Byron_Wan @_onionesque @AndrewM_Webb @hardmaru @sirajraval @JeffDean @goodfellow_ian @fchollet @ctnzr @jackclarkSF @risi1979 Unfollowed.
1183172094844113000,Sun Oct 13 00:06:24 +0000 2019,RT @glorisonne: "The future of NLP is not about bigger models, it's about bigger ideas" Phil Blunsom in #EurNLP2019 panel on Academia and I‚Ä¶
1182927805169967000,Sat Oct 12 07:55:41 +0000 2019,RT @str_t5: Had a great day yesterday at @eurnlp! Many good talks and interesting conversations. Thanks @barbara_plank, @seb_ruder and the‚Ä¶
1182698343362912300,Fri Oct 11 16:43:53 +0000 2019,Andr√©: Need to reflect on what makes professors go to industry. Teaching load, administration can improve in academ‚Ä¶ https://t.co/OHuwkBhHPM
1182697873432969200,Fri Oct 11 16:42:01 +0000 2019,Bonnie: Industry investment is great but don't want to lose people from training students. Dirk: A lot of good rese‚Ä¶ https://t.co/Toh7NKjgst
1182697179284168700,Fri Oct 11 16:39:15 +0000 2019,Nando: Universities protect their IP. Lack of diversity, i.e. lack of presence of Africans at ML/NLP conferences. S‚Ä¶ https://t.co/fDRvO5NBFC
1182695879523876900,Fri Oct 11 16:34:05 +0000 2019,Phil: Should have additional relationships besides industry-sponsored professorships. Students being able to access‚Ä¶ https://t.co/clQUhYozMt
1182694632309903400,Fri Oct 11 16:29:08 +0000 2019,Bonnie: Internships are the main way at the moment to get the benefits of both sides. Phil: Should not be that hard‚Ä¶ https://t.co/wNXtsFZ7Sf
1182693864198422500,Fri Oct 11 16:26:05 +0000 2019,Dirk: Industry can be more focused on short-term project while academia might be lacking in funding. Academia and i‚Ä¶ https://t.co/gmlSquQHcz
1182693235287834600,Fri Oct 11 16:23:35 +0000 2019,@yoavgo @_dmh @zehavoc @glnmario Thanks for the feedback everyone! Made it more prominent now. Will keep that in mind for future events.
1182692990319583200,Fri Oct 11 16:22:36 +0000 2019,Tim: Is doing a part-time PhD between industry and academia a good thing or are there conflicts? Angela: Different‚Ä¶ https://t.co/zMHI0HN3ZX
1182691995682328600,Fri Oct 11 16:18:39 +0000 2019,Angela: It's ok to work on something that is competitive as long as you're aware of it and can break it up into pub‚Ä¶ https://t.co/RSiioNusEn
1182690946103808000,Fri Oct 11 16:14:29 +0000 2019,Bonnie: Student has to think about what they can claim about their work; industry is organized about products. Need‚Ä¶ https://t.co/HqExjNwkVF
1182690329767678000,Fri Oct 11 16:12:02 +0000 2019,Tim: Any recommendations for 1st year PhD students to choose a topic? Dirk: If idea can be better implemented in in‚Ä¶ https://t.co/Ln6Ji3R85k
1182689793727258600,Fri Oct 11 16:09:54 +0000 2019,Phil: More resources do not lead to better research. Future is not about bigger models, but bigger and more thought‚Ä¶ https://t.co/LIWMCFP5BZ
1182689023430742000,Fri Oct 11 16:06:51 +0000 2019,RT @ArthurCamara: That‚Äôs a nice panel we have here! https://t.co/1iy4beguF0 https://t.co/oYIkSv5gvi
1182688986403393500,Fri Oct 11 16:06:42 +0000 2019,Tim: What is the position of academia in the current compute arms race of ever larger models in NLP? Dirk: Not bein‚Ä¶ https://t.co/QgleSrwSpR
1182688444721565700,Fri Oct 11 16:04:33 +0000 2019,Angela: Part-time PhD between industry and academia is a nice bridge. Phil: Lots of things to learn in industry, coming from academia.
1182687987580186600,Fri Oct 11 16:02:44 +0000 2019,Panelists briefly introduce their backgrounds and their relation to industry/academia. Lucia: Both sides have advan‚Ä¶ https://t.co/b6t7GyPJOm
1182686990812303400,Fri Oct 11 15:58:46 +0000 2019,Chair: @_rockt  Panelists (left to right): Bonnie Webber, @dirk_hovy, @lspecia, Andr√© Martins, Angela Fan and Phil‚Ä¶ https://t.co/LWoVVavNsa
1182686554655019000,Fri Oct 11 15:57:02 +0000 2019,Join us for the EurNLP panel discussion now.  Live stream: https://t.co/adyOLXplJ4 https://t.co/UV4hnHjWA1
1182684323918549000,Fri Oct 11 15:48:10 +0000 2019,For more info and analyses, check out the excellent blog post by @lena_voita: https://t.co/3ftJJ6o782 Paper: https://t.co/DwA0QmIgA3
1182683552451825700,Fri Oct 11 15:45:06 +0000 2019,In masked language modelling, information about the token identity is distributed across the sentence and more conc‚Ä¶ https://t.co/rEWnEUnmm7
1182682185557561300,Fri Oct 11 15:39:40 +0000 2019,Important self-attention heads are specialized and remain after pruning. https://t.co/0mD9lWYx8O
1182680917678137300,Fri Oct 11 15:34:38 +0000 2019,@zehavoc @_dmh @yoavgo "However, all talks will be live streamed and can be watched online on our Facebook page." Maybe too hidden?
1182680510092497000,Fri Oct 11 15:33:01 +0000 2019,For the last talk of the day, @RicoSennrich sheds light on what Transformers learn, using pruning techniques among‚Ä¶ https://t.co/g5NzY1GmyM
1182679896629432300,Fri Oct 11 15:30:35 +0000 2019,@zehavoc @_dmh @yoavgo It is. First item in news on the main page, after the intro:  https://t.co/JEXVV5PNJZ We'll‚Ä¶ https://t.co/c5SB18rvRE
1182679357803974700,Fri Oct 11 15:28:26 +0000 2019,Different pruning strategies do not really matter as long as you choose layers evenly throughout the model (not jus‚Ä¶ https://t.co/Czu0ikl7tz
1182678154940469200,Fri Oct 11 15:23:39 +0000 2019,Layer dropout during training is an effective regularizer. Models trained with layer dropout are robust to pruning‚Ä¶ https://t.co/y2jErMpOcU
1182676515269566500,Fri Oct 11 15:17:08 +0000 2019,To do this, she proposes to drop out entire layers. https://t.co/4FajZK0WlZ
1182675882529431600,Fri Oct 11 15:14:38 +0000 2019,Staying with the theme of sparse and more efficient Transformers, Angela Fan now discusses how to prune Transformer‚Ä¶ https://t.co/LdoyKRB3al
1182674470944870400,Fri Oct 11 15:09:01 +0000 2019,By replacing softmax in the attention heads of Transformers with entmax, we can make them sparse. Heads can adaptiv‚Ä¶ https://t.co/3qCd7II1NC
1182671100393771000,Fri Oct 11 14:55:37 +0000 2019,RT @NandoDF: ü§ó Muppets driving bulldozers  in NLP #eurnlp2019 https://t.co/6ZfWBbkmu2
1182671006080667600,Fri Oct 11 14:55:15 +0000 2019,Andr√© Martin's starts the last session on Transformers @eurnlp discussing sparse Transformers Live stream:‚Ä¶ https://t.co/j2mBWCbZj6
1182662840710291500,Fri Oct 11 14:22:48 +0000 2019,For multimodal MT, why are images not helping more? Lucia demonstrates that multimodal models do better when the in‚Ä¶ https://t.co/ot2cWAVv7N
1182659190881435600,Fri Oct 11 14:08:18 +0000 2019,@lspecia now discusses more holistic approaches to NLP focusing on other modalities and MT. @eurnlp #EurNLP2019 https://t.co/uFqQErZ0sX
1182658659807043600,Fri Oct 11 14:06:11 +0000 2019,RT @barbara_plank: if you have burning questions about academia and industry in NLP: @_rockt is charing the panel https://t.co/JVOwRRRUDZ
1182657290513584000,Fri Oct 11 14:00:45 +0000 2019,RT @eurnlp: Join us later at 16:30 BST for our panel on Academia and Industry in NLP. Chair: @_rockt Panelists: Bonnie Webber, @lspecia, An‚Ä¶
1182657064818090000,Fri Oct 11 13:59:51 +0000 2019,Can our models infer these missing elements? Reut describes how they collected training data from annotators for re‚Ä¶ https://t.co/OSisRWfiAm
1182654252600180700,Fri Oct 11 13:48:41 +0000 2019,Next, Reut Tsarfaty talks about the empty elements of NLP and how we can infer them. Such elements are different sy‚Ä¶ https://t.co/G3bhakeX6I
1182651150073581600,Fri Oct 11 13:36:21 +0000 2019,In conclusion, there is still a lot of room for improvement when dealing with the referential context of language. https://t.co/ZutYOzwazk
1182649538454143000,Fri Oct 11 13:29:57 +0000 2019,Study short-term meaning shift. An online community is a good testing ground. Some words like "F5" can shift meanin‚Ä¶ https://t.co/2zynI5rErA
1182647393512317000,Fri Oct 11 13:21:25 +0000 2019,Gemma Boleda starts the afternoon session @eurnlp by discussing situation-specific and generic information in distr‚Ä¶ https://t.co/OvrLh95v3w
1182629548720087000,Fri Oct 11 12:10:31 +0000 2019,In conclusion, even neural networks simulating Eisner's algorithm still require cubic time. For more on this, check‚Ä¶ https://t.co/vZjBNNN4sw
1182627342625185800,Fri Oct 11 12:01:45 +0000 2019,Cubic time is the best we can do for maximum spanning tree decoding, assuming the exponential time hypothesis is tr‚Ä¶ https://t.co/lT785JTU6J
1182625305636331500,Fri Oct 11 11:53:39 +0000 2019,As the last talk of the session, @natschluter talkes a look under the hood of neural syntactic parsing.  @eurnlp‚Ä¶ https://t.co/MuROGnakXL
1182621581299912700,Fri Oct 11 11:38:51 +0000 2019,In practice, content-oriented methods outperform structure-oriented approaches significantly. Modeling textual simi‚Ä¶ https://t.co/2ViGXyaQPp
1182619239812915200,Fri Oct 11 11:29:33 +0000 2019,Network embedding methods can be categories as structure-oriented or content-oriented. The first only consider simi‚Ä¶ https://t.co/Zr0EHGtGdf
1182618447412416500,Fri Oct 11 11:26:24 +0000 2019,RT @PMinervini: Just a quick reminder that talks are being live-streamed right now :) https://t.co/WJINE3cTSg https://t.co/XkuWGPfm8s
1182618428382818300,Fri Oct 11 11:26:19 +0000 2019,Next up, @ionandrou now talks about network embeddings in the biomedical domain. They can be useful for link predic‚Ä¶ https://t.co/LjtwiiSX88
1182616363740864500,Fri Oct 11 11:18:07 +0000 2019,In conclusion, humans make implicit inferences when reading text. Our systems for machine reading should thus also‚Ä¶ https://t.co/oBFy4fi18A
1182612928962400300,Fri Oct 11 11:04:28 +0000 2019,Multiple relations can hold in a discourse. When inserting conjunctions in a discourse, cases of disagreement among‚Ä¶ https://t.co/RHikeXO89U
1182611732658905000,Fri Oct 11 10:59:43 +0000 2019,Next, Bonnie Webber talks about implicit relations in discourse. A nice example of discourse coherence: The explici‚Ä¶ https://t.co/5gC0zGz4JU
1182609585330360300,Fri Oct 11 10:51:11 +0000 2019,Conclusion: Is the end of supervised parsing in sight? Maybe.  Future parsing models may no longer perform structur‚Ä¶ https://t.co/3WmZyRffCA
1182608081743700000,Fri Oct 11 10:45:13 +0000 2019,Structural features and parsing algorithms have become less important with deep neural networks.  Transition-based‚Ä¶ https://t.co/YppgCEj95x
1182606492127682600,Fri Oct 11 10:38:54 +0000 2019,RT @anyabelz: "Time of innocence is over", "we can no longer pretend this has nothing to do with us" - call for #nlproc to take responsibil‚Ä¶
1182606222295535600,Fri Oct 11 10:37:49 +0000 2019,@JoakimNivre reflects whether the end of supervised parsing (or all parsing that we know) is in sight at the second‚Ä¶ https://t.co/GnQ72NbgUB
1182596704668926000,Fri Oct 11 10:00:00 +0000 2019,Machine learning is split between empiricism and rationalism: Empiricism: Black-box models that explain complex pro‚Ä¶ https://t.co/ntJPpkEbgF
1182595389087719400,Fri Oct 11 09:54:46 +0000 2019,NLP has different sources of bias: 1. The selection of the training data. 2. The biases of the annotators. 3. The i‚Ä¶ https://t.co/Dmu5YC5g7p
1182593155423752200,Fri Oct 11 09:45:54 +0000 2019,@dirk_hovy talks about unintended consequences and dual use of new technologies. Great slides and presentation. Che‚Ä¶ https://t.co/BL2janePVV
1182589996324274200,Fri Oct 11 09:33:21 +0000 2019,RT @eurnlp: Vera Demberg's talk "Must NLP consider individual differences in language processing more?" is starting off #EurNLP session 1 o‚Ä¶
1182589750026350600,Fri Oct 11 09:32:22 +0000 2019,RT @_dmh: Now Vera Demberg (from @Saar_Uni's @1102Sfb)  opens the first session with a question: Must NLP consider individual differences i‚Ä¶
1182589513228574700,Fri Oct 11 09:31:26 +0000 2019,Vera Demberg discusses whether we should consider differences between individuals more in NLP. If differences are s‚Ä¶ https://t.co/8KyoahJPpH
1182589508006699000,Fri Oct 11 09:31:24 +0000 2019,RT @evanmiltenburg: #NLProc people: there‚Äôs a livestream of the #EurNLP conference at  https://t.co/AP9qXj8ocZ  @_dmh, @seb_ruder, @gloriso‚Ä¶
1182586027036303400,Fri Oct 11 09:17:34 +0000 2019,RT @eurnlp: Watching the #EurNLP livestream? We will be taking questions for the speakers from the livestream audience and the comments on‚Ä¶
1182585905363800000,Fri Oct 11 09:17:05 +0000 2019,Nando: RL can be useful for language similarly to how it is used in robotics, to learn from data and simulated lang‚Ä¶ https://t.co/nOHgOGG9iQ
1182578807536009200,Fri Oct 11 08:48:53 +0000 2019,Nando: (Task) embeddings are very powerful. They allow to modulate a model's behaviour and enable it to generalize‚Ä¶ https://t.co/YGqxckvboS
1182577889612574700,Fri Oct 11 08:45:14 +0000 2019,RT @eurnlp: Keynote speaker @NandoDF speaking about imitation and reinforcement learning. https://t.co/HZzlZwqRek #EurNLP2019 #livestreamin‚Ä¶
1182577853457686500,Fri Oct 11 08:45:06 +0000 2019,@NandoDF discusses recent work on imitation including work on speech synthesis via meta-learning drawing an analogy‚Ä¶ https://t.co/5QfCjNM5Sy
1182573732881207300,Fri Oct 11 08:28:43 +0000 2019,@eurnlp starting off with a keynote by @NandoDF on exploration and imitation Live stream is available at:‚Ä¶ https://t.co/CqA5kzEcW8
1182572891470205000,Fri Oct 11 08:25:23 +0000 2019,RT @eurnlp: #EurNLP is underway. Check out the livestream at https://t.co/HZzlZwqRek
1182338282602057700,Thu Oct 10 16:53:08 +0000 2019,RT @paperswithcode: üéâ Introducing sotabench : a new service with the mission of benchmarking every open source ML model. We run GitHub repo‚Ä¶
1182303581711605800,Thu Oct 10 14:35:14 +0000 2019,sotabench üèÖüõ†‚ÄîThis is a great new service by @paperswithcode! It enables benchmarking open-source ML models and comp‚Ä¶ https://t.co/G1w0R5B0uB
1182203287795388400,Thu Oct 10 07:56:42 +0000 2019,RT @barbara_plank: On my way to the first #EurNLP2019 summitü•∞-  a full day of Natural Language Processing research:  https://t.co/bryqUlLjw‚Ä¶
1181933146075521000,Wed Oct 09 14:03:15 +0000 2019,RT @pywirrarika: My project Naki (A list that keep track of #NLP research done for Indigenous Languages of the American continent) appeared‚Ä¶
1181873951368192000,Wed Oct 09 10:08:02 +0000 2019,RT @emnlp2019: Re recent events in Hong Kong, we are committed to the conference proceeding as planned, in the stability and broad safety o‚Ä¶
1181872621400535000,Wed Oct 09 10:02:45 +0000 2019,RT @emnlp2019: The  early registration deadline is approaching: October 10, 11:59PM EDT. Each paper (main conference or workshop) needs to‚Ä¶
1181701124052652000,Tue Oct 08 22:41:17 +0000 2019,@IAugenstein @DFF_raad @CopeNLU @ryandcotterell That's amazing!! Congrats!! üéâ
1181637381960716300,Tue Oct 08 18:28:00 +0000 2019,This is a nice diagram by Zhengyan Zhang and @BakserWang that shows how many recent pretrained language models are‚Ä¶ https://t.co/CJWJ5BtnyP
1181279523993731000,Mon Oct 07 18:46:00 +0000 2019,An open letter to the Deep Learning Indaba team‚Äî2019 edition A great post by @alienelf that beautifully describes t‚Ä¶ https://t.co/HFtFSzTp27
1180942967026126800,Sun Oct 06 20:28:38 +0000 2019,@ArxivDaily It might be worth selecting papers not just based on influential authors but based on other attributes‚Ä¶ https://t.co/inInexaUaS
1180942310395322400,Sun Oct 06 20:26:02 +0000 2019,Daily Arxiv Radiostation Great initiative by Hang Chu @ArxivDaily: Your daily dose of arXiv now as a podcast, for C‚Ä¶ https://t.co/a7iVB4aevD
1180939621892595700,Sun Oct 06 20:15:21 +0000 2019,RT @prlz77: #ICLR2020 submissions on graph neural networks, NLP and robustness have the greatest growth. @iclr_conf @openreviewnet https://‚Ä¶
1180935692987318300,Sun Oct 06 19:59:44 +0000 2019,RT @eurnlp: Join us next Friday, October 11 either in person or online for a day full of NLP. All talks will be live streamed on our Facebo‚Ä¶
1180915072299864000,Sun Oct 06 18:37:48 +0000 2019,@Khipu_AI @DeepIndaba For existing conferences, I hope we'll see more wide-spread usage of live-streaming, remote p‚Ä¶ https://t.co/pWaFyJT1CL
1180914497869009000,Sun Oct 06 18:35:31 +0000 2019,@Khipu_AI @DeepIndaba ..to the personal level (collaboration, advice, mentorship). A key theme is to lower barriers‚Ä¶ https://t.co/rfzcqp42CS
1180913640540659700,Sun Oct 06 18:32:06 +0000 2019,Efforts to help counter-balance this inequality can range from the organizational level (conferences focusing on un‚Ä¶ https://t.co/RNf1HVL2gW
1180911505421807600,Sun Oct 06 18:23:37 +0000 2019,@dannimassi That's awesome!! Huge congrats, Daniela!! üéâ
1180911227037503500,Sun Oct 06 18:22:31 +0000 2019,@MarekRei @cainesap Countries outside of North America, Asia, and Europe‚Äîparticularly Africa and South America‚Äîare‚Ä¶ https://t.co/PNORTPNoLv
1180910760832180200,Sun Oct 06 18:20:40 +0000 2019,The Geographic Diversity of NLP Conferences üåç This blog post by @MarekRei &amp; @cainesap highlights arguably one of th‚Ä¶ https://t.co/h0PxdrmnEm
1180039457925402600,Fri Oct 04 08:38:25 +0000 2019,RT @barbara_plank: One week to go: the first #EurNLP2019 summit in London  @eurnlp #NLProc  13 Speakers: @NandoDF Vera Demberg @dirk_hovy @‚Ä¶
1180034639039225900,Fri Oct 04 08:19:16 +0000 2019,RT @barbara_plank: Really cool stuff happening in the AI, machine learning, NLP summer school landscape in üá™üá∫:  üáµüáπLxMLS (https://t.co/HJmDE‚Ä¶
1179502397917405200,Wed Oct 02 21:04:20 +0000 2019,@RihabGrs @GoogleDevExpert Congrats, Rihab! That's awesome!! The GDE community couldn't have a better addition! üéâ
1178580403898200000,Mon Sep 30 08:00:39 +0000 2019,The CodeSearchNet Challenge üë©‚Äçüíª, a benchmark for evaluating #MLonCode methods by @HamelHusain @GitHubEng  Includes:‚Ä¶ https://t.co/KF3pNGdksF
1177693604522987500,Fri Sep 27 21:16:50 +0000 2019,@BenjaminRosman That's awesome!! Congrats!! üéâüéâ
1177493280289747000,Fri Sep 27 08:00:49 +0000 2019,RT @stephxsher: Very excited to be working on this iteration of @full_stack_dl! Courses that offer this sort of hands-on guidance and focus‚Ä¶
1176928263479976000,Wed Sep 25 18:35:38 +0000 2019,RT @full_stack_dl: We're excited to announce our third bootcamp! Join instructors @pabbeel @sergeykarayev @josh_tobin_  and guest speakers‚Ä¶
1176511589639499800,Tue Sep 24 14:59:56 +0000 2019,RT @emnlp2019: The registration for EMNLP-IJCNLP 2019 is now open.  Early registration ends on October 10. See you in Hong Kong!  https://t‚Ä¶
1176132410402119700,Mon Sep 23 13:53:12 +0000 2019,RT @emnlp2019: Registration for EMNLP 2019 will open in a few days. In the meantime, you can have a look at the registration fees for the c‚Ä¶
1175808431388876800,Sun Sep 22 16:25:50 +0000 2019,RT @kbeguir: Live from #AiHackTunisia! Competitors from #Tunisia, #Algeria, #Morocco, #Bahre√Øn (and many more) singing ‚Äúit‚Äôs just the begin‚Ä¶
1175172425719459800,Fri Sep 20 22:18:34 +0000 2019,@yogarshi @umdclip @umdcs Congrats!
1174820982193893400,Thu Sep 19 23:02:03 +0000 2019,RT @eurnlp: The complete schedule of EurNLP 2019 is now available. We will have 12 high-quality talks + 1 keynote from amazing speakers, a‚Ä¶
1174809108169613300,Thu Sep 19 22:14:52 +0000 2019,RT @shebogholo: I recently trained a language model for #Swahili using @fastdotai and @PyTorch for ~130 mins. I consider this as a stepping‚Ä¶
1174744631461986300,Thu Sep 19 17:58:40 +0000 2019,RT @emnlp2019: We have published a list of hotels remote from the protesting areas and advise participants to book one of these hotels. Stu‚Ä¶
1174739295099326500,Thu Sep 19 17:37:28 +0000 2019,@ancatmara Good to know it found a new home. ‚ò∫Ô∏è
1174739295099326500,Thu Sep 19 17:37:28 +0000 2019,@ancatmara Good to know it found a new home. ‚ò∫Ô∏è
1174642450260025300,Thu Sep 19 11:12:38 +0000 2019,RT @ThomasScialom: Episodic Memory in Lifelong Language Learning  https://t.co/9DPQF2MnTr  Motivation: prevent catastrophic forgetting¬†in n‚Ä¶
1174406359808467000,Wed Sep 18 19:34:30 +0000 2019,RT @alienelf: Caling all African NLPers (or even African devs or ML students interested in NLP) üì¢ üí•ü§Ø  Let's put Africa on the NLP map üåç‚ô•Ô∏èüöÄ‚Ä¶
1173958804934934500,Tue Sep 17 13:56:04 +0000 2019,RT @eurnlp: All student travel grants have been allocated. We are looking forward to seeing all registrants and authors on October 11! http‚Ä¶
1173660041922142200,Mon Sep 16 18:08:54 +0000 2019,@tommasopasini91 @RNavigli Congratulations!! üéâ
1173521960980091000,Mon Sep 16 09:00:13 +0000 2019,New NLP News: Experimental results and error reporting, Ethics and NLP, Distillation vol. 2, SemEval 2020 https://t.co/QFXmWzR0no via @revue
1173503395107397600,Mon Sep 16 07:46:26 +0000 2019,RT @PfeiffJo: "What do Deep Networks Like to Read?" - Here, we uncover artifacts that are encoded in our models, by finetuning a pretrained‚Ä¶
1173348823789441000,Sun Sep 15 21:32:14 +0000 2019,The #BenderRule: On Naming the Languages We Study and Why It Matters Great article by @emilymbender @gradientpub on‚Ä¶ https://t.co/BiQKOy4lDJ
1173282461792579600,Sun Sep 15 17:08:32 +0000 2019,The slides of all talks from the NLP session at @DeepIndaba are now online, featuring talks from @RichardSocher‚Ä¶ https://t.co/IaBQKYF6uK
1172897942048518100,Sat Sep 14 15:40:35 +0000 2019,RT @emnlp2019: We are getting ready to open the registration for EMNLP-IJCNLP 2019 next week.
1172616705467723800,Fri Sep 13 21:03:03 +0000 2019,If you're based in South America (üá¶üá∑üáßüá∑ üá®üá± üá®üá¥ üá≤üáΩ üá∫üáæ only, though), consider taking part in this NLP challenge! ü•á&amp; ü•à‚Ä¶ https://t.co/kx3vNBZV0L
1172614497611198500,Fri Sep 13 20:54:17 +0000 2019,@kaushal316 @huggingface @explosion_ai @deepset_ai Thanks! Looks great! I hadn't seen this one. :)  Also totally fo‚Ä¶ https://t.co/PJDXXkto33
1172608724676030500,Fri Sep 13 20:31:20 +0000 2019,@huggingface @explosion_ai @deepset_ai @zalandoresearch @feedly @ai2_allennlp Here's a nice comparison of the targe‚Ä¶ https://t.co/48shSfaPJS
1172607926919487500,Fri Sep 13 20:28:10 +0000 2019,@huggingface @explosion_ai @deepset_ai - flair by @zalandoresearch: https://t.co/SuFwWPwnhE - transfer-nlp by‚Ä¶ https://t.co/U0ItLxmkrZ
1172607702884933600,Fri Sep 13 20:27:17 +0000 2019,It's great to see the growing landscape of NLP transfer learning libraries: - pytorch-transformers by @huggingface:‚Ä¶ https://t.co/UauTPDM7Nb
1172534789720723500,Fri Sep 13 15:37:33 +0000 2019,RT @kdnuggets: The State of Transfer Learning in NLP https://t.co/CflpLyknLM by @seb_ruder https://t.co/nadNmIpAQi
1172432853864337400,Fri Sep 13 08:52:29 +0000 2019,RT @conll2019: Accepted paper list is now available on our webpage: https://t.co/EoHoWEGaCT! Congrats again to all authors! Detailed progra‚Ä¶
1172431576551972900,Fri Sep 13 08:47:25 +0000 2019,RT @eurnlp: The list of accepted abstracts is now available on our website. Congratulations to all the authors!  https://t.co/BcIe0OXNAS
1171549480409325600,Tue Sep 10 22:22:17 +0000 2019,@emnlp2019 @p_czarnowska @EXGRV @ryandcotterell @anncopestake We finally propose a simple morphological constraint‚Ä¶ https://t.co/I4C0cI36AE
1171549248061661200,Tue Sep 10 22:21:21 +0000 2019,@emnlp2019 @p_czarnowska @EXGRV @ryandcotterell @anncopestake Furthermore, controlling for different factors, we ob‚Ä¶ https://t.co/IUcsP2NRAx
1171549050245660700,Tue Sep 10 22:20:34 +0000 2019,@emnlp2019 @p_czarnowska @EXGRV @ryandcotterell @anncopestake We conduct a comprehensive analysis of morphological‚Ä¶ https://t.co/PBZIytL2Pd
1171548832393572400,Tue Sep 10 22:19:42 +0000 2019,Don't Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction‚Ä¶ https://t.co/VmJOxr33cq
1171449140280119300,Tue Sep 10 15:43:34 +0000 2019,RT @emnlp2019: We have a great line-up of workshops at EMNLP-IJCNLP 2019 this year: Day 1: FEVER, DiscoMT, LANTERN, MSR, LOUHI, DeepLo, COI‚Ä¶
1171121594401546200,Mon Sep 09 18:02:01 +0000 2019,@jor_gracia Thanks! Glad you're finding it helpful. :)
1171049947384488000,Mon Sep 09 13:17:19 +0000 2019,RT @soul_blis: 5 Major open problems in NLP.  https://t.co/sHZ3SYKpxB  Have compiled 5 major problems/opportunities for students, researche‚Ä¶
1171049924424818700,Mon Sep 09 13:17:13 +0000 2019,@soul_blis Thanks!! üòçVery nice overview of topics. Particularly like the quotes and pointers to resources.
1171014335038206000,Mon Sep 09 10:55:48 +0000 2019,@alienelf In case you haven't seen it, this should be a useful recent paper to look at, with a good overview of the‚Ä¶ https://t.co/L2ltt25Bzv
1170799432931119000,Sun Sep 08 20:41:51 +0000 2019,RT @emnlp2019: We'd like to announce the keynote speakers for EMNLP-IJCNLP 2019: Meeyoung Cha (KAIST), Kyunghyun Cho @kchonyc (NYU &amp; FAIR),‚Ä¶
1170657813196148700,Sun Sep 08 11:19:07 +0000 2019,@paulxaus @omarsar0 @minimaxir @Thom_Wolf Depends what genre you're targeting. Stance detection or sentiment analys‚Ä¶ https://t.co/SNu6yEoT2B
1170643298618355700,Sun Sep 08 10:21:26 +0000 2019,@ikekong @DaniYogatama @NeurIPSConf @DeepMindAI 4/ In a concurrent paper (also accepted to @NeurIPSConf), Rolnick e‚Ä¶ https://t.co/AbCcAshLHv
1170642882493124600,Sun Sep 08 10:19:47 +0000 2019,@ikekong @DaniYogatama @NeurIPSConf 3/ Episodic memory and replay are closely inspired by mechanisms in the brain.‚Ä¶ https://t.co/JKTE0nxfxj
1170642453256376300,Sun Sep 08 10:18:04 +0000 2019,@ikekong @DaniYogatama @NeurIPSConf 2/ We argue that episodic memory (a key-value memory of past experiences) is a‚Ä¶ https://t.co/mQDGuVyrPX
1170642228592763000,Sun Sep 08 10:17:11 +0000 2019,1/ Our paper Episodic Memory in Lifelong Language Learning with Cyprien de Masson d'Autume, @ikekong, and‚Ä¶ https://t.co/uhqC9AxPo0
1169013329660170200,Tue Sep 03 22:24:31 +0000 2019,@kbeguir @instadeepai @thomas_pierrot @DeepMindAI @scott_e_reed @NandoDF @DeepIndaba That's awesome!! Congrats! üéâ
1168581521827143700,Mon Sep 02 17:48:40 +0000 2019,RT @delliott: There is a faculty-wide call at the University of Copenhagen for PhD positions, in which you define the topic. I'm happy to d‚Ä¶
1168235670701314000,Sun Sep 01 18:54:23 +0000 2019,RT @Smerity: I'm incredibly proud that the low compute / low resource AWD-LSTM and QRNN that I helped develop at @SFResearch live on as fir‚Ä¶
1168179099111702500,Sun Sep 01 15:09:35 +0000 2019,RT @alienelf: Calling all African NLPers! Goal: Let's publish a paper with NMT baselines for all African languages  Slides https://t.co/9rK‚Ä¶
1167811834436604000,Sat Aug 31 14:50:12 +0000 2019,@liu_zhiyong Thanks! I'm glad you found it useful. :)
1167770256200347600,Sat Aug 31 12:04:59 +0000 2019,RT @emnlp2019: We are happy to announce Childcare Grants offered to participants with young children. Grants of up to US$400 per family are‚Ä¶
1167750791886704600,Sat Aug 31 10:47:39 +0000 2019,RT @maximeamblard: with @kloeebt, we are looking for post-doc (OLKI project) on dialogue modeling apply to pathological data. The candidate‚Ä¶
1167599240069222400,Sat Aug 31 00:45:26 +0000 2019,RT @DeepMindAI: Congrats to all the Deep Learning Indaba organisers, speakers and attendees for another incredible event - making Africa‚Äôs‚Ä¶
1167428131470418000,Fri Aug 30 13:25:30 +0000 2019,RT @emnlp2019: Applications for travel scholarships and student volunteers are now open! We also plan to offer scholarships to students and‚Ä¶
1167203645324169200,Thu Aug 29 22:33:29 +0000 2019,RT @Sasha511: NLP for low resource languages with the legendary @seb_ruder of https://t.co/vyudkrkgIv at the @DeepIndaba ü§© #DLIndaba2019 #S‚Ä¶
1167111461212176400,Thu Aug 29 16:27:10 +0000 2019,@HermanKamper: Overall, for low-resource ASR, there are engineering/technical and scientific challenges. It is part‚Ä¶ https://t.co/yuB7yXVC5f
1167096840040394800,Thu Aug 29 15:29:04 +0000 2019,RT @Dr_Tempest: Mentoring on research proposals &amp; PhD applications at #DLIndaba2019 with @dannimassi @RichardSocher @seb_ruder @BenjaminRos‚Ä¶
1167096770121408500,Thu Aug 29 15:28:48 +0000 2019,It can also help us better understand human language acquisition. Three directions towards unsupervised ASR: 1. Inc‚Ä¶ https://t.co/oCWCtfAzAZ
1167093519883931600,Thu Aug 29 15:15:53 +0000 2019,@HermanKamper: Unsupervised ASR is a fundamental ML problem. It can be useful for query-by-example (IR based on spe‚Ä¶ https://t.co/oG2pBofuYC
1167091559671763000,Thu Aug 29 15:08:05 +0000 2019,For the last talk of the session, @HermanKamper discusses (outrageously) low-resource speech processing. Most comme‚Ä¶ https://t.co/HmuYFmASsI
1167088076478439400,Thu Aug 29 14:54:15 +0000 2019,Brian describes the Cocohub community that aims to crowdsource translations of MS-COCO captions to African languages https://t.co/BVv9wZMR20
1167087203467649000,Thu Aug 29 14:50:47 +0000 2019,RT @alienelf: Future directions in Cross-lingual embeddings üéâ Let's hif it  Advice: Don't do incremental work, LEAPFROG! üê∏ #SautiYetu #DLIn‚Ä¶
1167087090171089000,Thu Aug 29 14:50:20 +0000 2019,RT @alienelf: So in true support of the diverse NLP @seb_ruder is addressing what we do about the world being English-centric ü§©   A solutio‚Ä¶
1167086905189642200,Thu Aug 29 14:49:36 +0000 2019,Brian Muhia now discusses how to train an AWD-LSTM language model for Swahili and crowdsourcing translation and cap‚Ä¶ https://t.co/zcTlihrAO1
1167077422782722000,Thu Aug 29 14:11:55 +0000 2019,@alienelf: Publish your code, your datasets, and your results (at workshops and conferences)! Work together to trai‚Ä¶ https://t.co/G1zUmWz7iO
1167075732025221000,Thu Aug 29 14:05:12 +0000 2019,For applying MT to low-resource languages there are several directions. SMT &lt; properly optimized NMT. Unsupervised‚Ä¶ https://t.co/7JZlQUm19L
1167074283564220400,Thu Aug 29 13:59:27 +0000 2019,Problems facing African MT are: 1. Lack of focus 2. Reproducibility 3. No benchmarks  4. Low discoverability 5. Low‚Ä¶ https://t.co/UogkrJRx5V
1167072120813342700,Thu Aug 29 13:50:51 +0000 2019,In the second part of the NLP and speech processing session @DeepIndaba, @alienelf presents her journey and work on‚Ä¶ https://t.co/IBZUFM0YAY
1167022232247394300,Thu Aug 29 10:32:37 +0000 2019,@davidpfahler We'll try to make slides available after the session.
1167018444858040300,Thu Aug 29 10:17:34 +0000 2019,The NLP session @DeepIndaba concludes with a call to action by @alienelf: Let's put Africa on the NMT map! Train an‚Ä¶ https://t.co/eHFeVSvgiJ
1167016290978078700,Thu Aug 29 10:09:00 +0000 2019,Finally, @sgouws points to two important future research directions: 1. Improving sample / computational efficiency‚Ä¶ https://t.co/r5slnLxtuV
1167014334272348200,Thu Aug 29 10:01:14 +0000 2019,@sgouws reviews advances in the recent history of NLP including word embeddings, cross-lingual word embeddings, and‚Ä¶ https://t.co/DfJplklxj1
1167012504616210400,Thu Aug 29 09:53:57 +0000 2019,@sgouws: The neural sequence model toolbox consists of 5 core components: 1. Layers 2. Architectures 3. Frameworks‚Ä¶ https://t.co/pvolPZwOuV
1167011522473799700,Thu Aug 29 09:50:03 +0000 2019,How did we get here? Using platforms, frameworks, and datasets (on a higher level) and lots of components, hyperpar‚Ä¶ https://t.co/bR39lULCSx
1167010872159625200,Thu Aug 29 09:47:28 +0000 2019,@sgouws observes that we have made significant progress on core NLP tasks such as language modelling, machine trans‚Ä¶ https://t.co/w54uSpchma
1167009286033170400,Thu Aug 29 09:41:10 +0000 2019,For the last talk of the first NLP session, @sgouws reviews the evolution and successes of neural sequence models f‚Ä¶ https://t.co/RwUAxvw2jl
1167007131716980700,Thu Aug 29 09:32:36 +0000 2019,Finally, @vukosi emphasizes that doing social media NLP research is not limited to only working with text or networ‚Ä¶ https://t.co/WAGSBUT35q
1167006016371855400,Thu Aug 29 09:28:10 +0000 2019,Overall, we need to work more on African languages, release datasets, share work, teach, and do interdisciplinary r‚Ä¶ https://t.co/32qDf3YBVz
1167005028315517000,Thu Aug 29 09:24:15 +0000 2019,@vukosi stresses that we need to teach NLP courses in universities and find means to support such courses.‚Ä¶ https://t.co/7w9lzW7gj5
1167004443994448000,Thu Aug 29 09:21:55 +0000 2019,Wikipedia is a common source of data for many NLP tasks. However, Wikipedias are still very small (or non-existant)‚Ä¶ https://t.co/0aU0GD1Y97
1167003838664052700,Thu Aug 29 09:19:31 +0000 2019,Vukosi emphasizes that in order to further research on AI, we have to work together with other fields such as the s‚Ä¶ https://t.co/Y2c6yLODEP
1167002264021733400,Thu Aug 29 09:13:16 +0000 2019,In the second talk of the NLP session, @vukosi talks about challenges and opportunities for NLP in Africa üåç‚Ä¶ https://t.co/aEb0HC6prV
1166999114510475300,Thu Aug 29 09:00:45 +0000 2019,Explanations can be generated by fine-tuning a language model on a small number of human explanations. The explanat‚Ä¶ https://t.co/ZmqRX3SRvt
1166997889631043600,Thu Aug 29 08:55:53 +0000 2019,Finally, Richard discusses recent work on generating explanations in the form of commonsense reasoning: they create‚Ä¶ https://t.co/0rBVyLW5cj
1166993910327074800,Thu Aug 29 08:40:04 +0000 2019,Next, Richard describes follow-up work to decaNLP: instead of framing tasks as QA, we can frame many tasks such as‚Ä¶ https://t.co/VahwqCxP3F
1166991532467138600,Thu Aug 29 08:30:37 +0000 2019,@RichardSocher starts his talk on Unifying and Explaining NLP with a review of recent transfer learning progress in‚Ä¶ https://t.co/wrY9b9boeO
1166988354954629000,Thu Aug 29 08:18:00 +0000 2019,@HermanKamper kicks off the NLP session at the @DeepIndaba featuring talks from @RichardSocher @vukosi @sgouws‚Ä¶ https://t.co/MaDroq9Y3w
1166960507083010000,Thu Aug 29 06:27:20 +0000 2019,@RichardSocher gives a keynote on the next phase of NLP &amp; AI, which will involve multi-task learning, pretraining,‚Ä¶ https://t.co/2tw9xkkFTN
1166755864168022000,Wed Aug 28 16:54:09 +0000 2019,RT @dannimassi: #DlIndaba2019: Slides from last night's "How to write a great research proposal" (&amp; general tips for PhD applications) here‚Ä¶
1166751992137605000,Wed Aug 28 16:38:46 +0000 2019,RT @SanhEstPasMoi: There is a trend for huge Transformers. We went the other way: decreasing the size! ü§ó  Introducing DistilBERT: a smaller‚Ä¶
1166751290069831700,Wed Aug 28 16:35:59 +0000 2019,RT @alienelf: Amazing to meet @Kelechukwu_ and @orevaahia who are applying Unsupervised NMT to West African Pidgin (Creole) to English! So‚Ä¶
1166698629496803300,Wed Aug 28 13:06:44 +0000 2019,RT @alienelf: Some of our results :) Please join us tomorrow at the NLP sessions.   We have plans and they require as many people across th‚Ä¶
1166673541498122200,Wed Aug 28 11:27:02 +0000 2019,@LauraMartinus and @alienelf presenting on benchmarking MT for Southern African languages at the #DLIndaba2019‚Ä¶ https://t.co/0gJo68ZPNP
1166647742631952400,Wed Aug 28 09:44:31 +0000 2019,RT @tejuafonja: RNN practical - thanks @taliesinb @seb_ruder and all the volunteers instructors. üíï  #SautiYetu #DLIndaba2019 https://t.co/M‚Ä¶
1166032352629469200,Mon Aug 26 16:59:11 +0000 2019,RT @DeepIndaba: At the Women in AI evening at #DLIndaba2019 celebrating the role of women, the leaky pipeline and what systematic issues. #‚Ä¶
1165988516695871500,Mon Aug 26 14:05:00 +0000 2019,RT @sleepinyourhat: NYC-area undergrads: If you‚Äôre not studying computer science or AI, but you‚Äôd like to know a bit more about how current‚Ä¶
1165952211509268500,Mon Aug 26 11:40:44 +0000 2019,RT @julien_c: This quote from almost exactly a year ago's ‚ÄúNLP's ImageNet moment has arrived‚Äù by @seb_ruder:  IS JUST ABSOLUTE üíØ  https://t‚Ä¶
1165331025960087600,Sat Aug 24 18:32:22 +0000 2019,RT @emnlp2019: We expect EMNLP-IJCNLP 2019 will proceed as planned. #NLProc   See updates on conference organization in light of the curren‚Ä¶
1164263612522930200,Wed Aug 21 19:50:50 +0000 2019,@liu_zhiyong Sure, that's fine.
1164102832108257300,Wed Aug 21 09:11:57 +0000 2019,RT @payaicha15: To participate to indaba 2019 in Nairobi ,we initiate a gofundme with the support of @ Louis Martin . Thanks @adjiboussodie‚Ä¶
1164099901078343700,Wed Aug 21 09:00:19 +0000 2019,New NLP News: Bigger vs. smaller models, powerful vs. dumb models https://t.co/L39kE1V1TX via @revue
1163862704123195400,Tue Aug 20 17:17:46 +0000 2019,Looking forward to the @DeepIndaba next week! üåç  Let me know if you'd like to chat about transfer learning, cross-l‚Ä¶ https://t.co/Ge9kELQgDp
1163839009786929200,Tue Aug 20 15:43:37 +0000 2019,Applications for research scientist internships at DeepMind for 2020 are now open. If you want to work with the Lan‚Ä¶ https://t.co/DkhXKO8oLc
1163835546327834600,Tue Aug 20 15:29:51 +0000 2019,RT @supernlpblog: We're back with a scorching summary of ACL 2019 in Florence ‚òÄÔ∏è: https://t.co/538jzA6dbS
1163581400739700700,Mon Aug 19 22:39:58 +0000 2019,RT @jacobeisenstein: @seb_ruder Great post! We recently explored another transfer scenario: target domain has limited unlabeled data, and n‚Ä¶
1163136578702758000,Sun Aug 18 17:12:25 +0000 2019,New blog post: The State of Transfer Learning in NLP A review of key insights and takeaways from our NAACL 2019 tut‚Ä¶ https://t.co/mqpHPfZBKY
1162683671125667800,Sat Aug 17 11:12:43 +0000 2019,RT @emnlp2019: If you think you need a visa for attending EMNLP 2019, please check the information on our website and apply as soon as poss‚Ä¶
1162445450336714800,Fri Aug 16 19:26:07 +0000 2019,RT @DeepMindAI: Curious about AI and want to learn more? Introducing the DeepMind podcast! @FryRsquared will be taking you behind the scene‚Ä¶
1162437443565555700,Fri Aug 16 18:54:18 +0000 2019,RT @radamihalcea: My second (and final) top pick at #ACL2019nlp ‚Äî in the category of ‚Äúhigh societal impact‚Äù  NLP support in 11(!) languages‚Ä¶
1162407887228100600,Fri Aug 16 16:56:51 +0000 2019,RT @eurnlp: We have sent out notifications regarding submitted abstracts. The selection process was very competitive. Out of 179 valid subm‚Ä¶
1161673727811235800,Wed Aug 14 16:19:34 +0000 2019,@JAIR_Editor For a more in-depth review that covers the most recent models, you can check out our book Cross-lingua‚Ä¶ https://t.co/7ZgsUPI7tG
1161673081972301800,Wed Aug 14 16:17:00 +0000 2019,A Survey on Cross-lingual Word Embedding Models has been published in @JAIR_Editor. If you're interested in cross-l‚Ä¶ https://t.co/w6RFIs3Iru
1161669848264560600,Wed Aug 14 16:04:09 +0000 2019,@pfau @iamtrask Good point. I've been meaning to add them for a while but haven't gotten around to it.
1161537011246870500,Wed Aug 14 07:16:18 +0000 2019,RT @imoayadhsn: Help take these Sudanese students to the indaba!  @JeffDean @emilymbender @Ridhwana_K @kchonyc @KaliTessera @seb_ruder @mpd‚Ä¶
1161281145927065600,Tue Aug 13 14:19:35 +0000 2019,@vukosi cc @marzieh_saeidi
1161269427800891400,Tue Aug 13 13:33:01 +0000 2019,If you are concerned about misinformation and interested in fostering the truthfulness of online communication, con‚Ä¶ https://t.co/mIJcsCbtjP
1161265526800310300,Tue Aug 13 13:17:31 +0000 2019,@m_strise @eusondavid @carlosgr_nlp Congrats! :)
1161216641293803500,Tue Aug 13 10:03:16 +0000 2019,RT @emnlp2019: We are in the process of sending out the notification emails. Please do not check the START system now. This would slow down‚Ä¶
1160813712309149700,Mon Aug 12 07:22:10 +0000 2019,RT @sigrep_acl: Thanks again to all attendees, presenters &amp; invited speakers for making #Repl4NLP2019 a successful workshop for the 4th yea‚Ä¶
1160576688029339600,Sun Aug 11 15:40:19 +0000 2019,RT @alienelf: Busy prepping for my talk at the NLP workshop at @DeepIndaba on Machine Translation for African Languages! üåçüíö  So excited! ü§©‚Ä¶
1159471517853462500,Thu Aug 08 14:28:46 +0000 2019,RT @manaalfar: I'm hiring a research intern for Fall 2019 at Google NYC. Possible areas of research include multilinguality, semantics, dia‚Ä¶
1159386908654559200,Thu Aug 08 08:52:34 +0000 2019,Cool post by @hxiao, author of bert-as-a-service that highlights the increasing focus on pretrained models and end-‚Ä¶ https://t.co/bVb3Le1aWM
1159165307812024300,Wed Aug 07 18:12:00 +0000 2019,Reviewing one-page abstracts is a nice change of pace. Seems to be a good format for research proposals‚Äîsuccinct &amp;‚Ä¶ https://t.co/GKvcIa0d2W
1159129974705655800,Wed Aug 07 15:51:36 +0000 2019,RT @eurnlp: If you are reviewing for EurNLP, do not forget to enter your reviews until Friday, August 9, 2019 23:59 (CEST).
1159104120919396400,Wed Aug 07 14:08:52 +0000 2019,RT @aclmeeting: Up over 1000 followers, but surely there are more #nlproc people on Twitter than that! One  especially good reason to follo‚Ä¶
1159037591704019000,Wed Aug 07 09:44:30 +0000 2019,RT @licwu: The slides of our @ACL2019_Italy  tutorial on "Unsupervised Cross-Lingual Representation Learning" with @seb_ruder and Anders S√∏‚Ä¶
1158710043648438300,Tue Aug 06 12:02:56 +0000 2019,RT @sshkhr16: @seb_ruder Thanks a lot for sharing. I created this list while I was applying for #mlss2019 and realized that there are tons‚Ä¶
1158493081794224000,Mon Aug 05 21:40:49 +0000 2019,@EDUFIERRO There's https://t.co/Neu6H9YRJO Maybe create a PR @Khipu_AI? Others might know about more cc @eisenjulian @ta_tiennee
1158474314078982100,Mon Aug 05 20:26:14 +0000 2019,Summer schools are a great way to meet your peers, make friends, and learn from experts. If you're looking for ML s‚Ä¶ https://t.co/xhCYAG4TT0
1158436556375580700,Mon Aug 05 17:56:12 +0000 2019,Only a weekend after @ACL2019_Italy, there are already awesome reviews available on various topics: - Trends in NLP‚Ä¶ https://t.co/DUAJ46BNpX
1157565900729606100,Sat Aug 03 08:16:31 +0000 2019,RT @spacy_io: Say hello to spacy-pytorch-transformers!  üõ∏ BERT, XLNet &amp; GPT-2 in your spaCy pipeline ü§ó Based on @HuggingFace's pytorch-tran‚Ä¶
1157382494519517200,Fri Aug 02 20:07:44 +0000 2019,@gentaiscool Thanks, but it was a team effort led by @IAugenstein and including @gspandana, @kelina1124, Burcu Can,‚Ä¶ https://t.co/1bZc4S5mds
1157379221968380000,Fri Aug 02 19:54:44 +0000 2019,@WilliamWangNLP @UCSBengineering That's awesome, congrats!!
1157379126937968600,Fri Aug 02 19:54:21 +0000 2019,@daniilmagpie Congrats!!
1157318969977704400,Fri Aug 02 15:55:19 +0000 2019,The RepL4NLP workshop concludes with 3 best paper awards and a massive thanks to everyone involved! @sigrep_acl‚Ä¶ https://t.co/0UUilrKqF1
1157285772267085800,Fri Aug 02 13:43:24 +0000 2019,@mattthemathman presenting our paper To Tune or Not to Tune at the poster session of the RepL4NLP workshop‚Ä¶ https://t.co/Esg3xiSIJG
1157272441644826600,Fri Aug 02 12:50:25 +0000 2019,RT @nimirea_: hey @ACL_NLP @ACL2019_Italy let's talk accessibility  this venue was NOT wheelchair accessible. will it be different at other‚Ä¶
1157269920654200800,Fri Aug 02 12:40:24 +0000 2019,The paper that these relate to and build upon:  Von Mises-Fisher Loss for Training Sequence to Sequence Models with‚Ä¶ https://t.co/6WfA4PMnPc
1157269440813305900,Fri Aug 02 12:38:30 +0000 2019,Interesting research directions for phrase-based continuous output generation including one-to-many, and MWEs, synt‚Ä¶ https://t.co/URCemU3GvW
1157265985897992200,Fri Aug 02 12:24:46 +0000 2019,Yulia Tsvetkov talks about continuous modeling of output spaces (vs. a discrete softmax) at RepL4NLP @sigrep_acl‚Ä¶ https://t.co/65BCMMynHt
1157236859040338000,Fri Aug 02 10:29:02 +0000 2019,RT @sigrep_acl: #Repl4NLP2019 has started in Hall 1. 400 registered participants -- the jointly largest workshop at #acl2019 #acl2019acl Do‚Ä¶
1157217906599649300,Fri Aug 02 09:13:43 +0000 2019,RT @hadyelsahar: Marco Baroni is starting the first invited talk at #rep4nlp  "Language is representations by itself. " #ACL2019nlp @sigrep‚Ä¶
1157194437686124500,Fri Aug 02 07:40:28 +0000 2019,@IAugenstein kicking off the 4th RepL4NLP workshop at @ACL2019_Italy with a brief overview of the workshop‚Ä¶ https://t.co/uz6bEmyZff
1156952994652610600,Thu Aug 01 15:41:03 +0000 2019,If you're still around at @ACL2019_Italy on Friday, join us for a panel discussion on big open problems in represen‚Ä¶ https://t.co/fftW6608om
1156952588132200400,Thu Aug 01 15:39:26 +0000 2019,RT @sigrep_acl: Join us at the 4th RepL4NLP workshop tomorrow! We'll have a great program with invited talks by Marco Baroni, Yulia Tsvetko‚Ä¶
1156914787688898600,Thu Aug 01 13:09:14 +0000 2019,RT @anotherjohng: If you're at ACL today, drop by WMT to see some new work from Aylien research being presented by lead author @Mystical_Wi‚Ä¶
1156595214104125400,Wed Jul 31 15:59:22 +0000 2019,RT @sjmielke: Meeting of the #NLProc Sebastians @ACL2019_Italy! üòç  Left to right: @nlpado @seb_ruder @sebp992 @sebastianarnold @sjmielke @r‚Ä¶
1156550202620334000,Wed Jul 31 13:00:30 +0000 2019,@suzatweet Thanks for the kind words! üòç
1156550079450423300,Wed Jul 31 13:00:01 +0000 2019,RT @suzatweet: I'll spend my summer reading this. A-mazing. https://t.co/vvaQxgsbzY
1155790111264694300,Mon Jul 29 10:40:10 +0000 2019,@F_Delahunty Sure. Just say hi when you see me.
1155524783473397800,Sun Jul 28 17:05:51 +0000 2019,RT @artetxem: I am in Florence to attend #acl2019nlp! Let me know if you would like to chat about cross-lingual learning, unsupervised mach‚Ä¶
1155491126587117600,Sun Jul 28 14:52:07 +0000 2019,RT @Hrant25: Multilingual word alignment seems to work better with supervision @seb_ruder  #acl2019nlp https://t.co/2yiDcbB3Tr
1155454944046198800,Sun Jul 28 12:28:20 +0000 2019,RT @PMinervini: Full room for @licwu giving a really cool tutorial on Unsupervised Cross Lingual Representation Learning with @seb_ruder! #‚Ä¶
1155450876359794700,Sun Jul 28 12:12:10 +0000 2019,RT @mageed: The tutorial on Unsupervised Cross-lingual learning. @seb_ruder, Andreas Soggard, and Ivan Vulic. An important topic. #ACL2019‚Ä¶
1155449885329645600,Sun Jul 28 12:08:14 +0000 2019,@licwu kicking off our tutorial on unsupervised cross-lingual learning @ACL2019_Italy #acl2019nlp  Slides:‚Ä¶ https://t.co/mJUmbGC664
1155217362519564300,Sat Jul 27 20:44:16 +0000 2019,@alienelf You typically want to use Y as a pivot language. Have a look at this ACL 2019 paper and its related work‚Ä¶ https://t.co/p6b5mjuldq
1155076907072970800,Sat Jul 27 11:26:09 +0000 2019,@fromamine @DeepMindAI Sure. Just send me an email with your questions.
1155068152042139600,Sat Jul 27 10:51:22 +0000 2019,This is a great set of tips for first-timers attending @ACL2019_Italy next week (or any big ML conference) by‚Ä¶ https://t.co/keANgtByJ7
1154687724877754400,Fri Jul 26 09:39:41 +0000 2019,RT @artetxem: Our paper "Bilingual Lexicon Induction through Unsupervised Machine Translation" (w/ @glabaka &amp; @eagirre) has been nominated‚Ä¶
1154519291124826000,Thu Jul 25 22:30:23 +0000 2019,RT @aggielaz: Almost there for @ACL2019_Italy. If you want to chat about pasta, wine, coffee (only espresso) and brioche/cornetto (not the‚Ä¶
1154516076232880000,Thu Jul 25 22:17:37 +0000 2019,@nzhiltsov @Thom_Wolf @swabhz @mattthemathman Very nice article! Happy to share it. :)
1154515971190579200,Thu Jul 25 22:17:12 +0000 2019,NAACL ‚Äô19 Notes: Practical Insights for NLP Applications ‚Äî Part I In-depth summary of @NAACLHLT highlights by‚Ä¶ https://t.co/73GGeAswGW
1154426459831918600,Thu Jul 25 16:21:30 +0000 2019,RT @dirk_hovy: Getting ready for @ACL2019_Italy. If you want to talk about #nlproc, fairness, bias, or computational social science, come f‚Ä¶
1154296824787849200,Thu Jul 25 07:46:23 +0000 2019,RT @sigrep_acl: As is tradition, we will have a star-studded panel at #Repl4NLP2019 #acl2019 What questions / issues do you think the panel‚Ä¶
1154071829155328000,Wed Jul 24 16:52:20 +0000 2019,@maryama59767955 @ACL2019_Italy @DeepMindAI @licwu The tutorial should be recorded as far as I'm aware.
1154071742853369900,Wed Jul 24 16:51:59 +0000 2019,@llotus_eater @ACL2019_Italy @DeepMindAI Sure. :) Just ping me when you're there and we'll find a time.
1154071198319423500,Wed Jul 24 16:49:49 +0000 2019,@ahammami0 @ACL2019_Italy @DeepMindAI Sorry about that. I'm behind on emails due to travels, so bear with me.
1154063721477619700,Wed Jul 24 16:20:07 +0000 2019,@ACL2019_Italy @DeepMindAI I'll also be giving a tutorial with @licwu &amp; Anders S√∏gaard on unsupervised cross-lingua‚Ä¶ https://t.co/50nra2riYc
1154063145947807700,Wed Jul 24 16:17:50 +0000 2019,I'll be at @ACL2019_Italy next week. Let me know if you'd like to chat about transfer learning, cross-lingual learn‚Ä¶ https://t.co/4FxwFtexMI
1153730738606477300,Tue Jul 23 18:16:58 +0000 2019,RT @sigrep_acl: The complete schedule of the RepL4NLP workshop at @ACL2019_Italy is now online. We'll have an awesome program including inv‚Ä¶
1153585557521518600,Tue Jul 23 08:40:04 +0000 2019,@migballesteros Congrats, Miguel!
1153228251264954400,Mon Jul 22 09:00:15 +0000 2019,New NLP News: NLP in Industry, Leaderboard madness, https://t.co/Uax3FSbOBV NLP, Transfer learning tools https://t.co/jgGfAaYukG via @revue
1153040628638453800,Sun Jul 21 20:34:42 +0000 2019,RT @eurnlp: We received an impressive number of 187 (!) abstract submissions for the first EurNLP Summit. We are fortunate to be able to as‚Ä¶
1152882515130142700,Sun Jul 21 10:06:25 +0000 2019,The New Era of NLP (SciPy 2019 Keynote): This is a great presentation by @math_rachel that focuses on transfer lear‚Ä¶ https://t.co/EE0uLrANe1
1152177599612895200,Fri Jul 19 11:25:20 +0000 2019,@sudharsansai123 @DeepMindAI You can find my email on my website.
1151849897781551100,Thu Jul 18 13:43:10 +0000 2019,@WeboAR @DeepMindAI No.
1151849749370331100,Thu Jul 18 13:42:35 +0000 2019,@AhmedBahaaElD18 @DeepMindAI Internship applications open in August/September, but are generally for students doing‚Ä¶ https://t.co/yHOlV3hhRr
1151849497401729000,Thu Jul 18 13:41:35 +0000 2019,@ankitXdhankhar @DeepMindAI Yes.
1151815605907054600,Thu Jul 18 11:26:54 +0000 2019,@aliebrahiiimi @NAACLHLT @Thom_Wolf @swabhz @mattthemathman It's not yet available as far as I know. I'll share once it's uploaded.
1151776361847107600,Thu Jul 18 08:50:58 +0000 2019,RT @nstrodt: Excited to share our recent preprint https://t.co/19D7q4mL6E turns out that language model pretraining a la ULMFiT by @seb_rud‚Ä¶
1151776198567059500,Thu Jul 18 08:50:19 +0000 2019,@rjurney @NirantK ‚ò∫Ô∏è Thanks!
1151621776356630500,Wed Jul 17 22:36:42 +0000 2019,RT @sleepinyourhat: Hi world! I'm hiring a full-time research engineer. If you're interested in transitioning from software engineering to‚Ä¶
1151598583050854400,Wed Jul 17 21:04:32 +0000 2019,@PySanjeevi Have a look at some of these resources: https://t.co/kWNYpIsy7M
1151596494887227400,Wed Jul 17 20:56:14 +0000 2019,If you're doing anything with NLP, this is a great place to start! A PyTorch library of state-of-the-art pretrained‚Ä¶ https://t.co/YbzX2ez0hE
1151570477523116000,Wed Jul 17 19:12:51 +0000 2019,@RyanEloff @DeepMindAI Sure. :)
1151554688308957200,Wed Jul 17 18:10:07 +0000 2019,@xennygrimmato_ @DeepMindAI @ACL2019_Italy Email is good.
1151532461404434400,Wed Jul 17 16:41:47 +0000 2019,@moinnadeem @DeepMindAI Sure, just send me an email.
1151532162879017000,Wed Jul 17 16:40:36 +0000 2019,@stringharsh @DeepMindAI @ACL2019_Italy Best would be to send me an email.
1151522245325574100,Wed Jul 17 16:01:12 +0000 2019,@DeepMindAI If you have any questions, send me a message or chat to us at @ACL2019_Italy.
1151521975866667000,Wed Jul 17 16:00:07 +0000 2019,Are you excited about building models that can understand language? Do you want to understand how humans acquire la‚Ä¶ https://t.co/jxi6GwoA2e
1151202096806617100,Tue Jul 16 18:49:02 +0000 2019,RT @math_rachel: My #SciPy2019 keynote: The New Era of NLP  https://t.co/7ELrqzoRpZ https://t.co/PVYXN0piCP
1150887893675720700,Mon Jul 15 22:00:30 +0000 2019,See below for 12 fantastic talks about NLP research and applications featuring @yoavgo, @MarkNeumannnn, @OxyKodit,‚Ä¶ https://t.co/bpISj7UdXT
1150667758176280600,Mon Jul 15 07:25:46 +0000 2019,RT @IAugenstein: The Dept of Computer Science at the University of Copenhagen is currently seeking to recruit a TT Asst Prof (https://t.co/‚Ä¶
1150480750652919800,Sun Jul 14 19:02:40 +0000 2019,RT @radamihalcea: Calling all mentors! If you are attending @ACL2019_Italy &amp; you have previously attended NLP conferences and/or are a seni‚Ä¶
1149953092445757400,Sat Jul 13 08:05:56 +0000 2019,@CamachoCollados @S_Wales_Chess Congrats! That's awesome!! üëè
1149937187645022200,Sat Jul 13 07:02:44 +0000 2019,RT @DaniYogatama: First time co-organizing a large event and what an experience it was. So thankful to have the best co-organizers (@wittaw‚Ä¶
1149646023079280600,Fri Jul 12 11:45:45 +0000 2019,@flowing @Wadenschwinger @DeepMindAI Totally. Thanks for catching that!
1149412334814867500,Thu Jul 11 20:17:10 +0000 2019,RT @Wadenschwinger: Great talk today by @seb_ruder on cross-lingual and continual transfer learning in NLP @DeepMindAI https://t.co/kPWneDy‚Ä¶
1149042017730814000,Wed Jul 10 19:45:39 +0000 2019,RT @cognition_x: üèÜ #CogX19 Outstanding Contribution in AI for PhD Thesis goes to Sebastian Ruder for his work in #NLP - you can read more a‚Ä¶
1148502620333588500,Tue Jul 09 08:02:17 +0000 2019,RT @_inesmontani: Like many of you, I'm incredibly disappointed by DataCamp. I wanted to make a free version of my spaCy course so you don'‚Ä¶
1148500602143555600,Tue Jul 09 07:54:16 +0000 2019,RT @eurnlp: Due to an error (parsing time zones is hard), we had closed the submission early. We have now reopened. It will stay open for a‚Ä¶
1148326200999338000,Mon Jul 08 20:21:15 +0000 2019,RT @eurnlp: As we have received a large number of registrations, you will be put on a waitlist if you register now. You will receive a noti‚Ä¶
1148277806712967200,Mon Jul 08 17:08:57 +0000 2019,RT @iamtrask: I am *very* excited to share the first RAAIS OpenMined Grants, funded by https://t.co/1Uq3iJBtAK  Paid Open-Source #Privacy P‚Ä¶
1148270514466697200,Mon Jul 08 16:39:59 +0000 2019,RT @math_rachel: Excited to share our newest https://t.co/ktYtgBpxGr course: A Code-First Introduction to Natural Language Processing All c‚Ä¶
1148256599682797600,Mon Jul 08 15:44:41 +0000 2019,@mihail_eric Thanks for this, Mihail! Nice work! üëè
1148256516291649500,Mon Jul 08 15:44:21 +0000 2019,RT @mihail_eric: Want to stay up-to-date on your #nlproc literature but don‚Äôt have the time to read each paper? Np, I got you! üòè  I added s‚Ä¶
1148214640196890600,Mon Jul 08 12:57:57 +0000 2019,@RABawden Congrats, Rachel!! üéâ
1147833655785640000,Sun Jul 07 11:44:03 +0000 2019,If you're at the @DeepIndaba in Nairobi this year, join us for the NLP Session on August 29. We'll have an amazing‚Ä¶ https://t.co/0C1eKtbdKN
1147626203081302000,Sat Jul 06 21:59:43 +0000 2019,Thanks for such a great event, @_inesmontani and @honnibal and thanks for having me! It was a blast‚Äîand a particula‚Ä¶ https://t.co/iQZapTBKN9
1147616704463351800,Sat Jul 06 21:21:58 +0000 2019,RT @yoavgo: Thanks @honnibal and @_inesmontani for organizing #spacyIRL and for inviting me, I had a great time.   It was superbly organize‚Ä¶
1147538879832055800,Sat Jul 06 16:12:43 +0000 2019,@honnibal and @_inesmontani on the future of @spacy_io: new features, focus on data structures and pipelines, focus‚Ä¶ https://t.co/GKqYSJ6yMP
1147484805623365600,Sat Jul 06 12:37:51 +0000 2019,@MarkNeumannnn on developing scispacy, spacy for the biomedical domain: biomedical NLP, robustness, and spacy for o‚Ä¶ https://t.co/qNN8qF7RWg
1147470371517534200,Sat Jul 06 11:40:30 +0000 2019,@yoavgo on (some of the) missing elements in NLP. Future vision: humans writing rules aided by ML. #spaCyIRL https://t.co/a79mhmewTa
1147440104023167000,Sat Jul 06 09:40:13 +0000 2019,Peter Baumgartner on metaphors for applied ML: Companies hire specialists that can make kitchen appliances (build a‚Ä¶ https://t.co/BIjwkvXQ5K
1147435470919483400,Sat Jul 06 09:21:49 +0000 2019,RT @arnicas: .@seb_ruder release your models - it‚Äôs expensive to make them, environmentally. #spaCyIRL https://t.co/5VA7ay3NlU
1147063650806632400,Fri Jul 05 08:44:20 +0000 2019,@carsonkahn We encourage participation of startups and would love to see some of the European startups in the space‚Ä¶ https://t.co/CsNNrQcT8b
1147061624483778600,Fri Jul 05 08:36:17 +0000 2019,This is going to be a great opportunity to listen to high-quality speakers, get feedback from experts, and meet wit‚Ä¶ https://t.co/0UiImIKebv
1147056120516751400,Fri Jul 05 08:14:25 +0000 2019,RT @riedelcastro: @eurnlp @seb_ruder This is going to be a great event! Cannot stress enough: if you submit an abstract and it gets accepte‚Ä¶
1146891703967801300,Thu Jul 04 21:21:05 +0000 2019,RT @eurnlp: We are happy to announce that registration for EurNLP is open now: https://t.co/ySQ7JkAi6o Travel grants are available. One stu‚Ä¶
1146131454709129200,Tue Jul 02 19:00:07 +0000 2019,RT @eurnlp: The abstract submission deadline for EurNLP is in less than a week, on July 8. Submitting an abstract is a great way to present‚Ä¶
1145976885844815900,Tue Jul 02 08:45:55 +0000 2019,RT @riedelcastro: Heads up: the abstract submission deadline for https://t.co/HGjIOmUwMf is June 30! Submission highly recommended, not onl‚Ä¶
1144875156281724900,Sat Jun 29 07:48:02 +0000 2019,RT @jeremyphoward: Thank you everyone for your patience - our new deep learning MOOC is here! Includes 5 lessons diving into the foundation‚Ä¶
1144602317062332400,Fri Jun 28 13:43:52 +0000 2019,RT @Khipu_AI: Friendly reminder that today is the last day to apply for #khipu2019! Registration is free for students, and applicants can a‚Ä¶
1144586699718176800,Fri Jun 28 12:41:49 +0000 2019,@arankomatsuzaki @riedelcastro @eurnlp Sure, you can submit to both. For EurNLP, travel grants will likely only be‚Ä¶ https://t.co/U1jVZM6fSj
1144373216061939700,Thu Jun 27 22:33:30 +0000 2019,@laure_delisle @superrzk @AnimaAnandkumar @dannimassi Sure. Just DM me the details.
1144191602711310300,Thu Jun 27 10:31:50 +0000 2019,RT @OriolVinyalsML: Reminder that applications to @Khipu_AI close this Friday. EVERYONE can / should apply! (speakers @chelseabfinn @kchony‚Ä¶
1143968753018908700,Wed Jun 26 19:46:19 +0000 2019,RT @lilianweng: Meta RL is a great idea üí°: After trained over a distribution of tasks, an agent is able to solve a new task by developing a‚Ä¶
1143547511124181000,Tue Jun 25 15:52:27 +0000 2019,@daniebrant @ibrahimygana @PromisePreston I'd encourage you to take a look at these: https://t.co/K9eYM6JNBr There'‚Ä¶ https://t.co/oycTgAcLpY
1143297797648764900,Mon Jun 24 23:20:11 +0000 2019,@elmelis Congrats! üéâ
1143081388519952400,Mon Jun 24 09:00:15 +0000 2019,New NLP News‚ÄîBERT, GPT-2, XLNet, NAACL, ICML, arXiv, EurNLP https://t.co/4URmn0kd9e (via @revue)
1142896621702844400,Sun Jun 23 20:46:03 +0000 2019,RT @MatthewTeschke: Improving ULMFiT by up to 20%? See how we added metadata to ULMFiT models to significantly improve accuracy: https://t.‚Ä¶
1142789160371150800,Sun Jun 23 13:39:02 +0000 2019,RT @Miles_Brundage: I've been thinking a bit about the growing practice of fine-tuning generic pretrained models: first in computer vision,‚Ä¶
1142758183213174800,Sun Jun 23 11:35:56 +0000 2019,If you're a student and would like to attend the first annual EurNLP Summit, then the best way is to submit an abst‚Ä¶ https://t.co/OY35QgJZfG
1142104745252986900,Fri Jun 21 16:19:25 +0000 2019,RT @DeepMindAI: There's still time to apply to @Khipu_AI, an AI teaching conference designed to help strengthen the AI community across Lat‚Ä¶
1141290081346568200,Wed Jun 19 10:22:14 +0000 2019,RT @parsaghaffari: Excited to announce that we've expanded our benefits package yet again to make @_aylien a great place to work at!  Check‚Ä¶
1141263927113916400,Wed Jun 19 08:38:18 +0000 2019,RT @emnlp2019: Since the review process has started, authors now cannot withdraw their submissions directly in START. If you‚Äôd like to with‚Ä¶
1141046377453105200,Tue Jun 18 18:13:50 +0000 2019,RT @aidanematzadeh: Here are our #naacl2019 tutorial slides: https://t.co/557JdSSRJ4  (Language acquisition by me, followed by language pro‚Ä¶
1141003941905391600,Tue Jun 18 15:25:13 +0000 2019,@kchonyc Thanks for sharing this. Really glad you are ok.
1140903801056108500,Tue Jun 18 08:47:17 +0000 2019,@nmstoker Great to meet you too! ‚ò∫Ô∏è
1140268373168853000,Sun Jun 16 14:42:19 +0000 2019,@Tanmoy_Chak @emnlp2019 @ACL2019_Italy We're currently finalizing reviewer assignment and will inform reviewers soon when review starts.
1140260459670638600,Sun Jun 16 14:10:53 +0000 2019,RT @emnlp2019: If you are a reviewer for EMNLP-IJCNLP 2019, please check our instructions for reviewers before reviewing. We adopted the re‚Ä¶
1139518590573523000,Fri Jun 14 13:02:57 +0000 2019,Thanks to @iatitov &amp; @EdinburghNLP for inviting me to give a talk. Thanks to @_dmh for live tweeting. See the threa‚Ä¶ https://t.co/envHeg4zmE
1139513617437659100,Fri Jun 14 12:43:12 +0000 2019,RT @EdinburghNLP: Sebastian Ruder is giving a talk about transfer learning in NLP and beyond to a full room at @EdinburghNLP https://t.co/H‚Ä¶
1139513243624284200,Fri Jun 14 12:41:43 +0000 2019,RT @artetxem: 1/4 New @ACL2019_Italy paper by our awesome student @aormazabalo on the limitations of cross-lingual word embedding mappings‚Ä¶
1139230959402926100,Thu Jun 13 18:00:01 +0000 2019,New blog post: My @NAACLHLT 2019 highlights‚Äîtransfer learning, common sense reasoning, natural language generation,‚Ä¶ https://t.co/PcrQFRJJsb
1139214451587080200,Thu Jun 13 16:54:25 +0000 2019,We have some amazing speakers at the first European NLP Submit (EurNLP) including @NandoDF @natschluter @dirk_hovy‚Ä¶ https://t.co/fpJ5Tk8cwJ
1139164118194294800,Thu Jun 13 13:34:25 +0000 2019,@_josh_meyer_ @lorenlugosch Depending on how hard your task is, I'd still say yes as it can given you a useful indu‚Ä¶ https://t.co/YincAJJyFF
1139145545321189400,Thu Jun 13 12:20:36 +0000 2019,@_josh_meyer_ If you have already have enough samples, then most transfer or semi-supervised learning techniques wo‚Ä¶ https://t.co/CXnlRR641z
1138494455760265200,Tue Jun 11 17:13:25 +0000 2019,I'm super happy that our book on Cross-lingual Word Embeddings with Anders S√∏gaard, @licwu, &amp; @manaalfar is now ava‚Ä¶ https://t.co/kDuW94ELQa
1138470759372853200,Tue Jun 11 15:39:15 +0000 2019,RT @jeremyphoward: Lots of requests to @seb_ruder &amp; I for full replication details for ULMFiT on IMDb. Here it is! And thanks to @GuggerSyl‚Ä¶
1138455205094338600,Tue Jun 11 14:37:27 +0000 2019,In our new paper (my first collaboration at DeepMind, yay!) with Cyprien, @ikekong, &amp; @DaniYogatama, we leverage ep‚Ä¶ https://t.co/FT1kmKnp93
1138363892021768200,Tue Jun 11 08:34:36 +0000 2019,RT @sigrep_acl: The list of accepted papers for RepL4NLP at @ACL2019_Italy is now available. Overall, we've accepted 39 out of 88 submissio‚Ä¶
1138023056255922200,Mon Jun 10 10:00:14 +0000 2019,RT @yanaiela: Our (w/ @yoavgo) new task and dataset on Missing Elements, and more specifically Numeric Fused-Head are now on #nlpprogress,‚Ä¶
1138016131938230300,Mon Jun 10 09:32:43 +0000 2019,RT @licwu: Happy to share that our book on "Cross-lingual Word Embeddings" has seen the light of day! Massive thanks to @MorganClaypool and‚Ä¶
1137796046069915600,Sun Jun 09 18:58:11 +0000 2019,If you're looking for a faculty position, consider applying to this amazing opportunity! Copenhagen is a great city‚Ä¶ https://t.co/Y1JctapgeX
1137122670418550800,Fri Jun 07 22:22:26 +0000 2019,RT @alexandraxron: Thanks to everyone who attended my talk at #naacl2019 for "An Embarrassingly Simple Approach for Transfer Learning from‚Ä¶
1137118147490791400,Fri Jun 07 22:04:27 +0000 2019,@LysandreJik @srush_nlp @Tetreault_NLP It was great meeting you! See you at future conferences! :)
1136714936992915500,Thu Jun 06 19:22:14 +0000 2019,RT @Thom_Wolf: Fascinating talk by Sash Rush (@srush_nlp) on pre-trained models for conditional language generation.  If you followed our T‚Ä¶
1136361625005514800,Wed Jun 05 19:58:18 +0000 2019,@ravfogel: Key takeaway: not all languages are English! Work on non-English languages to be able to accurately char‚Ä¶ https://t.co/zBVQ8VGbdI
1136361377566773200,Wed Jun 05 19:57:19 +0000 2019,@ravfogel @ravfogel: - limitations: natural languages are intricate (exceptions to word order, order of non-core ar‚Ä¶ https://t.co/tV43ZTmpYH
1136360892411592700,Wed Jun 05 19:55:23 +0000 2019,@ravfogel: - word order matters: performance is higher in subject-verb-object order (as in English) than in subject‚Ä¶ https://t.co/UKJsNkn6CS
1136360451045056500,Wed Jun 05 19:53:38 +0000 2019,Creating corpus with different word orders requires repeatedly swapping complements https://t.co/wo1MCUMhuY
1136360205078454300,Wed Jun 05 19:52:40 +0000 2019,@ravfogel: - for polypersonal agreement, lower performance on object vs. subject prediction; jointly predicting of‚Ä¶ https://t.co/fhZyTm0yf8
1136359438556762100,Wed Jun 05 19:49:37 +0000 2019,@ravfogel @ravfogel: - given a parsed corpus, generate artificial languages that differ in either agreement, word o‚Ä¶ https://t.co/EZQOXX4uZe
1136359161216806900,Wed Jun 05 19:48:31 +0000 2019,@ravfogel: - naive option would be to just compare raw performance; however: not clear how to compare as languages‚Ä¶ https://t.co/l9WH0i4VT2
1136358948934692900,Wed Jun 05 19:47:40 +0000 2019,@ravfogel: - Previous work: case study on Basque https://t.co/7vrTCssR6i - LSTMs perform substantially worse on Bas‚Ä¶ https://t.co/1enNc3puhP
1136358715064496100,Wed Jun 05 19:46:44 +0000 2019,@ravfogel: - agreement prediction was first done in https://t.co/m8aLyQZ121 for English - follow-up work showed com‚Ä¶ https://t.co/9bfUKDsgvY
1136358259139534800,Wed Jun 05 19:44:56 +0000 2019,Shauli Ravfogel @ravfogel: - Main goals: study how RNNs acquire syntax, whether they find some syntactic features m‚Ä¶ https://t.co/jb8N2HvsmW
1136357602915471400,Wed Jun 05 19:42:19 +0000 2019,For the last paper of our session, we have a systematic study of the inductive biases of RNNs across (synthetic) la‚Ä¶ https://t.co/RrZf8jPXfS
1136357139734306800,Wed Jun 05 19:40:29 +0000 2019,Han: - visualization of task utility reveals that for the primary task SST-2, the model assigns the highest utility‚Ä¶ https://t.co/AkwkWFzS1W
1136356371539157000,Wed Jun 05 19:37:26 +0000 2019,Han: - for results, model outperforms single-task and standard multi-task baseline - in terms of aux tasks, MultiNL‚Ä¶ https://t.co/8MMssOxWQg
1136355919166693400,Wed Jun 05 19:35:38 +0000 2019,Han: - for learning the mixing ratio, use Bayesian optimization based on a Gaussian process (very sample-efficient)‚Ä¶ https://t.co/CQyDYztfLk
1136355136333328400,Wed Jun 05 19:32:31 +0000 2019,In a picture: https://t.co/wWX2GZ3O3G
1136355011527622700,Wed Jun 05 19:32:01 +0000 2019,Han:  - frame task selection as a multi-armed bandit; each task has a utility (prior is parameterized by Beta distr‚Ä¶ https://t.co/SDTJ7Zgd0Z
1136354557565526000,Wed Jun 05 19:30:13 +0000 2019,Han:  - standard setup for multitask learning; model uses ELMo embeddings, has different task-specific output layers https://t.co/RPxwwyLEC6
1136354305445978100,Wed Jun 05 19:29:13 +0000 2019,Lots of related work: https://t.co/jE40qBv9gM
1136354090785620000,Wed Jun 05 19:28:22 +0000 2019,Han: - Propose AutoSeM: 1. use a multi-armed bandit for auxiliary task selection; and 2. Bayesian optimization base‚Ä¶ https://t.co/6fP03S724w
1136353634386632700,Wed Jun 05 19:26:33 +0000 2019,Han Guo:  - In multi-task learning (MTL), we use information from related tasks to improve generalization performan‚Ä¶ https://t.co/9eeCda4Zjr
1136353018302074900,Wed Jun 05 19:24:06 +0000 2019,For our fourth paper, we'll also leverage a Bayesian approach, but in the context of Bayesian optimization for mult‚Ä¶ https://t.co/iQKwWNNcHV
1136352446186500100,Wed Jun 05 19:21:50 +0000 2019,Ehsan:  - For small numbers of samples, introducing prior knowledge via a prior over parameters can be useful. - Dr‚Ä¶ https://t.co/CDZ8FDvUaa
1136351710039949300,Wed Jun 05 19:18:54 +0000 2019,Ehsan: - for prediction, we can get multiple samples to compute the structure with the minimum Bayes risk  - model‚Ä¶ https://t.co/ab60hpQ5uh
1136351495882977300,Wed Jun 05 19:18:03 +0000 2019,Derivation: https://t.co/2qEbCdXI5L
1136350966943559700,Wed Jun 05 19:15:57 +0000 2019,Ehsan: - derive gradient via stochastic gradient langevin dynamics (looks similar to SGD + Gaussian noise) from https://t.co/sXas3aVATd
1136350521156145200,Wed Jun 05 19:14:11 +0000 2019,Ehsan: - Bayesian approach: learn posterior distribution over parameters of model; problem: integration is intracta‚Ä¶ https://t.co/dhL6OZD1QY
1136349783231205400,Wed Jun 05 19:11:15 +0000 2019,Ehsan:  - Parsing setup in this paper: a first-order factorized graph-based parser (score of a tree is decomposed i‚Ä¶ https://t.co/8gseYb0xox
1136349279201693700,Wed Jun 05 19:09:15 +0000 2019,Ehsan: - Cons: can lead to overfitting and poor generalization; ignores uncertainty and model is over-confident - S‚Ä¶ https://t.co/xpAH1aS9Ox
1136348878842794000,Wed Jun 05 19:07:39 +0000 2019,A recorded presentation by Ehsan Shareghi. Ehsan:  - Big picture view of supervised learning: lots of parameters, m‚Ä¶ https://t.co/fSLPlWAtW6
1136347977298128900,Wed Jun 05 19:04:04 +0000 2019,We slightly shift gears and move from the low-memory to the low-data regime with our third paper: Bayesian Learning‚Ä¶ https://t.co/1v3wLp1MS8
1136347367282810900,Wed Jun 05 19:01:39 +0000 2019,Shota: - evaluate on word similarity, model compression, NER, and textual entailment - Method outperforms compariso‚Ä¶ https://t.co/xsNRoDvCAt
1136346388596441100,Wed Jun 05 18:57:45 +0000 2019,Shota: Re 2: - don't use summation for combining subwords;  - use key-value attention; crucially, has a context-dep‚Ä¶ https://t.co/vEEG0BQXME
1136345657944477700,Wed Jun 05 18:54:51 +0000 2019,Shota: Re 1.: propose different strategies: - discard infrequent subwords (use only top-k most frequent ones) - mem‚Ä¶ https://t.co/VnITCZqhYp
1136345161703809000,Wed Jun 05 18:52:53 +0000 2019,Shota: - Cons of this approach: Large # of subword embeddings blows up memory size of model - Two key ideas of appr‚Ä¶ https://t.co/DnIY6y3iK4
1136344778281537500,Wed Jun 05 18:51:22 +0000 2019,Shota:  - Related work (Zhao et al., EMNLP 2018): Bag-of-subwords reconstructs pretrained word embeddings to suppor‚Ä¶ https://t.co/CV2lKE3hpY
1136344482264338400,Wed Jun 05 18:50:11 +0000 2019,In a picture: https://t.co/kjkJRBXXfz
1136344172573732900,Wed Jun 05 18:48:57 +0000 2019,@WenhuChen Shota Sasaki: - Introduce novel word embeddings, which are open-vocabulary and compact - They achieve be‚Ä¶ https://t.co/CPqkW7JAe8
1136343660520443900,Wed Jun 05 18:46:55 +0000 2019,@WenhuChen In a nice segue, we now move from words to subwords for our second paper:  Subword-based Compact Reconst‚Ä¶ https://t.co/oXkNqL3Sj7
1136343311999017000,Wed Jun 05 18:45:32 +0000 2019,@WenhuChen Overall, a creative approach to deal with a somewhat under-studied but fundamental aspect of common NLP‚Ä¶ https://t.co/ZHyYiyoFX5
1136342746736799700,Wed Jun 05 18:43:17 +0000 2019,@WenhuChen @WenhuChen: - Pros: compared to subword methods, can be applied to languages, which cannot be directly d‚Ä¶ https://t.co/6qfp1x43Kx
1136342447414546400,Wed Jun 05 18:42:06 +0000 2019,@WenhuChen @WenhuChen: - Evaluate on DBpedia, AG-News, and Yelp - Frequency-based cut-off is a strong baseline - Va‚Ä¶ https://t.co/MnNyH4kyo6
1136341814267580400,Wed Jun 05 18:39:35 +0000 2019,@WenhuChen:  - Replace binary masking vector with a Gaussian approximation (sampled from a normal distribution) fro‚Ä¶ https://t.co/mt0n7Pr97w
1136341070386749400,Wed Jun 05 18:36:38 +0000 2019,@NAACLHLT Wenhu: - Objective is not differentiable - Reinterpret problem in a Bayesian way: Associate a dropout pro‚Ä¶ https://t.co/b9oRlMXY4u
1136340805973680100,Wed Jun 05 18:35:34 +0000 2019,@NAACLHLT Wenhu: - Vocabulary selection is important in memory-constrained scenarios Re 2:  - Formulate vocabulary‚Ä¶ https://t.co/RXToeXPZlk
1136340279584350200,Wed Jun 05 18:33:29 +0000 2019,@NAACLHLT Wenhu: Re 1: - Sample different vocabulary sizes with fixed budget - For sample experiment, take 10 diffe‚Ä¶ https://t.co/KF7cRHfCJG
1136339921805987800,Wed Jun 05 18:32:04 +0000 2019,@NAACLHLT Wenhu: Goals of paper: 1. Importance of vocabulary selection 2. Minimum required vocabulary size
1136339721972531200,Wed Jun 05 18:31:16 +0000 2019,@NAACLHLT Wenhu: - Typically need to predefine vocabulary to get embeddings - Most common approach: frequency-based‚Ä¶ https://t.co/lW2BwM5IPl
1136339467965538300,Wed Jun 05 18:30:15 +0000 2019,@NAACLHLT First paper: How Large a Vocabulary Does Text Classification Need? A Variational Approach to Vocabulary S‚Ä¶ https://t.co/rGdKKA9iXk
1136339394640646100,Wed Jun 05 18:29:58 +0000 2019,Coming up: A live Twitter thread of Session 8B: Machine Learning @NAACLHLT with some awesome papers on vocabulary s‚Ä¶ https://t.co/QjzQau048X
1136264607729893400,Wed Jun 05 13:32:47 +0000 2019,RT @DaniYogatama: We think episodic memory is a key component of general linguistic intelligence models. In this work, we use it for sparse‚Ä¶
1136032714296037400,Tue Jun 04 22:11:20 +0000 2019,RT @emilymbender: Why do I care so much about getting people to name the language they're working on (*cough* English *cough*)? Here's part‚Ä¶
1135702210006962200,Tue Jun 04 00:18:01 +0000 2019,RT @emilymbender: Awesome! Well done &amp; thanks, @seb_ruder et al :)  https://t.co/L9xLuBhIy0  #NAACL2019  #EnglishIsNotGenericforLanguage #B‚Ä¶
1135294117049765900,Sun Jun 02 21:16:24 +0000 2019,@ikwattro @Thom_Wolf No. You can just train them on your target data.
1135283076647587800,Sun Jun 02 20:32:32 +0000 2019,RT @soldni: Absolutely PACKED room for @seb_ruder, @Thom_Wolf, @swabhz, and @mattthemathman‚Äôs tutorial on transfer learning for NLP #NAACL2‚Ä¶
1135275731783753700,Sun Jun 02 20:03:21 +0000 2019,RT @Thom_Wolf: Colab for our tutorial #NAACLTransfer @seb_ruder https://t.co/mmnQCS3UHV
1135269935574323200,Sun Jun 02 19:40:19 +0000 2019,@pywirrarika @Thom_Wolf @amitmoryossef @NAACLHLT @swabhz @mattthemathman Yes, see here: Colab:‚Ä¶ https://t.co/1IEZdSi9wX
1135266082200215600,Sun Jun 02 19:25:00 +0000 2019,For anyone having issues accessing the shortened links, here are the complete links: Slides:‚Ä¶ https://t.co/E4otHnxfSd
1135264635014254600,Sun Jun 02 19:19:15 +0000 2019,@Thom_Wolf @amitmoryossef @NAACLHLT @swabhz @mattthemathman Here's a link to a .pptx version of the slides: https://t.co/dDWZ7MgNyC
1135228720250347500,Sun Jun 02 16:56:33 +0000 2019,@zaidalyafeai @NAACLHLT @Thom_Wolf @swabhz @mattthemathman Unfortunately not.
1135224233448153100,Sun Jun 02 16:38:43 +0000 2019,@NAACLHLT @Thom_Wolf @swabhz @mattthemathman If you are attending the tutorial, please bring your laptops in order‚Ä¶ https://t.co/6Wck6xNENR
1135223959828537300,Sun Jun 02 16:37:38 +0000 2019,Here are the materials for our @NAACLHLT tutorial on Transfer Learning in NLP with @Thom_Wolf @swabhz‚Ä¶ https://t.co/uhmADVKb94
1135156010220511200,Sun Jun 02 12:07:37 +0000 2019,RT @DeepMindAI: We are proud to be Diversity and Inclusion Champions and Platinum sponsors of #NAACL2019 this week! See below for a schedul‚Ä¶
1135013124107571200,Sun Jun 02 02:39:50 +0000 2019,RT @aidanematzadeh: At #naacl2019 and interested in (modeling/getting inspired by) language evolution, acquisition, and processing? Come to‚Ä¶
1135006213119324200,Sun Jun 02 02:12:23 +0000 2019,RT @yoavartzi: . @NAACLHLT attendees (and these just watching from afar), I am looking for a postdoc next year. Position will be at the @co‚Ä¶
1134972245225418800,Sat Jun 01 23:57:24 +0000 2019,RT @uwnlp: Kicking it off on June 2nd, come check out the ‚ÄúTransfer Learning in Natural Language Processing‚Äù tutorial organized by @seb_rud‚Ä¶
1134858203102154800,Sat Jun 01 16:24:14 +0000 2019,@adveisner Congrats, Jason! Exciting news! :)
1134857822607482900,Sat Jun 01 16:22:44 +0000 2019,If you're interested in Fake News or misinformation, the submission deadline for the Conference for Truth and Trust‚Ä¶ https://t.co/hHRwaE4qIy
1134857425406832600,Sat Jun 01 16:21:09 +0000 2019,@sivareddyg @mcgillu @MILAMontreal Congrats, Siva! :)
1134810807651446800,Sat Jun 01 13:15:54 +0000 2019,Great opportunity for a postdoc in one of the most productive NLP groups in the US, at UC Santa Barbara! https://t.co/P9QZzFa4MG
1134565199472672800,Fri May 31 20:59:57 +0000 2019,RT @Thom_Wolf: Excited to be in Minneapolis for @NAACLHLT, next week. Ping me if you'd like to chat about @huggingface or NLP/NLG/...!  Thi‚Ä¶
1134228225813561300,Thu May 30 22:40:56 +0000 2019,@galsenai @NAACLHLT @DeepMindAI @Thom_Wolf @swabhz @mattthemathman Sure. I'll share as soon as I get it. We'll also‚Ä¶ https://t.co/s1fBkWkI21
1134222990864986100,Thu May 30 22:20:08 +0000 2019,@galsenai @NAACLHLT @DeepMindAI @Thom_Wolf @swabhz @mattthemathman Yes, I think so.
1134222923210924000,Thu May 30 22:19:52 +0000 2019,@sanjaykamath @NAACLHLT @DeepMindAI @Thom_Wolf @swabhz @mattthemathman The tutorial should be recorded. Stay tuned!
1134172797935206400,Thu May 30 19:00:41 +0000 2019,I'll be at @NAACLHLT next week. Ping me if you'd like to chat about transfer learning or working at @DeepMindAI. If‚Ä¶ https://t.co/RqWty2wdni
1134168354929938400,Thu May 30 18:43:02 +0000 2019,RT @AthensNlp: Few hours left to apply as student to the school. #AthNLP2019 will take place September 18th-25th at the campus of NCSR ‚ÄúDem‚Ä¶
1134086500134596600,Thu May 30 13:17:46 +0000 2019,RT @wluper_: Ambiguity!  #ML #MachineLearning #NLProc #NLP https://t.co/FDGONJ6ujT
1133841564105662500,Wed May 29 21:04:29 +0000 2019,@rameshkrshah Thank you! :)
1133789958647484400,Wed May 29 17:39:25 +0000 2019,@srush_nlp Congrats, Sasha!
1133788903603941400,Wed May 29 17:35:14 +0000 2019,@ybisk @SCSatCMU @LTIatCMU @CMU_Robotics This is great! Congrats, yonatan!
1133504281989066800,Tue May 28 22:44:14 +0000 2019,@jacobeisenstein @GoogleAI Congrats, Jacob!
1133421218663731200,Tue May 28 17:14:11 +0000 2019,RT @dirk_hovy: I am looking to fill a postdoc position for up to 18 months, starting November 2019 (some flexibility). If you are intereste‚Ä¶
1132960587120599000,Mon May 27 10:43:48 +0000 2019,@NinaCog @_stefan_munich @ACL2019_Italy Yes.
1132653077322960900,Sun May 26 14:21:51 +0000 2019,RT @adveisner: Attending #NAACL2019?  Huge demand for mentoring. If you've been to NLP confs before, sign up as a "big sibling" to show a n‚Ä¶
1132015728369033200,Fri May 24 20:09:16 +0000 2019,RT @conll2019: Did you miss the @EMNLP2019 deadline and/or decided to wait for a better version? Submit your paper to @CoNLL2019 and still‚Ä¶
1131683183559368700,Thu May 23 22:07:51 +0000 2019,RT @alfcnz: ¬´Model-predictive policy learning with uncertainty regularisation for driving in dense traffic¬ª video summary. Authors: @Henaff‚Ä¶
1131612080249946100,Thu May 23 17:25:18 +0000 2019,@alex_conneau Congrats, Alexis! Well done! üéâ
1131597487867879400,Thu May 23 16:27:19 +0000 2019,@strubell @LTIatCMU @SCSatCMU @facebookai Congrats!
1131208495737528300,Wed May 22 14:41:36 +0000 2019,RT @SanhEstPasMoi: A few weeks ago, a friend of mine asked me "Which papers can I read to catch up with the latest trends in modern NLP?".‚Ä¶
1130958240114778100,Tue May 21 22:07:11 +0000 2019,RT @Thom_Wolf: @mark_riedl @seb_ruder @mattthemathman @swabhz Here is a very small (&lt;150 lines) gist with the model &amp; a training code on wi‚Ä¶
1130923244133998600,Tue May 21 19:48:07 +0000 2019,RT @AbrahamStarosta: @seb_ruder thank you for adding my blog to your NLP newsletter! Never imagined that would happen. For those interested‚Ä¶
1130778861543804900,Tue May 21 10:14:24 +0000 2019,RT @emnlp2019: Good luck with your paper submissions today! Remember: The deadline is at 11:59 pm Pacific Daylight Savings Time (UTC-7h). W‚Ä¶
1130397827295785000,Mon May 20 09:00:18 +0000 2019,New NLP Newsletter: Marvel, Stanford &amp; CMU NLP Playlists, Voynich, Bitter Lesson Vol. 2, ICLR 2019, Dialogue Demos‚Ä¶ https://t.co/JCPwBr8duX
1130393338350383100,Mon May 20 08:42:28 +0000 2019,Thanks, everyone, for the wealth of suggestions! This is a lot of food for thought for us and we'll see what we can‚Ä¶ https://t.co/7f7NOYTEHk
1130050862204821500,Sun May 19 10:01:35 +0000 2019,Besides the obvious things (ELMo, BERT, etc.), is there anything that we should definitely discuss at the NAACL "Tr‚Ä¶ https://t.co/hs3JHQn8l0
1129677277829226500,Sat May 18 09:17:06 +0000 2019,RT @Thom_Wolf: Currently working on the coming NAACL "Transfer Learning in NLP" tutorial with @seb_ruder @mattthemathman and @swabhz. Prett‚Ä¶
1129477366659604500,Fri May 17 20:02:43 +0000 2019,If you don't have any plans on October 11, join us in London for the EurNLP summit. Submit an abstract until July 8‚Ä¶ https://t.co/u2bFSH28vc
1129154769417850900,Thu May 16 22:40:50 +0000 2019,Happy to share that our paper on how to (properly) evaluate cross-lingual word embeddings with @gg42554,  Robert Li‚Ä¶ https://t.co/qK6CqR92hu
1128595079957045200,Wed May 15 09:36:50 +0000 2019,RT @emnlp2019: If anything is unclear regarding the submission of the abstract or full paper, have a look at the Submission FAQs for the Ma‚Ä¶
1128427162539888600,Tue May 14 22:29:35 +0000 2019,RT @artetxem: 4 papers accepted at @ACL2019_Italy!!!  Titles and a brief description of each below üëá
1128291123871588400,Tue May 14 13:29:01 +0000 2019,@Thom_Wolf @ACL2019_Italy Yay! Congrats! üéâ
1128287693698367500,Tue May 14 13:15:23 +0000 2019,RT @math_rachel: Top item on my wish list for AI researchers: write a blog post to accompany your paper. This is a chance to reach a broade‚Ä¶
1128236361788608500,Tue May 14 09:51:24 +0000 2019,@RABawden @ACL2019_Italy @NAACLHLT That sucks so much. Really sorry to hear that, Rachel. ‚òπÔ∏è
1128229343707258900,Tue May 14 09:23:31 +0000 2019,@akkikiki @ACL2019_Italy @mjp39 @boydgraber @palen @kenbod That's awesome! Congrats!
1127978555667877900,Mon May 13 16:46:59 +0000 2019,RT @emnlp2019: If you have some concerns regarding paper submission, check out the updated submission guidelines. In particular, the abstra‚Ä¶
1127977917487702000,Mon May 13 16:44:27 +0000 2019,RT @chipro: Top trends I saw at #ICLR2019 include the rise of unsupervised representation learning, RNN losing its luster, GANs still domin‚Ä¶
1127147097667571700,Sat May 11 09:43:04 +0000 2019,@senja_trrr @NandoDF @JoakimNivre @RicoSennrich @natschluter Not decided yet, but we're aiming to give participants‚Ä¶ https://t.co/aGxNBaiX9x
1126823852175130600,Fri May 10 12:18:36 +0000 2019,@neal_lathia We want to encourage participation from industry and are currently thinking about an industry-focused‚Ä¶ https://t.co/kh9Si4hS9K
1126821666611519500,Fri May 10 12:09:55 +0000 2019,@sazoo_nlp @NandoDF @JoakimNivre @RicoSennrich @natschluter We'll make the decisions on a case by case basis, but t‚Ä¶ https://t.co/bhJ1szr5xy
1126770634715910100,Fri May 10 08:47:08 +0000 2019,@NandoDF @JoakimNivre @RicoSennrich @natschluter We are super thrilled about such an amazing line-up, with more spe‚Ä¶ https://t.co/LElgbxeycG
1126769507521839100,Fri May 10 08:42:39 +0000 2019,Our first set of speakers for EurNLP has been confirmed: Besides @NandoDF, we'll have Gemma Boleda (UPF), Angela Fa‚Ä¶ https://t.co/Ro3wrpRVx4
1126767695595130900,Fri May 10 08:35:27 +0000 2019,@moret1788 Sure!
1126615682899222500,Thu May 09 22:31:24 +0000 2019,RT @emnlp2019: Don't forget that you should submit an abstract before the actual paper submission. The abstract submission deadline is in 6‚Ä¶
1123981578655412200,Thu May 02 16:04:25 +0000 2019,RT @NAACLHLT: #nlproc Twitter! Help communicate #naacl2019 as it happens by becoming an official livetweeter! You'll even get mentioned on‚Ä¶
1123847123106259000,Thu May 02 07:10:08 +0000 2019,RT @emnlp2019: Please be aware of this year's multiple submission policy, which is a change from last year. We will not consider any paper‚Ä¶
1122918952282271700,Mon Apr 29 17:41:55 +0000 2019,RT @AnnUkhanova: üì¢ Dear PhD students, I'm happy to invite you to participate in our exclusive #NLP Summit on June 24-26 in #Zurich!üéì‚õ∞Ô∏èüåç  ‚ú®‚Ä¶
1121898685787516900,Fri Apr 26 22:07:45 +0000 2019,RT @sigrep_acl: We've extended the submission deadline to RepL4NLP another week, until next Friday, 3 May. That should give you more time t‚Ä¶
1121896876880662500,Fri Apr 26 22:00:33 +0000 2019,@RangwaniHarsh We've actually extended the deadline until next Friday, so you have a bit more time. :)
1121812023183138800,Fri Apr 26 16:23:23 +0000 2019,@NinaCog @_stefan_munich @ACL2019_Italy The SRW specifies that it adheres to ACL 2019 call for papers regarding mul‚Ä¶ https://t.co/vpLuOoc4ps
1121779240591085600,Fri Apr 26 14:13:07 +0000 2019,@SvenGiesselbach Ah, yes. We decided to extend the deadline a week to give people a bit more time to work on their submissions. :)
1121700443766132700,Fri Apr 26 09:00:00 +0000 2019,Today is the last day to submit a paper to RepL4NLP! Besides regular workshop papers, extended abstracts (4 or 8 pa‚Ä¶ https://t.co/hKBLwrc5uu
1121173419687600100,Wed Apr 24 22:05:48 +0000 2019,RT @nlpandcss: We are excited to announce that thanks to our sponsors, we will be able to provide a number of travel grants to support thei‚Ä¶
1121152189139968000,Wed Apr 24 20:41:26 +0000 2019,RT @adveisner: Now hiring an Assistant Research Professor!  (Spread the word plz?)  If you love developing fundamental #MachineLearning mod‚Ä¶
1121109083145351200,Wed Apr 24 17:50:09 +0000 2019,@NinaCog @_stefan_munich @ACL2019_Italy Sure, I think you can submit to both.
1121081638237216800,Wed Apr 24 16:01:05 +0000 2019,@NinaCog @_stefan_munich @ACL2019_Italy Not directly but you can submit preliminary ideas or results as an extended abstract (8 or 4 pages).
1121009726874161200,Wed Apr 24 11:15:20 +0000 2019,@_stefan_munich üéâ
1120977012426981400,Wed Apr 24 09:05:21 +0000 2019,@_stefan_munich @ACL2019_Italy As far as I'm aware, you can only upload one file in the submission form for RepL4NL‚Ä¶ https://t.co/ul4xAkiCfw
1120974444455956500,Wed Apr 24 08:55:08 +0000 2019,RT @seb_ruder: Don't forget to submit your paper to RepL4NLP for a chance to present your work at one of the biggest workshops at NLP confe‚Ä¶
1120974356123914200,Wed Apr 24 08:54:47 +0000 2019,Don't forget to submit your paper to RepL4NLP for a chance to present your work at one of the biggest workshops at‚Ä¶ https://t.co/o6WK4puWBo
1120973551958396900,Wed Apr 24 08:51:36 +0000 2019,@shobhitjn1 11:59 pm PST. You should be fine as long as it is still April 26 somewhere on the planet.
1120972589730496500,Wed Apr 24 08:47:46 +0000 2019,RT @emnlp2019: EMNLP invites submissions for system demonstrations (ranging from prototype to production). We particularly encourage submit‚Ä¶
1120762089465876500,Tue Apr 23 18:51:19 +0000 2019,@Tuhin66978276 @javycoolnsmart @jeremyphoward @fastdotai Please share the camera-ready version once it's out. :D
1120760860245483500,Tue Apr 23 18:46:26 +0000 2019,@Tuhin66978276 @javycoolnsmart @jeremyphoward @fastdotai Nice work! :)
1120334221019627500,Mon Apr 22 14:31:07 +0000 2019,@javycoolnsmart @jeremyphoward @fastdotai Finally, here are some results from the https://t.co/Uax3FSbOBV community: https://t.co/egjUP9Oczk
1120334115763564500,Mon Apr 22 14:30:42 +0000 2019,@javycoolnsmart @jeremyphoward @fastdotai For an extensive list of references, you can look at Google Scholar:‚Ä¶ https://t.co/456uHPvopk
1119206810827874300,Fri Apr 19 11:51:12 +0000 2019,…ô ä ma…™ g…íd! Writing tweets just got a lot more linguistically interesting. Gboard now supports IPA (International P‚Ä¶ https://t.co/qnREqEH7jF
1119191103419318300,Fri Apr 19 10:48:47 +0000 2019,If you're interested in representation learning in NLP, consider submitting to RepL4NLP at @ACL2019_Italy. The dead‚Ä¶ https://t.co/68DtdmmIo5
1119188215683063800,Fri Apr 19 10:37:18 +0000 2019,RT @sigrep_acl: The submission deadline for the 4th Workshop on Representation Learning is in a week, on April 26. We accept normal worksho‚Ä¶
1119171623255511000,Fri Apr 19 09:31:22 +0000 2019,RT @gradientpub: Since our first call for ideas, we've published over a dozen articles on state-of-the-art machine learning. Our second cal‚Ä¶
1118530705397571600,Wed Apr 17 15:04:36 +0000 2019,RT @basakbuluz: Thanks to @seb_ruder for excellent collaborative work and sharing ü§û  ‚≠êÔ∏è Yapay Sinir Aƒülarƒ± Temelli Doƒüal Dil ƒ∞≈üleme Tarihi‚Ä¶
1118438563908026400,Wed Apr 17 08:58:27 +0000 2019,RT @DataScienceNIG: One more reason we are excited about ULMFiT @fastdotai It just won 1st position at PolEval2019  Kudos again @jeremyphow‚Ä¶
1118274277013323800,Tue Apr 16 22:05:38 +0000 2019,RT @art_sobolev: @seb_ruder Does MLSS count? https://t.co/6kKnIs2Ngn
1118270366512816100,Tue Apr 16 21:50:06 +0000 2019,Applications for EurNLP are not yet open, so save the date.  Feel free to reply with any big, community-driven ML e‚Ä¶ https://t.co/DBYYTK3OYe
1118269793554051100,Tue Apr 16 21:47:49 +0000 2019,There's still time to apply for awesome ML events around the world this year: - DL Indaba, Nairobi üá∞üá™‚Ä¶ https://t.co/Ejjo9qO8VM
1118268107771056100,Tue Apr 16 21:41:07 +0000 2019,RT @sigrep_acl: The submission deadline for the 4th Workshop on Representation Learning in NLP is in ten days, on April 26. Don't forget to‚Ä¶
1118265837964681200,Tue Apr 16 21:32:06 +0000 2019,RT @emnlp2019: If you still want to submit your EMNLP submission to arXiv, you should do it within the next five days. The anonymity period‚Ä¶
1118235662208184300,Tue Apr 16 19:32:12 +0000 2019,RT @HudaKhay: Undergrads (in the Americas) interested in #NLProc and #SpeechProc! apply for this @naacl-funded scholarship to attend a 2 we‚Ä¶
1117729393730818000,Mon Apr 15 10:00:28 +0000 2019,New NLP Newsletter: The Bitter Lesson, How to Write X, ML Events in 2019, and a lot more! https://t.co/8JzFmdBt5p (via @revue)
1117684507082199000,Mon Apr 15 07:02:06 +0000 2019,RT @b_niranjan: Some #NLProc #postdoc opps that I know of:   1. @mohitban47  https://t.co/4vaLs1AYI7 2. @yoavartzi: https://t.co/GCNF6e2Nhl‚Ä¶
1117551274730446800,Sun Apr 14 22:12:41 +0000 2019,@jorgecarcavallo I'm a bit short on time at the moment. I'm sure there are a lot of people that would be interested‚Ä¶ https://t.co/iQNsYUOcHS
1117543381490597900,Sun Apr 14 21:41:19 +0000 2019,RT @Rasa_HQ: We‚Äôre very excited to announce our Series A funding, led by @Accel! With this funding, we‚Äôre doubling down on building the sta‚Ä¶
1117539756483780600,Sun Apr 14 21:26:55 +0000 2019,@JidinDinesh @omarsar0 Have a look at page 77 of my thesis: https://t.co/zlsRxXdEe2 let me know if this makes things clearer.
1117538884513148900,Sun Apr 14 21:23:27 +0000 2019,@jorgecarcavallo I think that's a fantastic idea! I wish you a lot of success!
1117538712735424500,Sun Apr 14 21:22:46 +0000 2019,@Maaarcocr Definitely! Stay tuned for news on this. :)
1117538121401479200,Sun Apr 14 21:20:25 +0000 2019,@basakbuluz Well done! üëè
1117538121401479200,Sun Apr 14 21:20:25 +0000 2019,@basakbuluz Well done! üëè
1117442712775602200,Sun Apr 14 15:01:18 +0000 2019,RT @alienelf: Last chance to get your @DeepIndaba applications in at  https://t.co/uL5u50OKci or https://t.co/1YN29kvXVF  Plz retweet @Jeff‚Ä¶
1117430221773443100,Sun Apr 14 14:11:40 +0000 2019,RT @PiotrCzapla: Our ULMFiT won 1st place on Poleval2019, @misterkardas  tweaks have put our version far above ensemble of BERT. Showing ho‚Ä¶
1117215241371291600,Sat Apr 13 23:57:25 +0000 2019,RT @sleepinyourhat: I'm hiring a postdoc for work on language understanding in NLP! Interested in working on transfer learning (think BERT)‚Ä¶
1117197301414551600,Sat Apr 13 22:46:07 +0000 2019,RT @PiotrCzapla: Link to the results page https://t.co/O9rIFnGZ2E
1116719403255640000,Fri Apr 12 15:07:08 +0000 2019,RT @yoavartzi: I am looking for a postdoc to join my group at Cornell in the Cornell Tech NYC campus. Details in thread. Contact me for det‚Ä¶
1116376108813803500,Thu Apr 11 16:23:00 +0000 2019,This post by the Snorkel team gives a great overview of ingredients that make up a state-of-the-art approach on GLU‚Ä¶ https://t.co/9JpLyWeNwq
1116019258192990200,Wed Apr 10 16:45:00 +0000 2019,Developing things from scratch is often the best way to learn. This is a nice tutorial that develops basic neural n‚Ä¶ https://t.co/zNr5yDBLvb
1115966926533091300,Wed Apr 10 13:17:03 +0000 2019,RT @DeepIndaba: We encourage students, academics, industry professionals and anyone with a passion for machine learning to apply to attend‚Ä¶
1115902713882189800,Wed Apr 10 09:01:54 +0000 2019,RT @barbara_plank: We are really excited to announce the first EurNLP ("your" NLP) summit to take place on Oct 11 in London #NLProc - pls s‚Ä¶
1115693784115839000,Tue Apr 09 19:11:41 +0000 2019,@emilymbender @NandoDF Thanks, Emily! Yes, we're definitely planning on avoiding that and hope to have speakers fro‚Ä¶ https://t.co/JWhBlKBphR
1115693139413565400,Tue Apr 09 19:09:07 +0000 2019,@honnibal Ah, that's a shame! I wasn't aware of that event. :(
1115691710657499100,Tue Apr 09 19:03:27 +0000 2019,Registration will open soon. In the meantime, we'll hope you'll save the date and consider joining us for what shou‚Ä¶ https://t.co/mlzNYxrp0w
1115690615612825600,Tue Apr 09 18:59:05 +0000 2019,We'll have talks from research leaders on the latest advances in NLP. @NandoDF will be giving the keynote and more‚Ä¶ https://t.co/X5cvB14OD5
1115690002233557000,Tue Apr 09 18:56:39 +0000 2019,We encourage submissions of new, previously, or concurrently published research. The event should be a forum for re‚Ä¶ https://t.co/fs8B7064bQ
1115689254271819800,Tue Apr 09 18:53:41 +0000 2019,We are really excited to announce the first EurNLP Summit, an opportunity to foster collaboration between NLP resea‚Ä¶ https://t.co/V4LC6VCOZB
1115653849753108500,Tue Apr 09 16:33:00 +0000 2019,This is a great post that highlights the connection between the building blocks used in Transformers and Capsule Ne‚Ä¶ https://t.co/svelKtxQbp
1115648308901490700,Tue Apr 09 16:10:59 +0000 2019,@JamesAllingham Yep. üòä
1115509754531594200,Tue Apr 09 07:00:25 +0000 2019,RT @IAugenstein: One week to apply for a postdoc position with me @CopeNLU! The *earliest possible* start date is 16 May, i.e. a couple of‚Ä¶
1115311594962149400,Mon Apr 08 17:53:00 +0000 2019,Applications for the Deep Learning Indaba 2019 close this Friday, April 12. This is an awesome opportunity to engag‚Ä¶ https://t.co/c6zDWUrv4r
1115308520205881300,Mon Apr 08 17:40:47 +0000 2019,RT @emnlp2019: For anyone interested in submitting to EMNLP 2019, the anonymity period begins in less two weeks, on April 21. https://t.co/‚Ä¶
1115306140093624300,Mon Apr 08 17:31:19 +0000 2019,RT @sigrep_acl: Less than three weeks left until the submission deadline for the 4th Workshop on Representation Learning in NLP. Don't forg‚Ä¶
1115275596366536700,Mon Apr 08 15:29:57 +0000 2019,Forgot the link. https://t.co/lOqDqxpRT9
1115274601200635900,Mon Apr 08 15:26:00 +0000 2019,There's a new venue in town, the Conference for Truth and Trust Online, that aims to bring together people fighting‚Ä¶ https://t.co/hSVC0Z9Zos
1114869796166873100,Sun Apr 07 12:37:27 +0000 2019,@DerekChia That's a way better pun than mine! üòÇ
1114860301898403800,Sun Apr 07 11:59:43 +0000 2019,@GersonVizcarra Thanks for the note, Gerson. I hope to make it next year.
1114860071694028800,Sun Apr 07 11:58:48 +0000 2019,Understanding humor is arguably one of the hardest challenges on towards natural language understanding. The HAHA c‚Ä¶ https://t.co/SfNsMGlOgf
1114272258418249700,Fri Apr 05 21:03:03 +0000 2019,@poornaprudhvi @jeremyphoward @radamihalcea Well, if you have a supervised approach available, why would you not tr‚Ä¶ https://t.co/v1Y0PTOFgG
1114255796303429600,Fri Apr 05 19:57:38 +0000 2019,@poornaprudhvi @jeremyphoward @radamihalcea https://t.co/Kk1RyG8J0f
1114182134506430500,Fri Apr 05 15:04:56 +0000 2019,Papers with Code now has badges to put on your GitHub repo that indicate that your model is state-of-the-art. üèÖ Thi‚Ä¶ https://t.co/ZC6xxfUOpA
1113187907647496200,Tue Apr 02 21:14:13 +0000 2019,RT @Khipu_AI: Hello world! We are happy to announce the first edition of Khipu: Latin American Meeting in Artificial Intelligence in Novemb‚Ä¶
1113141583786991600,Tue Apr 02 18:10:09 +0000 2019,@sai_prasanna I'm not aware of any method that has been trained as a language model on large amounts of code. Appro‚Ä¶ https://t.co/oEHhVJKqHj
1112998244118859800,Tue Apr 02 08:40:34 +0000 2019,RT @Towards_Entropy: So it turns out ULMFiT by @seb_ruder and @jeremyphoward works great for classifying genomic sequences, producing compe‚Ä¶
1112984335458689000,Tue Apr 02 07:45:18 +0000 2019,Pretrained language models are not only applicable to natural language but also to other domains where sequences ha‚Ä¶ https://t.co/gZeooukefZ
1112822517213872100,Mon Apr 01 21:02:18 +0000 2019,@ArneKoehn @NAACLHLT @ACL_NLP This is great! Thanks so much for this, Arne!
1112822474507513900,Mon Apr 01 21:02:07 +0000 2019,RT @ArneKoehn: ~700 more videos of conference presentations linked in the ACL anthology: #EMNLP 15,16,17, @NAACLHLT 15,18, @ACL_NLP 17,18,‚Ä¶
1112073564402995200,Sat Mar 30 19:26:13 +0000 2019,RT @spacy_io: Incredibly proud to invite you to the first spaCy IRL on July 5-6, 2019 in Berlin!  üèù One-day single-track conference üóù Keyno‚Ä¶
1112073475211038700,Sat Mar 30 19:25:52 +0000 2019,This is going to be so much fun! üòä https://t.co/72Zrz6Z2Ve
1111978981203546100,Sat Mar 30 13:10:23 +0000 2019,14/ If this piqued your interest, you can find the call for papers here: https://t.co/gWALA11Kua Submission deadlin‚Ä¶ https://t.co/6abbZeZiA1
1111978629897039900,Sat Mar 30 13:08:59 +0000 2019,13/ Overall, I am excited what interesting problems this year‚Äôs submissions will tackle. If the past is any indicat‚Ä¶ https://t.co/D14bkPT6Pj
1111978304507129900,Sat Mar 30 13:07:42 +0000 2019,12/ For more examples from EMNLP and ACL last year, have a look here: https://t.co/OEHmHQBD1e https://t.co/MFGvngYUeX
1111978242381103100,Sat Mar 30 13:07:27 +0000 2019,11/ Some of my favourite papers from last year were papers that seek to gain a better understanding of the capabili‚Ä¶ https://t.co/0wvsXkmOwf
1111978142271529000,Sat Mar 30 13:07:03 +0000 2019,@nelsonfliu @omerlevy_ @royschwartz02 @ChenhaoTan @nlpnoah 10/ They also show that LSTMs use some neurons to count‚Ä¶ https://t.co/HbGtnXPaao
1111978009966403600,Sat Mar 30 13:06:31 +0000 2019,9/ 3rd: LSTMs Exploit Linguistic Attributes of Data by @nelsonfliu, @omerlevy_, @royschwartz02, @ChenhaoTan, and‚Ä¶ https://t.co/TrD0LTgrqu
1111977821495279600,Sat Mar 30 13:05:46 +0000 2019,8/ It is great to see work that not just looks at English, but considers morphologically rich languages. Subword re‚Ä¶ https://t.co/ctsKiCJyWn
1111977484491411500,Sat Mar 30 13:04:26 +0000 2019,7/ They propose morph2vec, a method that uses attention over different morphological segmentations of a word. Morph‚Ä¶ https://t.co/NgDjKbMvgz
1111977353490694100,Sat Mar 30 13:03:55 +0000 2019,6/ 2nd paper: Characters or Morphemes: How to Represent Words?  by Ahmet √úst√ºn, Murathan Kurfalƒ± and Burcu Can:‚Ä¶ https://t.co/BMRcqDZYrN
1111977201078095900,Sat Mar 30 13:03:18 +0000 2019,5/ Sentence embedding models are one of the most popular and versatile embedding methods. For a brief overview of c‚Ä¶ https://t.co/5ETuNFAkMp
1111977046610186200,Sat Mar 30 13:02:42 +0000 2019,4/ The resulting model performs well and is conceptually simple: normalize the word vectors, compute a weighted ave‚Ä¶ https://t.co/lkVjbSAmDN
1111976896747786200,Sat Mar 30 13:02:06 +0000 2019,3/ This work is a great example of a paper that identifies a weakness in an existing method (i.e. that the model is‚Ä¶ https://t.co/SMQqPJZOza
1111976750546907100,Sat Mar 30 13:01:31 +0000 2019,@sigrep_acl @ACL2019_Italy 2/ Unsupervised Random Walk Sentence Embeddings by Kawin Ethayarajh:‚Ä¶ https://t.co/3FnIhw32Cc
1111976535223926800,Sat Mar 30 13:00:40 +0000 2019,1/ The 4th RepL4NLP workshop @sigrep_acl takes place at @ACL2019_Italy this year. We'll again have best paper award‚Ä¶ https://t.co/5AWVD1XZzF
1111954182204985300,Sat Mar 30 11:31:50 +0000 2019,RT @sigrep_acl: Would you like to discuss with others your work on representation learning that is ongoing, ready to publish, or already pu‚Ä¶
1111401075494387700,Thu Mar 28 22:53:59 +0000 2019,RT @barbara_plank: deadline approaching - postdoc with me at @ITUkbh - apply by March 29 https://t.co/4n2d7z8IXc
1111399299164733400,Thu Mar 28 22:46:56 +0000 2019,RT @StuartReid1929: The Deep Learning Indaba was the #1 highlight of my 2018. I highly recommend that everybody apply. The experience is tr‚Ä¶
1111333342006468600,Thu Mar 28 18:24:50 +0000 2019,The Deep Learning Indaba 2018 was one of my highlights of the year. It's a great event where you can learn, exchang‚Ä¶ https://t.co/pmuW0TrLgD
1111328288780116000,Thu Mar 28 18:04:46 +0000 2019,RT @jeremyphoward: Pretrained ULMFiT language models for 10 Indian languages! https://t.co/zor6KQTtyd https://t.co/W6FOYTq47e
1111021420497301500,Wed Mar 27 21:45:23 +0000 2019,RT @a7med_rashwan: (1/4) Today I'm Publishing the SOTA pre-trained Language model for Arabic trained on ~800,000 Wikipedia articles followi‚Ä¶
1110641856646467600,Tue Mar 26 20:37:08 +0000 2019,@yashuseth @iamtrask A custom NER tagger I'd say, but I haven't looked much into this task.
1110468382078455800,Tue Mar 26 09:07:48 +0000 2019,RT @emnlp2019: We are happy to announce that Childcare Grants of up to US$400 will be available to attendees with young children. Attendees‚Ä¶
1110275213047091200,Mon Mar 25 20:20:13 +0000 2019,RT @pnderthevstnes: ULMFit from @fastai + Data Augmentation with backtranslation can get 80+% validation accuracy using only 50 training ex‚Ä¶
1109519529498107900,Sat Mar 23 18:17:24 +0000 2019,My PhD thesis Neural Transfer Learning for Natural Language Processing is now online. It includes a general review‚Ä¶ https://t.co/AbWPf5x7Nc
1109148388409397200,Fri Mar 22 17:42:37 +0000 2019,@yanaiela Hi Yanai, great job on the first blog post! Keep up the great work! :)
1109146485545934800,Fri Mar 22 17:35:03 +0000 2019,@bradenjhancock Thanks for sharing this blog post! Great work! :)
1109012154714411000,Fri Mar 22 08:41:16 +0000 2019,RT @olatz87: We are glad to announce the upcoming #DL4NLPcourse from July 3rd to 5th 2019 in Donostia/San Sebastian. Register before 15/5/2‚Ä¶
1108753364123291600,Thu Mar 21 15:32:56 +0000 2019,@prajjwal_1 @TryGhost @gatsbyjs I've been using buster. It's a bit hacky but works.
1108693562869407700,Thu Mar 21 11:35:18 +0000 2019,RT @DeepIndaba: Application to join in the Deep Learning Indaba 2019, in Nairobi Kenya are open.  See our blog for an overview, and the web‚Ä¶
1108656161681821700,Thu Mar 21 09:06:41 +0000 2019,RT @emnlp2019: The call for papers for EMNLP-IJCNLP 2019 is out now! Abstracts are due on May 14. Submissions are due a week later, on May‚Ä¶
1108475924478152700,Wed Mar 20 21:10:29 +0000 2019,RT @math_rachel: Repository to track progress in NLP, including datasets and current state-of-the-art for the most common NLP tasks. @seb_r‚Ä¶
1108282049440505900,Wed Mar 20 08:20:06 +0000 2019,RT @DaniYogatama: We are organizing the first Southeast Asia Machine Learning School (@wittawatj, @lmthang, Adhi). Grateful to our sponsors‚Ä¶
1108277409462317000,Wed Mar 20 08:01:39 +0000 2019,@_inesmontani @adrianmouat @honnibal @kengzwl @spacy_io Thanks for bringing that to my attention.
1108042472071012400,Tue Mar 19 16:28:06 +0000 2019,RT @sleepinyourhat: This is packed with useful experiments. I would probably have run half of them myself if this hadn't come out, and I pr‚Ä¶
1107373261107667000,Sun Mar 17 20:08:54 +0000 2019,@salil_23 @prajjwal_1 Thanks for the help, Salil. Yes, the thesis should be online in a few weeks.
1107232165186678800,Sun Mar 17 10:48:14 +0000 2019,@StuartReid1929 @ulrichpaquet @HermanKamper @DeepIndaba @GoogleAI Congrats, Herman!!
1106952632311181300,Sat Mar 16 16:17:28 +0000 2019,@michalwols @mattthemathman @nlpnoah For BERT, we mostly modified the provided Colaboratory notebooks, while for EL‚Ä¶ https://t.co/XxmuoKjcfQ
1106930520301150200,Sat Mar 16 14:49:36 +0000 2019,New paper with @mattthemathman &amp; @nlpnoah on adapting pretrained representations: We compare feature extraction &amp; f‚Ä¶ https://t.co/qLsPgx4cd3
1106829306485985300,Sat Mar 16 08:07:25 +0000 2019,RT @nlpnoah: New @ai2_allennlp paper by @mattthemathman , @seb_ruder , @nlpnoah -- to tune or not to tune?  Includes guidelines for NLP res‚Ä¶
1106667226265014300,Fri Mar 15 21:23:22 +0000 2019,RT @Miles_Brundage: Very useful looking paper! "To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks," Peters and @‚Ä¶
1106667104621772800,Fri Mar 15 21:22:53 +0000 2019,@_stefan_munich That's interesting. We only tried the first one, not the first and last subword of a sentence.
1106608598120046600,Fri Mar 15 17:30:24 +0000 2019,RT @barbara_plank: I'm hiring - 2y postdoc position on multi-task/multilingual learning (more broadly, transfer learning) for #NLProc avail‚Ä¶
1106607040720523300,Fri Mar 15 17:24:12 +0000 2019,RT @bhutanisanyam1: Here is the fourth, 5-minute summary: This paper compares Feature Extraction‚ùÑÔ∏èVs Fine-Tuningüî•for Transfer Learning in N‚Ä¶
1106515813878640600,Fri Mar 15 11:21:42 +0000 2019,RT @iamtrask: Note: all of the @OpenAI scholars have a research blog  I don't think this is an accident - blogging:  1) forces you to teach‚Ä¶
1106165090540245000,Thu Mar 14 12:08:03 +0000 2019,@saxenaudit @_rockt @ucl Here are the slides of the presentation: https://t.co/E4r7il8EXV
1106106942097948700,Thu Mar 14 08:17:00 +0000 2019,RT @WiNLPWorkshop: This seems like a pretty great opportunity for PhD students to get to visit Peru and hopefully inspire some you g minds‚Ä¶
1105912692823474200,Wed Mar 13 19:25:07 +0000 2019,@mariokostelac üòç
1105577484374999000,Tue Mar 12 21:13:07 +0000 2019,@saxenaudit @_rockt @ucl Working on it. üòä
1105571255007232000,Tue Mar 12 20:48:22 +0000 2019,RT @chipro: A proud misogynist who drag races on the 101, nicknames his female coworker 'PMS', disparages people of color, calls women of B‚Ä¶
1105571194215059500,Tue Mar 12 20:48:07 +0000 2019,If you care about inclusivity and diversity in tech, read this take by @chipro. If we want an inclusive culture, we‚Ä¶ https://t.co/cU3GmyObNE
1105497408052580400,Tue Mar 12 15:54:55 +0000 2019,RT @_rockt: Next, great to have @seb_ruder deliver a Guest Lecture on "Frontiers of Multi-task Learning" in the Natural Language Processing‚Ä¶
1105045873011118100,Mon Mar 11 10:00:41 +0000 2019,New NLP News: GPT-2, Sequence generation in arbitrary order, and much more https://t.co/4hb9iTjUsu (via @revue)
1104523592933535700,Sat Mar 09 23:25:20 +0000 2019,RT @sigrep_acl: Did you miss the ACL deadline? Why not submit your paper to the RepL4NLP workshop. It's a great way to get feedback and dis‚Ä¶
1104517887149117400,Sat Mar 09 23:02:39 +0000 2019,RT @poly_ai: @TechCrunch has posted an article on @poly_ai and what we're all about following the closing of Series A. Check it out!  https‚Ä¶
1103971138756112400,Fri Mar 08 10:50:04 +0000 2019,RT @RicoSennrich: I have an open postdoctoral research positions at the University of Zurich on multi-task and multilingual NLP. Apply by M‚Ä¶
1103600957550723100,Thu Mar 07 10:19:06 +0000 2019,RT @_rockt: Apply for PhD to the new @UCL Centre for Doctoral Training (CDT) in Foundational AI starting in October 2019: https://t.co/SyeK‚Ä¶
1103598126982725600,Thu Mar 07 10:07:51 +0000 2019,@nik0spapp @snsf_ch @nlpnoah @uwnlp @uwcse @UW Awesome! Congrats!
1103332284588019700,Wed Mar 06 16:31:30 +0000 2019,RT @nlpmattg: #nlphighlights 82: an overview of visual reasoning, with @yoavartzi. We talk about datasets like VQA, GQA, NLVR, NLVR2, and C‚Ä¶
1103039537809363000,Tue Mar 05 21:08:13 +0000 2019,@sbmaruf One way to differentiate them is that languages can be seen as domains that don't share features but where‚Ä¶ https://t.co/OsZZFrii7e
1103039095683665900,Tue Mar 05 21:06:28 +0000 2019,@_stefan_munich @conll2019 I'm not involved with the shared task organization. Best to ask via the email on their w‚Ä¶ https://t.co/VXfOhSY4kY
1102703018208886800,Mon Mar 04 22:51:01 +0000 2019,@alienelf @LauraMartinus @ACL2019_Italy Congrats! üéâ
1101530035985494000,Fri Mar 01 17:10:00 +0000 2019,RT @emnlp2019: Are you struggling to make it to the @ACL2019_Italy deadline? Why not give your paper some love, add more analyses, statisti‚Ä¶
1101527189739237400,Fri Mar 01 16:58:42 +0000 2019,RT @sigrep_acl: Workshops are a great venue to have a focused discussion about a topic. If you're working on representation learning, consi‚Ä¶
1101461194479677400,Fri Mar 01 12:36:27 +0000 2019,@atanasovapepa @CopeNLU @IAugenstein Congrats! You couldn't have asked for a better mentor! All the best for your journey!
1101460541850206200,Fri Mar 01 12:33:51 +0000 2019,@bhutanisanyam1 @kaggle That's an awesome milestone! Congrats!! üéâ
1100723853280591900,Wed Feb 27 11:46:31 +0000 2019,In the meantime, here are the slides from my PhD defence presentation on Neural Transfer Learning for Natural Langu‚Ä¶ https://t.co/nriBWbdAT5
1100695698205495300,Wed Feb 27 09:54:39 +0000 2019,@XandaSchofield @vnfrombucharest That's fantastic! Congrats!! üéâ
1100428824339193900,Tue Feb 26 16:14:11 +0000 2019,The camera-ready version of the thesis should be online in a month or so. I hope it'll be useful as a general resou‚Ä¶ https://t.co/OA2mICRoYq
1100427313165021200,Tue Feb 26 16:08:11 +0000 2019,@_aylien @insight_centre Next week, I'll start as a research scientist at @DeepMindAI in London where I'll be worki‚Ä¶ https://t.co/nV28tar5UQ
1100426837027708900,Tue Feb 26 16:06:17 +0000 2019,I'm excited to share some personal news: I've successfully defended my dissertation "Neural Transfer Learning for N‚Ä¶ https://t.co/VplRZLLAjl
1100424333472743400,Tue Feb 26 15:56:20 +0000 2019,RT @johnbreslin: Very proud of Dr @seb_ruder @Insight_Centre @SocialSemantics @_AYLIEN who defended his PhD @NUIGalway @DSIatNUIG Co-Funder‚Ä¶
1100283445245345800,Tue Feb 26 06:36:30 +0000 2019,@royschwartz02 @CseHuji Congrats, Roy!
1098626717265084400,Thu Feb 21 16:53:15 +0000 2019,RT @jeremyphoward: @MarkNeumannnn ULMFit has code, pretrained models, and tutorials, requires 50x fewer resources than GPT, yet *still* bea‚Ä¶
1098253332215402500,Wed Feb 20 16:09:33 +0000 2019,@CalixtoIacer @kchonyc @nyuniversity @AntoineBordes @fb_research @illc @UvA_Amsterdam @raquelfdzrovira That's aweso‚Ä¶ https://t.co/izhhQGrAeN
1098216489788862500,Wed Feb 20 13:43:09 +0000 2019,RT @Thom_Wolf: [1/3] OpenAI #GPT2 has highlighted important developments/issues in Language Generation: -NLG is evolving fast &amp; impact NLP‚Ä¶
1097919139598032900,Tue Feb 19 18:01:35 +0000 2019,Dear OpenAI: Please Open Source Your Language Model Nice piece by @gradientpub's @hughbzhang that makes the case th‚Ä¶ https://t.co/fWWPA2EfmZ
1096800221244330000,Sat Feb 16 15:55:24 +0000 2019,RT @Thom_Wolf: Really excited about our up-coming tutorial on Transfer Learning. I hope it will be recorded &amp; streamed. With the growth of‚Ä¶
1096533671790686200,Fri Feb 15 22:16:14 +0000 2019,RT @NAACLHLT: Six great tutorials are coming up at #naacl2019, check the list out here: https://t.co/s3tI5fABeW
1096356613529788400,Fri Feb 15 10:32:40 +0000 2019,RT @delliott: Another opportunity to study for a Ph.D on language and vision topics in Copenhagen.  Apply by April 1st and name me as a sui‚Ä¶
1096356562497622000,Fri Feb 15 10:32:28 +0000 2019,RT @IAugenstein: I‚Äôm also looking for a postdoc to work on question answering &amp; knowledge base population for 1+ years, based in my new res‚Ä¶
1096336313941483500,Fri Feb 15 09:12:00 +0000 2019,@paperswithcode @Smerity @OpenAI @BFelbo I'd assume other recent big LMs (e.g. Transformer-XL) would achieve compar‚Ä¶ https://t.co/nvbWWc1GBC
1096335334969933800,Fri Feb 15 09:08:07 +0000 2019,@paperswithcode @Smerity @OpenAI All other models on PTB, WT-2, enwik8, text8, WT-103 (not sure about LAMBADA) only‚Ä¶ https://t.co/Ye6dIUwR0p
1096333841093062700,Fri Feb 15 09:02:11 +0000 2019,RT @gregd_nlp: https://t.co/ptRxqhyMG7
1096144396456808400,Thu Feb 14 20:29:24 +0000 2019,RT @paperswithcode: New state-of-the-art for language modeling. The new paper "Better Language Models and their Implications" from @OpenAI‚Ä¶
1096105124894773200,Thu Feb 14 17:53:21 +0000 2019,@AndrewLBeam @OpenAI That'd be really useful to see. Given the quality, my first assumption was that some of the te‚Ä¶ https://t.co/QQuTQJ3I7j
1096103852611387400,Thu Feb 14 17:48:17 +0000 2019,@OpenAI We've seen before that LMs can do zero-shot learning. What I find most exiting about this is the quality of‚Ä¶ https://t.co/3uFfSXEXrK
1096101933436338200,Thu Feb 14 17:40:40 +0000 2019,A new bigger, better language model by @OpenAI: - Scaled-up version of their Transformer (10x params) - Trained on‚Ä¶ https://t.co/F5JkeaP5QW
1096080885080834000,Thu Feb 14 16:17:01 +0000 2019,@anantzoid @revue Thanks for reading! ‚ò∫Ô∏è
1096061585372729300,Thu Feb 14 15:00:20 +0000 2019,@EdwardDixon3 üôèüèª
1095745542305525800,Wed Feb 13 18:04:29 +0000 2019,@STeplitsky @yoavgo @allen_ai Are the slides available somewhere?
1095650122292187100,Wed Feb 13 11:45:20 +0000 2019,@mmbollmann @uni_copenhagen @coastalcph Congrats, Marcel! üéâ
1095256354221183000,Tue Feb 12 09:40:38 +0000 2019,@Tbeltramelli @UizardIO @ForbesUnder30 Awesome! Congrats, man!! üëèüèª
1094989695623274500,Mon Feb 11 16:01:02 +0000 2019,@haryoaw Glad you like it! Thanks for recommending it. :)
1094899001957273600,Mon Feb 11 10:00:38 +0000 2019,New NLP News: BERT, Transfer learning for dialogue, Deep Learning SOTA 2019, Gaussian Processes, VI, NLP lesson cur‚Ä¶ https://t.co/Q82hqfoz6X
1094704317657952300,Sun Feb 10 21:07:02 +0000 2019,RT @sigrep_acl: Our two final confirmed speakers are Raquel Fern√°ndez (University of Amsterdam) and Mohit Bansal (UNC Chapel Hill). Join us‚Ä¶
1094703118837837800,Sun Feb 10 21:02:16 +0000 2019,@thawani_avijit @shashank_bits @gaborangeli @sleepinyourhat @ai2_allennlp @huggingface @nayakne There were a couple‚Ä¶ https://t.co/kioYs88SyT
1094603724763422700,Sun Feb 10 14:27:19 +0000 2019,@tagtog_net Awesome! üí™
1094323766816112600,Sat Feb 09 19:54:52 +0000 2019,RT @mohitban47: Excited to co-chair #CoNLL2019 with @alinev3010 (&amp; our awesome area-chairs/organizers)! Call-for-papers is out, please mark‚Ä¶
1093899219713380400,Fri Feb 08 15:47:52 +0000 2019,RT @karinv: We are seeking 3(!) new PhD students interested in analysing biological database quality using Network Science #NetworkScience‚Ä¶
1093861375502356500,Fri Feb 08 13:17:29 +0000 2019,@strubell Argh. I'm sorry to see that. An incorrect comparison is very annoying and impacts future papers, too. :-/‚Ä¶ https://t.co/xlv9QZG9Zg
1093795230195875800,Fri Feb 08 08:54:39 +0000 2019,RT @USC_ISI: Apply now for summer internships at ISI! Join in groundbreaking research, working on impactful projects with our senior resear‚Ä¶
1093794703747764200,Fri Feb 08 08:52:33 +0000 2019,@vegaspathak @feralvam Check out the resources here: https://t.co/7y3HkzEYlZ I'll also share some new resources in‚Ä¶ https://t.co/S1pMmALmIe
1093623108525805600,Thu Feb 07 21:30:42 +0000 2019,@shrutirij Thanks for the great work! :)
1093620953727283200,Thu Feb 07 21:22:08 +0000 2019,My AAAI 2019 Highlights‚Äîincluding dialogue, reproducibility, question answering, the Oxford style debate, invited t‚Ä¶ https://t.co/FgrOTj6W8l
1093615225960579100,Thu Feb 07 20:59:22 +0000 2019,@iamasharkskin We're trying to add more work on low-resource languages (see the pages on Hindi and Vietnamese), so‚Ä¶ https://t.co/Ng3aBp0cMR
1093556396212269000,Thu Feb 07 17:05:36 +0000 2019,@Abdullah_MA_b üòç
1093536570198712300,Thu Feb 07 15:46:49 +0000 2019,New on NLP-progress: A comprehensive overview of the state-of-the-art in text simplification, due to @feralvam üëèüèª  https://t.co/SIMjB3tl40
1093534102970646500,Thu Feb 07 15:37:01 +0000 2019,RT @_aylien: The deadline for the AYLIEN-based PhD/MSc with @IrishResearch is fast approaching! Get in touch by next Wednesday (Feb 13th) w‚Ä¶
1093447204214313000,Thu Feb 07 09:51:43 +0000 2019,@danaludwig Thank you! We haven't run a one-to-one comparison all the datasets, but as our model is similar to ELMo‚Ä¶ https://t.co/bkeb0AUt4I
1093446555023990800,Thu Feb 07 09:49:08 +0000 2019,@Aloksaan @NAACLHLT @mattthemathman @swabhz @Thom_Wolf In the past, not all sessions have been recorded. I hope tha‚Ä¶ https://t.co/ykubYpaI65
1093428298355486700,Thu Feb 07 08:36:35 +0000 2019,RT @dmimno: Our two-year postdoc developing data science curriculum for humanities at Cornell is now available! https://t.co/HBjX2zjyIq
1093202655046971400,Wed Feb 06 17:39:58 +0000 2019,@amartyaamp @NAACLHLT @mattthemathman @swabhz @Thom_Wolf Not entirely sure. I hope so.
1093058965674565600,Wed Feb 06 08:09:00 +0000 2019,We hope you will join us at @NAACLHLT or @ACL2019_Italy. Slides will be made available online.
1093058870598082600,Wed Feb 06 08:08:37 +0000 2019,At @ACL2019_Italy, @licwu, Anders S√∏gaard, and I will be presenting on unsupervised cross-lingual representation le‚Ä¶ https://t.co/yYyZS40mjx
1093058769947365400,Wed Feb 06 08:08:13 +0000 2019,At @NAACLHLT, @mattthemathman, @swabhz, @Thom_Wolf, and I will be discussing transfer learning in NLP. Including an‚Ä¶ https://t.co/QAOmiCBII5
1093058692524769300,Wed Feb 06 08:07:55 +0000 2019,I'm super thrilled to be presenting two tutorials with awesome colleagues at NLP conferences this year!
1092878360320061400,Tue Feb 05 20:11:20 +0000 2019,RT @artetxem: Interesting comparative study of cross-lingual word embeddings beyond bilingual lexicon induction. Nice to see that our tool‚Ä¶
1092746318664093700,Tue Feb 05 11:26:39 +0000 2019,RT @_aylien: We've opened another position at AYLIEN: complete a PhD or MSc while working on our Research team! Get in touch fast with a CV‚Ä¶
1092582921406300200,Tue Feb 05 00:37:22 +0000 2019,Fantastic in-depth piece by @skynet_today on job loss due to AI. The main argument: AI will (most likely) *not be s‚Ä¶ https://t.co/mSfw5VIRHO
1092558205433966600,Mon Feb 04 22:59:09 +0000 2019,RT @math_rachel: The 4 Biggest Open Problems in NLP by @seb_ruder, based on @DeepIndaba panel https://t.co/pDAaSFMzQj
1092429607351930900,Mon Feb 04 14:28:09 +0000 2019,RT @PfeiffJo: Amazing paper by @gg42554 R. Litschko @seb_ruder and @licwu that gives an extensive overview of cross lingual embedding model‚Ä¶
1092212293515890700,Mon Feb 04 00:04:37 +0000 2019,@EdwardDixon3 @brandonhilkert Thanks for the pointer! ‚ò∫Ô∏è
1092210164344676400,Sun Feb 03 23:56:10 +0000 2019,@sudharsan2020 Seems like you have 2 problems: 1) Determining an appropriate training data size; 2) interpreting mo‚Ä¶ https://t.co/b6aggjJ1Ti
1092209484485734400,Sun Feb 03 23:53:28 +0000 2019,@saxenaudit @BasmaEAB Just uploaded them. You can find them here: https://t.co/daUnJF5YhC
1092124248712212500,Sun Feb 03 18:14:46 +0000 2019,@amartyaamp Yep. That's in the works. :)
1091937666243805200,Sun Feb 03 05:53:21 +0000 2019,@EliasWalyBa @jeremyphoward Ow! Thank you so much! üòç
1091601766574043100,Sat Feb 02 07:38:36 +0000 2019,@mithunpaul08 Yep. We're trying to merge in the data from nlpprogress so that we just need to maintain one resource.
1091601535648227300,Sat Feb 02 07:37:41 +0000 2019,@saikrishnaklu @ursachi @rbstojnic @rosstaylor90 Yes, we're trying to merge in the data so that we ultimately just‚Ä¶ https://t.co/HjE9gHQNTf
1091600925095952400,Sat Feb 02 07:35:16 +0000 2019,RT @BasmaEAB: @seb_ruder talking about multi-task architecture learning at #AAAI19 https://t.co/IQHgpGDRRw
1091441761887543300,Fri Feb 01 21:02:48 +0000 2019,@ursachi Thanks! The ones who deserve the beer, though, are @rbstojnic and @rosstaylor90 who created the resource.
1091416940751405000,Fri Feb 01 19:24:10 +0000 2019,This is a super cool resource: Papers With Code now includes 950+ ML tasks, 500+ evaluation tables (including SOTA‚Ä¶ https://t.co/z4dtyssnBp
1091410604940611600,Fri Feb 01 18:59:00 +0000 2019,RT @DataScienceNIG: Take your NLP to the next level with LASER, a newly open-sourced @facebookai  natural language processing toolkit, whic‚Ä¶
1091152177802821600,Fri Feb 01 01:52:06 +0000 2019,RT @boknilev: @SanhEstPasMoi from @huggingface on A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks at #AAAI19‚Ä¶
1090800536905932800,Thu Jan 31 02:34:48 +0000 2019,@dennybritz @deliprao @yoavgo Same problem here. Any recommendations?
1090738702089838600,Wed Jan 30 22:29:06 +0000 2019,RT @TMLeiden: You probably didn't know this: @Zalando has a research division and their method for Named Entity Recognition is currently ra‚Ä¶
1090737694156582900,Wed Jan 30 22:25:05 +0000 2019,@icecold_spinbot That's a great question. I'm actually not aware of any resource that talks about this in-depth. Pr‚Ä¶ https://t.co/mBgdjMmPEJ
1090662827688263700,Wed Jan 30 17:27:36 +0000 2019,RT @anotherjohng: We're looking for a Research Scientist to join our team, get in touch if you're interested (DMs open): https://t.co/pHlnm‚Ä¶
1090464971421405200,Wed Jan 30 04:21:23 +0000 2019,Overall, that was a really fun debate. While I mostly tweeted about the comments that elicited laughs, there were s‚Ä¶ https://t.co/Fuhsx8gJ96
1090460626885857300,Wed Jan 30 04:04:07 +0000 2019,Oren: "Do we really think that we can solve fundamental problems of science just with multiplying and gradient descent?"
1090459122854588400,Wed Jan 30 03:58:09 +0000 2019,Oren: "If you think of the short term, publishing papers, ML is great. It‚Äôs a paper publishing methodology."
1090458894600568800,Wed Jan 30 03:57:14 +0000 2019,Michael: "Peter Norvig wears a Hawaii shirt almost every time he gives a talk. How could he have missed #AAAI2019 (in Hawaii)?"
1090458208454434800,Wed Jan 30 03:54:31 +0000 2019,Check @boknilev's tweets for some of the more serious statements: https://t.co/PxYdYRai19
1090457907630460900,Wed Jan 30 03:53:19 +0000 2019,Michael: "Let's talk about puns. What do you call a green cow in a field? Invisible. If you train a NN on a tons of‚Ä¶ https://t.co/KY7sUdkh9F
1090457290291826700,Wed Jan 30 03:50:52 +0000 2019,Michael: "I am sad that Peter and Jen are on the pro ML side. What happened to you guys? Your theses were so good."
1090457084112429000,Wed Jan 30 03:50:03 +0000 2019,Oren: "Let's look at key figures in the movements. The Trump movement has Roger Stone who got arrested by the FBI l‚Ä¶ https://t.co/jcjaMOxwH4
1090456360318251000,Wed Jan 30 03:47:10 +0000 2019,@ProfJenNeville @etzioni @mlittmancs @boknilev Some highlights: Oren: "There are 3 kinds of populist movements in o‚Ä¶ https://t.co/sIELwqHLT8
1090455226870423600,Wed Jan 30 03:42:40 +0000 2019,@erfannoury Yeah. Hmm...
1090454767438946300,Wed Jan 30 03:40:50 +0000 2019,Oxford style debate between @ProfJenNeville, Peter Stone, @etzioni, and @mlittmancs at #AAAI2019 on whether AI comm‚Ä¶ https://t.co/SdBHvopXu0
1090452035680391200,Wed Jan 30 03:29:59 +0000 2019,RT @boknilev: The #AAAI19 debate on The Future of AI is about to start: "The AI community today should continue to focus mostly on ML metho‚Ä¶
1089991881402904600,Mon Jan 28 21:01:30 +0000 2019,This is a cool Colaboratory notebook that shows how you can apply ML and NLP to the content of your own @feedly fee‚Ä¶ https://t.co/YMXuY7mLKX
1089988133322600400,Mon Jan 28 20:46:36 +0000 2019,@PfeiffJo @zalandoresearch @UKPLab Well done! üëèüèª
1089986213719683100,Mon Jan 28 20:38:58 +0000 2019,@EliasWalyBa @bayethiernodiop @ylecun @JeffDean @fchollet @francesc @dickoah @jeremyphoward @crowdglory‚Ä¶ https://t.co/DKqcZ1T3AI
1089984670664601600,Mon Jan 28 20:32:51 +0000 2019,@EliasWalyBa @bayethiernodiop @ylecun @JeffDean @fchollet @francesc @dickoah @jeremyphoward @crowdglory‚Ä¶ https://t.co/4UKkEURkR4
1089368163538489300,Sun Jan 27 03:43:04 +0000 2019,@Iluvds1 Yep, training a custom named entity recognition model seems like your best bet for this.
1089156059187445800,Sat Jan 26 13:40:14 +0000 2019,@sai_prasanna Do you mean the size of the labelled data, unlabeled data, sampling size, etc? In most cases, it depe‚Ä¶ https://t.co/mLxbozCQ1v
1089155520244506600,Sat Jan 26 13:38:06 +0000 2019,@Iluvds1 Could you be more specific? Do you try to remove personal information? If so, what kind of information are‚Ä¶ https://t.co/Ah4v14uzYs
1088777385154105300,Fri Jan 25 12:35:31 +0000 2019,RT @coastalcph: Anders S√∏gaard/@coastalcph is looking for a PhD candidate with a strong background in NLP/ML to work on semantic parsing, Q‚Ä¶
1088553054335631400,Thu Jan 24 21:44:07 +0000 2019,The #AlphaStar games were super interesting to watch. The model uses techniques from NLP, such as a Transformer and‚Ä¶ https://t.co/enMkwaxAjv
1088412926803873800,Thu Jan 24 12:27:18 +0000 2019,RT @adamshamsudeen: @seb_ruder @Smerity @jeremyphoward We were working on the Malayalam Language model and text classifier for the past few‚Ä¶
1088394648761061400,Thu Jan 24 11:14:40 +0000 2019,RT @_aylien: Great to be featured on @siliconrepublic's list of 25 European #deeptech #startups to watch in 2019, alongside some other exci‚Ä¶
1088375986025123800,Thu Jan 24 10:00:30 +0000 2019,Natural Questions: A new QA dataset consisting of 300,000+ naturally occurring questions (posed to Google search) w‚Ä¶ https://t.co/sBxZx3pBoV
1088124526842519600,Wed Jan 23 17:21:18 +0000 2019,A comprehensive overview and break-down of 14 NLP research paper highlights from 2018 https://t.co/ap00YX9Oc9
1088124526842519600,Wed Jan 23 17:21:18 +0000 2019,A comprehensive overview and break-down of 14 NLP research paper highlights from 2018 https://t.co/ap00YX9Oc9
1088080133641158700,Wed Jan 23 14:24:54 +0000 2019,RT @delliott: I have funding for a Ph.D student to work in the general area of multimodal machine learning from images, videos, audio, and‚Ä¶
1088079634049232900,Wed Jan 23 14:22:54 +0000 2019,RT @alex_conneau: 1/4 XLM: Cross-lingual language model pretraining. We extend BERT to the cross-lingual setting. New state of the art on X‚Ä¶
1087992691034595300,Wed Jan 23 08:37:26 +0000 2019,@anas_ant @ND_CSE @davidweichiang Congrats!! üéâ
1087676327845601300,Tue Jan 22 11:40:19 +0000 2019,RT @AIMS_Next: The wait is over! The African Masters in Machine intelligence (AMMI) applications are now open!  Apply to become a machine I‚Ä¶
1087389292207525900,Mon Jan 21 16:39:44 +0000 2019,If you are considering a postdoc in #NLProc, then this is a great opportunity to work with amazing people on cuttin‚Ä¶ https://t.co/lOdgNi88ck
1087382709494779900,Mon Jan 21 16:13:35 +0000 2019,@joabingel @IAugenstein 2. A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks by‚Ä¶ https://t.co/EIKibuBQi7
1087382387015729200,Mon Jan 21 16:12:18 +0000 2019,I'll be at #AAAI19 next week presenting two papers on multi-task learning: 1. Latent Multi-task Architecture Learni‚Ä¶ https://t.co/3uAK8jibCe
1087363780458303500,Mon Jan 21 14:58:22 +0000 2019,RT @barbara_plank: We have a 2 year postdoc opening @ITUkbh on transfer learning (MTL, cross-lingual learning, domain adaptation) for Natur‚Ä¶
1087334886422298600,Mon Jan 21 13:03:33 +0000 2019,@sai_prasanna @jeremyphoward @Thom_Wolf @math_rachel @deliprao Yeah, not sure either. Maybe have a look at the proc‚Ä¶ https://t.co/UzBNy6IcZU
1087330852323897300,Mon Jan 21 12:47:31 +0000 2019,@sai_prasanna @jeremyphoward @Thom_Wolf @math_rachel @deliprao That'd be awesome! It just seems hard to do in pract‚Ä¶ https://t.co/AQdCmzXYlL
1087327378316185600,Mon Jan 21 12:33:43 +0000 2019,@_mbartolo Very nice! üëèüèªLooking forward to the ML Index. ‚ò∫Ô∏è
1087109800373248000,Sun Jan 20 22:09:08 +0000 2019,@sjmielke @GoogleAI @BrianRhoArc Congrats, Sebastian! üéâ
1087063843719839700,Sun Jan 20 19:06:31 +0000 2019,@EliasWalyBa @bayethiernodiop Thanks for clarifying. No, I haven't seen this before. Maybe someone else who has seen it can chip in.
1087047805124911100,Sun Jan 20 18:02:47 +0000 2019,@bayethiernodiop @EliasWalyBa Sorry, I'm not aware of any Harden's Law. I also don't completely follow: 0 &lt; \beta &lt;‚Ä¶ https://t.co/ZmB0Bthjeg
1086187745142366200,Fri Jan 18 09:05:13 +0000 2019,@barbara_plank @zeljkoagic @amazon That's great news! Congrats, Barbara!
1086000507116814300,Thu Jan 17 20:41:12 +0000 2019,RT @sigrep_acl: This year, we'll be accepting short and long papers on many exciting topics, including compositionality, tensor decompositi‚Ä¶
1085956055832686600,Thu Jan 17 17:44:34 +0000 2019,RT @DeepMindAI: If you're from an underrepresented group, interested in AI, and considering studying for a Masters, there's still time to a‚Ä¶
1085919758313275400,Thu Jan 17 15:20:20 +0000 2019,RT @DeepIndaba: We are actively seeking sponsors for the 2019 Deep Learning Indaba - this year in Nairobi üá∞üá™ If you, your organisation, or‚Ä¶
1085855893835706400,Thu Jan 17 11:06:33 +0000 2019,@pablo_gps @DeepIndaba @alienelf @omojumiller @sgouws @bernardt_d @HermanKamper Thanks a lot for the kind feedback!
1085819279579201500,Thu Jan 17 08:41:04 +0000 2019,@artetxem Congrats, Mikel! That's super well deserved! üéâ
1085674395908083700,Wed Jan 16 23:05:21 +0000 2019,RT @kdnuggets: 10 Exciting Ideas of 2018 in #NLP  https://t.co/C7kwvePCAL https://t.co/ntcZynul9S
1085663234089197600,Wed Jan 16 22:21:00 +0000 2019,RT @barbara_plank: We are still looking for sponsors who help make WiDS Copenhagen an unforgettable event (Women in Data Science, a one day‚Ä¶
1085650484923314200,Wed Jan 16 21:30:20 +0000 2019,RT @sigrep_acl: As in previous editions, we'll have outstanding speakers, spotlight presentations, a poster session, a panel discussion, an‚Ä¶
1085649472737009700,Wed Jan 16 21:26:19 +0000 2019,RT @rctatman: I promised this a while ago and I finally delivered; I wrote a gosh-darn magnum opus on what I don't like about BLEU &amp; some a‚Ä¶
1085447498574032900,Wed Jan 16 08:03:44 +0000 2019,RT @JeffDean: Nice write-up of @alexk_ai's experience teaching AI in Somaliland.  I exchanged emails with him about using some slides for h‚Ä¶
1085309572259168300,Tue Jan 15 22:55:40 +0000 2019,New blog post ‚Äì The 4 Biggest Open Problems in NLP Based on our expert survey and the panel discussion at the NLP s‚Ä¶ https://t.co/zSF20jOfdx
1085290881492676600,Tue Jan 15 21:41:24 +0000 2019,@davidelnunes If you want feedback or its early stage, then I'd definitely recommend a workshop. RepL4NLP has prett‚Ä¶ https://t.co/404klVgPuG
1085287021311131600,Tue Jan 15 21:26:04 +0000 2019,RT @sigrep_acl: Join us for the 4th Workshop on Representation Learning for NLP at @ACL2019_Italy on August 2!  The first call for papers i‚Ä¶
1085286955724738600,Tue Jan 15 21:25:48 +0000 2019,For anyone working on or interested in representation learning, this is a great workshop to attend + submit to. https://t.co/V1at1np0VB
1085221160822992900,Tue Jan 15 17:04:21 +0000 2019,RT @coastalcph: #CoAStaL looking for a 1y postdoc in core #nlproc to start asap - w one day a week in big industry ML team. Email soegaard@‚Ä¶
1085184143758024700,Tue Jan 15 14:37:16 +0000 2019,How I went to Somaliland and... Taught Artificial Intelligence --  Some useful advice and material from @alexk_ai f‚Ä¶ https://t.co/6lqknizXBZ
1085179546603802600,Tue Jan 15 14:19:00 +0000 2019,@_mbartolo @uclmr @riedelcastro @_rockt @PSH_Lewis @backprop2seed @PMinervini @Johannes_Welbl @mindjimmy @egrefen A‚Ä¶ https://t.co/jYlTi58YGh
1084891005025947600,Mon Jan 14 19:12:26 +0000 2019,RT @kdnuggets: Trends in Natural Language Processing with Sebastian Ruder - TWiML Talk #216 https://t.co/awpCm0sitq https://t.co/QpgDjdtfK9
1084767152878612500,Mon Jan 14 11:00:17 +0000 2019,New NLP Newsletter - Challenges in Few-shot learning; 2019 predictions; JAX; Explainable models; MT reading list; F‚Ä¶ https://t.co/mYNbe6fCYW
1084546003037052900,Sun Jan 13 20:21:31 +0000 2019,@iandanforth @abhshkdz This would be super cool, but the parsing part is still an open research problem. @allen_ai'‚Ä¶ https://t.co/XP2LLn5kIi
1083797188323373000,Fri Jan 11 18:46:00 +0000 2019,If you're interested in interpretability and better understanding #NLProc models üîé, read this excellent TACL '19 su‚Ä¶ https://t.co/VSiqQxZKO4
1083651693588422700,Fri Jan 11 09:07:51 +0000 2019,"If you‚Äôre building an NLP algorithm today, don‚Äôt do it on your own! Start from the best open-sourced algorithms [.‚Ä¶ https://t.co/3nI55Jjkqp
1083282519817109500,Thu Jan 10 08:40:53 +0000 2019,RT @artetxem: 2018 was a productive year! I am 3rd in this ranking with 5 first author papers in top ML/NLP venues https://t.co/hnPSj0WYOf‚Ä¶
1082976529326444500,Wed Jan 09 12:24:59 +0000 2019,RT @MarekRei: Analysis of ML and NLP publication statistics from 2018. https://t.co/ph1HCKu5bg #machinelearning #nlproc https://t.co/Le6sBp‚Ä¶
1082920079405301800,Wed Jan 09 08:40:41 +0000 2019,RT @acl_srw: ACL 2019 Student Research Workshop First Call for Papers: üî•üî•üî•  üëáüëáüëá https://t.co/zK7cc8n9mS üëÜüëÜüëÜ  We welcome paper submissions f‚Ä¶
1082735493228519400,Tue Jan 08 20:27:12 +0000 2019,@srchvrs I mention in the review that the Collobert &amp; Weston paper already did word embeddings. There are other ear‚Ä¶ https://t.co/X5YkiqBJWv
1082686796604080100,Tue Jan 08 17:13:42 +0000 2019,@emilymbender @Thom_Wolf @tolgab0 @r_speer Btw, thanks for the thread and the feedback. :)
1082686569859960800,Tue Jan 08 17:12:48 +0000 2019,@emilymbender @Thom_Wolf @tolgab0 @r_speer Yes, that list is non-exhaustive and was mainly based on suggestions fro‚Ä¶ https://t.co/lAExKWc43B
1082685884342915100,Tue Jan 08 17:10:04 +0000 2019,@emilymbender @Thom_Wolf Fair point. Were you looking for a reference that classic word representations such as Bro‚Ä¶ https://t.co/qHJYYaqdUs
1082685079565021200,Tue Jan 08 17:06:52 +0000 2019,RT @emilymbender: Thanks @seb_ruder for this helpful overview!   A Review of the Neural History of Natural Language Processing https://t.co‚Ä¶
1082576459263344600,Tue Jan 08 09:55:15 +0000 2019,RT @algo_luca: It was a fun ride and @seb_ruder's content is awesome. On a side note, it was a good occasion to reflect on the way we talk,‚Ä¶
1082414234183651300,Mon Jan 07 23:10:38 +0000 2019,RT @nlpmattg: Back from the holidays, and just in time for NAACL review season, here is #nlphighlights 77: @nlpnoah on how to review papers‚Ä¶
1082413816485425200,Mon Jan 07 23:08:58 +0000 2019,For all Italian speakers, here is an Italian translation of my review of the neural history of NLP (‚Ä¶ https://t.co/E0FjvNs70g
1082270093395091500,Mon Jan 07 13:37:52 +0000 2019,RT @Thom_Wolf: A workshop on novel methods for training &amp; evaluating language generation models  üì£First Call for Papers Submission deadline‚Ä¶
1082241733126209500,Mon Jan 07 11:45:10 +0000 2019,RT @nikaletras: A fully funded 3-year #PhD #studentship is available (Amazon Alexa Fellowship scheme). If you like to do research in #nlpro‚Ä¶
1081237689389723600,Fri Jan 04 17:15:28 +0000 2019,@sudarshan0522 @jeremyphoward Sure. :)
1081102624777216000,Fri Jan 04 08:18:46 +0000 2019,RT @dhruvghulati: We are hiring a data project manager at Factmata, do let me know if you know someone (or are someone) who might be a good‚Ä¶
1080790348698452000,Thu Jan 03 11:37:53 +0000 2019,@zhongxuank Hi Zhong, I'm really glad that you've found the article and the newsletter helpful. I wish you all the‚Ä¶ https://t.co/hxxsu0YuoX
1080789908422361100,Thu Jan 03 11:36:08 +0000 2019,RT @zhongxuank: For day 2 of my #YearOfNLP I wanted to get an overview of relevant concepts and recent trends in NLP. @seb_ruder's excellen‚Ä¶
1080774594083078100,Thu Jan 03 10:35:17 +0000 2019,RT @twimlai: In our conversation with Sebastian Ruder, we cover a bunch of interesting papers spanning topics such as pre-trained language‚Ä¶
1080581203537858600,Wed Jan 02 21:46:49 +0000 2019,RT @EmmanuelAmeisen: 2018 has been a continuing flurry of exciting work in Machine Learning. If you are interested in being part of the fie‚Ä¶
1080222575119212500,Tue Jan 01 22:01:46 +0000 2019,"First and foremost, we want to have the system that understands children's stories"  The Art of AI Storytelling‚Äîgr‚Ä¶ https://t.co/bpnnGoSM92
1080104179216121900,Tue Jan 01 14:11:18 +0000 2019,@superrzk @samcharrington Thanks! Happy new year to you too!
1080066756536909800,Tue Jan 01 11:42:36 +0000 2019,@NarahariNitw @kdnuggets Yep. I'm mentioning them here: https://t.co/roV5pFzMQe Both of them are not that up-to-dat‚Ä¶ https://t.co/MAIy8C6ZCX
1080064256039620600,Tue Jan 01 11:32:39 +0000 2019,@neozero497 @samcharrington ‚ò∫Ô∏è
1080063301927743500,Tue Jan 01 11:28:52 +0000 2019,Happy New Year to you all! If you want to know what were my highlights in NLP last year and some of the things I lo‚Ä¶ https://t.co/olZ68Q81QO
1079837366385365000,Mon Dec 31 20:31:05 +0000 2018,RT @AnimaAnandkumar: Summarizing my experiences in 2018. #AI #ML #research #products #protestNIPS #meTooSTEM  https://t.co/LUcSl54UOT
1079813873652457500,Mon Dec 31 18:57:44 +0000 2018,RT @twimlai: Today we're joined by @seb_ruder of @nuigalway and @_aylien, to discuss trends in Natural Language Processing in 2018 and beyo‚Ä¶
1079497887602622500,Sun Dec 30 22:02:07 +0000 2018,RT @riedelcastro: Interested in a PhD at @UCLMR to work with @_rockt , https://t.co/zpSkmBuXBa, @egrefen, @PMinervini and me, as well as an‚Ä¶
1078724404044066800,Fri Dec 28 18:48:34 +0000 2018,RT @kdnuggets: Repository to track the progress in Natural Language Processing, including the datasets and the current state-of-the-art for‚Ä¶
1078237654079164400,Thu Dec 27 10:34:24 +0000 2018,RT @artetxem: Check out our new paper "Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond" (w/ Holg‚Ä¶
1076933713009827800,Sun Dec 23 20:13:00 +0000 2018,@mariokostelac @msikic Besides what was mentioned already, also try https://t.co/PHeaqbyVJL
1076933361699045400,Sun Dec 23 20:11:36 +0000 2018,@kk54709 Depending on if you only care about entities, this is either entity extraction / disambiguation or word se‚Ä¶ https://t.co/GIWkiTH4uG
1076345801033556000,Sat Dec 22 05:16:51 +0000 2018,RT @PiotrCzapla: How about 30% reduction of error rate for Arabic Sentiment Analysis by @akhooli using #ULMFiT architecture (by @jeremyphow‚Ä¶
1076186092850896900,Fri Dec 21 18:42:13 +0000 2018,RT @allen_ai: 2018 was a big year for #NLP! Check out @seb_ruder's collection of 10 Exciting Ideas of 2018 in NLP, including work on word e‚Ä¶
1076175790386098200,Fri Dec 21 18:01:17 +0000 2018,@_KarenHao Looking forward to those words on NLP. üòÄ
1076060536742314000,Fri Dec 21 10:23:18 +0000 2018,ML Breakthroughs in 2018 &amp; Trends for 2019: Comprehensive article by @AnalyticsVidhya about highlights in NLP, CV,‚Ä¶ https://t.co/qgBViwxff2
1076057642076913700,Fri Dec 21 10:11:48 +0000 2018,RT @RicoSennrich: I have two funded PhD positions at the University of Zurich on multi-task and multilingual NLP. Apply by January 20! http‚Ä¶
1075886659739312100,Thu Dec 20 22:52:23 +0000 2018,RT @ChrisGPotts: We're now accepting applications for the 6th CSLI Undergraduate Summer Internship Program, which places students in Stanfo‚Ä¶
1075735990214553600,Thu Dec 20 12:53:40 +0000 2018,@knmnyn Thanks for your hard work, Min!
1075677625601929200,Thu Dec 20 09:01:45 +0000 2018,RT @tryolabs: üéá¬†This year was a big one for #DeepLearning! Nice overview of some major accomplishments in 2018, written by @JCoutoNLP. http‚Ä¶
1075484767666270200,Wed Dec 19 20:15:24 +0000 2018,10 Exciting Ideas of 2018 in NLP: A collection of 10 ideas that I found exciting and impactful this year‚Äîand that w‚Ä¶ https://t.co/hghhCl4MNc
1075438488202461200,Wed Dec 19 17:11:30 +0000 2018,RT @alanmnichol: TIL: every minute someone downloads @Rasa_HQ ! Excited for us to hit these milestones - credit to our awesome community üó∫üåé‚Ä¶
1075328603192213500,Wed Dec 19 09:54:52 +0000 2018,@bayethiernodiop Well, there's an arXiv paper, which we updated last year (https://t.co/Ivhq5689VR). We'll update t‚Ä¶ https://t.co/FenfuDtpRm
1075143155434033200,Tue Dec 18 21:37:58 +0000 2018,RT @Varal7: I was not satisfied with any of the existing tools to collect tagging annotations for Information Extraction / Named Entity Rec‚Ä¶
1075126758129758200,Tue Dec 18 20:32:48 +0000 2018,RT @FastForwardLabs: Our favorite research, talks, art and other stuff from 2018 https://t.co/HFJX0DAqQl
1075085778819211300,Tue Dec 18 17:49:58 +0000 2018,The slides by @strubell on Linguistically-Informed Self-Attention for Semantic Role Labeling (best paper, EMNLP 201‚Ä¶ https://t.co/kFYvOZhDad
1075067637959397400,Tue Dec 18 16:37:53 +0000 2018,@UnderdogGeek @NirantK Good idea! Pointers to key datasets and state-of-the-art methods would go a long way. Feel f‚Ä¶ https://t.co/oa4vQS17o0
1075067288078954500,Tue Dec 18 16:36:29 +0000 2018,@Valentin_Gut @Varal7 @amazonmturk Thanks for the pointer! Noted. ‚úçÔ∏è
1075041889810874400,Tue Dec 18 14:55:34 +0000 2018,RT @DataScienceNIG: Great time for NLP as Facebook open sources PyText, an NLP modeling framework that currently¬†produces more than a billi‚Ä¶
1075027088082657300,Tue Dec 18 13:56:45 +0000 2018,RT @JCornebise: Very proud to reveal today the results of our first project of many in the partnership between @amnesty incredible Human Ri‚Ä¶
1074940680735215600,Tue Dec 18 08:13:24 +0000 2018,RT @math_rachel: videos of #NeurIPS2018 invited talks: https://t.co/YDBAKS02bd videos of more sessions here: https://t.co/1dXt7m1bAC h/t @s‚Ä¶
1074759901245726700,Mon Dec 17 20:15:03 +0000 2018,RT @StateoftheartAI: We've expanded the NLP section to incorporate https://t.co/FNziWCQCLu's results! Thank you so much to @seb_ruder and a‚Ä¶
1074747537553322000,Mon Dec 17 19:25:55 +0000 2018,@bayethiernodiop The best one is NLP highlights by @nlpmattg and @waleed_ammar: https://t.co/9itPNaicA3 @twimlai al‚Ä¶ https://t.co/Uo9dYemQrc
1074725281636597800,Mon Dec 17 17:57:29 +0000 2018,From meeting at @DeepIndaba to a #NeurIPS workshop paper in &lt; 3 months. This is an inspiring post full of valuable‚Ä¶ https://t.co/9J8tccdJcW
1074706588487860200,Mon Dec 17 16:43:12 +0000 2018,@MatthewTeschke Thanks for the kind words! üòç
1074620282403516400,Mon Dec 17 11:00:15 +0000 2018,New NLP Newsletter: üéÖNeurIPS 2018; The nature of research; Advances in image generation, protein folding, and RL‚Ä¶ https://t.co/0mBpqDMywT
1074415652897783800,Sun Dec 16 21:27:07 +0000 2018,@SrGee316 Thanks for the shout-out! Glad you liked the article. :)
1074415574179082200,Sun Dec 16 21:26:49 +0000 2018,@WWRob Thanks for the shout-out! :)
1073982975291859000,Sat Dec 15 16:47:49 +0000 2018,RT @daniilmagpie: I had a hard time beating my own baseline for entity linking that used hand-engineered features. https://t.co/9tadZ7kobG
1073981395314335700,Sat Dec 15 16:41:32 +0000 2018,@sudharsan2020 I'd recommend you to check out Danqi Chen's PhD thesis (https://t.co/dKLaWc7gBt) and Patrick Lewis'‚Ä¶ https://t.co/nv8gFSARHg
1073867916313952300,Sat Dec 15 09:10:37 +0000 2018,@msamonov Feel free to shoot me an email.
1073867609110659100,Sat Dec 15 09:09:24 +0000 2018,@bayethiernodiop In syntax tree parsing, you try to produce a tree for a sentence that represents its syntactic str‚Ä¶ https://t.co/jvjAyoUylX
1073865772064272400,Sat Dec 15 09:02:06 +0000 2018,@EdwardDixon3 Agreed. Actually had talked with @honnibal about this. Great to see it published!
1073733976857198600,Sat Dec 15 00:18:23 +0000 2018,@emilymbender @mmitchell_ai @verena_rieser @DeepIndaba @redpony @RichardSocher Thanks for mentioning that! You're totally right.
1073732618959708200,Sat Dec 15 00:13:00 +0000 2018,@mmitchell_ai @emilymbender @verena_rieser @DeepIndaba @redpony @RichardSocher We asked a diverse group of people f‚Ä¶ https://t.co/3JJrRR07Re
1073658409302536200,Fri Dec 14 19:18:07 +0000 2018,RT @kdnuggets: NLP Breakthrough Imagenet Moment has arrived @seb_ruder https://t.co/SxstTPD4wy https://t.co/AfXnpLOFm5
1073619913628155900,Fri Dec 14 16:45:08 +0000 2018,Check out @danqi_chen's just published PhD thesis for an up-to-date overview of the world (and future) of neural re‚Ä¶ https://t.co/BgOxiIkEPi
1073237880653393900,Thu Dec 13 15:27:05 +0000 2018,@JustinTimesUK @RichardSocher This is interesting. Haven't looked much into that. Do you have a reference / a point‚Ä¶ https://t.co/f2EfFygN8D
1073161966691209200,Thu Dec 13 10:25:25 +0000 2018,@Benudek I haven't seen any notebooks applying LMs to NMT yet. Probably best to start with notebooks for pretrained‚Ä¶ https://t.co/XveFkFkKMG
1073161524242456600,Thu Dec 13 10:23:40 +0000 2018,@JustinTimesUK @RichardSocher The only one I'm aware of is the PPDB (https://t.co/u2VXw9aBWg) consisting of 220M pa‚Ä¶ https://t.co/AxfcZpxttf
1073160564577316900,Thu Dec 13 10:19:51 +0000 2018,@sudharsan2020 I'm not really too familiar with text mining on clinical text unfortunately. :-/ Maybe these clinica‚Ä¶ https://t.co/wPjXJFyo0g
1073159469931073500,Thu Dec 13 10:15:30 +0000 2018,@saqibns @DeepIndaba @redpony @RichardSocher @sgouws @HermanKamper These can range from using strong hand-engineere‚Ä¶ https://t.co/6LGouUWFiz
1073158834003370000,Thu Dec 13 10:12:59 +0000 2018,@saqibns @DeepIndaba @redpony @RichardSocher Thanks! This was done together with @sgouws and @HermanKamper who dese‚Ä¶ https://t.co/2PtjbW4flr
1073158366355210200,Thu Dec 13 10:11:07 +0000 2018,RT @WebDevSugandha: Here‚Äôs a summary from @PacktPub on the tech interview conducted by @seb_ruder @HermanKamper and @sgouws on the state of‚Ä¶
1072982279927070700,Wed Dec 12 22:31:25 +0000 2018,RT @math_rachel: Two developments in AI in 2018: 1. the successful application of transfer learning to NLP 2. growing attention to dystopia‚Ä¶
1072472492080590800,Tue Dec 11 12:45:42 +0000 2018,@DeepIndaba @redpony @RichardSocher Thanks to my co-organizers @HermanKamper and @sgouws for their great work and t‚Ä¶ https://t.co/2j5dBMlrFq
1072442557764960300,Tue Dec 11 10:46:45 +0000 2018,@Pythonner @DeepIndaba @redpony @RichardSocher Sorry about that. No-one was required to provide answers to all ques‚Ä¶ https://t.co/FajVwO4l9i
1072431709243744300,Tue Dec 11 10:03:38 +0000 2018,"What are the 3 biggest open problems in NLP?" We had asked experts a few simple but big questions for the NLP sess‚Ä¶ https://t.co/1CuVByY8yr
1072415167714115600,Tue Dec 11 08:57:55 +0000 2018,RT @seth_stafford: Has protein folding just had its ‚ÄúImageNet moment‚Äù(*)?  Some soul-searching by @MoAlQuraishi regarding DeepMind‚Äôs top ra‚Ä¶
1070994019604447200,Fri Dec 07 10:50:47 +0000 2018,@surafelml üëçüèªOn the whole, I think we're still ways away from a "general principle" for unsupervised learning in NL‚Ä¶ https://t.co/lKuxRfaYYe
1070973282755756000,Fri Dec 07 09:28:22 +0000 2018,@surafelml I think objectives like next sentence prediction already go beyond very local predictions. Finding ways‚Ä¶ https://t.co/kaoas49KQv
1070953112561700900,Fri Dec 07 08:08:14 +0000 2018,@MamadyNabeke @iamtrask @math_rachel Here's a short list: https://t.co/D6HC6TOzTR Good luck! :)
1070943800250503200,Fri Dec 07 07:31:13 +0000 2018,RT @NalKalchbrenner: More compute, better architectures and novel robust orderings have fueled tremendous progress for AR image models. SPN‚Ä¶
1070792078018445300,Thu Dec 06 21:28:20 +0000 2018,RT @SanhEstPasMoi: If you're at #NeurIPS2018 or in Montreal, stop by!   I'll be presenting our latest Multi-Task Learning architecture: HMT‚Ä¶
1070791329901350900,Thu Dec 06 21:25:22 +0000 2018,@StateoftheartAI @NirantK Sounds good! Looking forward to your message!
1070782657913593900,Thu Dec 06 20:50:54 +0000 2018,@gspmoreira Unfortunately not. Next time. Hope you enjoy! üòÄ
1070604691258658800,Thu Dec 06 09:03:43 +0000 2018,@roeeaharoni Cheers! Didn't remember that they did this analysis already in the original paper. I guess it's not so‚Ä¶ https://t.co/iZB7kOc2Lq
1070470060987310100,Thu Dec 06 00:08:45 +0000 2018,Tutorial on Unsupervised Deep Learning at #NeurIPS2018. NLP part starts at 1:16:00. Still sizable gap between unsup‚Ä¶ https://t.co/gibZBrvCFB
1070359574857830400,Wed Dec 05 16:49:43 +0000 2018,@bkj____ Haven't really looked at it, so not sure. I doubt it's full fledged architecture search. It more likely us‚Ä¶ https://t.co/sLHm2sYf4e
1070356596692320300,Wed Dec 05 16:37:53 +0000 2018,@UKnownknown Thanks! I really appreciate the kind words! ‚ò∫Ô∏è
1070356487577485300,Wed Dec 05 16:37:27 +0000 2018,@NirantK Yep, really like it. Perhaps we can join forces / contribute things. @StateoftheartAI
1070279882960236500,Wed Dec 05 11:33:03 +0000 2018,@emnlp2019 @ACL2019_Italy @NAACLHLT #NeurIPS2018 is doing a great job of live-streaming sessions, which is awesome‚Ä¶ https://t.co/YjmQpnxEjI
1070276567471919100,Wed Dec 05 11:19:53 +0000 2018,Super interesting tutorial on visualization for ML at #NeurIPS2018 w/ case study on multilingual embedding visualiz‚Ä¶ https://t.co/eQILIhDZq0
1070034939972849700,Tue Dec 04 19:19:44 +0000 2018,@akhooli @PiotrCzapla @fastdotai With current language models, you don't want to remove any tokens.
1069933730884714500,Tue Dec 04 12:37:34 +0000 2018,@alienelf @NipsConference @LauraMartinus @_tabbz @BenjaminRosman @dannimassi @Dr_Tempest We'll catch up at the Indaba next year! üòÄ
1069932725950398500,Tue Dec 04 12:33:34 +0000 2018,@alienelf @NipsConference @LauraMartinus @_tabbz @BenjaminRosman @dannimassi @Dr_Tempest Enjoy!
1069685135258386400,Mon Dec 03 20:09:44 +0000 2018,@akhooli @fastdotai @PiotrCzapla Abed, as you can see in the ongoing issues, the repo is still work in progress.
1069597351998890000,Mon Dec 03 14:20:55 +0000 2018,This is a super intuitive (and well illustrated) guide to state-of-the-art Transfer Learning methods in NLP. From t‚Ä¶ https://t.co/burniMSiiW
1069508839668674600,Mon Dec 03 08:29:12 +0000 2018,@kdnuggets Sure. :)
1068497501987250200,Fri Nov 30 13:30:30 +0000 2018,RT @sigrep_acl: We're excited to report that #Repl4NLP has been accepted as a full-day workshop at #acl2019! Details will follow on the wor‚Ä¶
1068486277660586000,Fri Nov 30 12:45:54 +0000 2018,@pablo_gps Thank you so much!
1068470914235412500,Fri Nov 30 11:44:51 +0000 2018,RT @hardmaru: ‚ÄúSo you want to be a Research Scientist‚Äù by Vincent Vanhoucke @GoogleAI: ‚Ä¢Your will spend a career working on things that don‚Ä¶
1068469289324298200,Fri Nov 30 11:38:24 +0000 2018,@pablo_gps Thanks for spotting that! I've reached out to the translator. Will update the links if I hear back.
1068456451788730400,Fri Nov 30 10:47:23 +0000 2018,@anmarasovic @allen_ai Awesome! Congrats!!! üéâ
1068215047162347500,Thu Nov 29 18:48:08 +0000 2018,@alanmnichol Awesome! Thanks for sharing! :)
1068172624428245000,Thu Nov 29 15:59:34 +0000 2018,RT @Moustapha_6C: The application window for the Google Africa PhD fellowship is now open. https://t.co/ZyvMPDpnUw has the application info‚Ä¶
1068104051294904300,Thu Nov 29 11:27:04 +0000 2018,@JJ_Hepboin @_Sharraf Sure!
1068104028838608900,Thu Nov 29 11:26:59 +0000 2018,RT @_Sharraf: We should all thank @seb_ruder for creating https://t.co/VxdV5Qubcp ‚ù§. Saves loads of time searching for datasets and SotA pa‚Ä¶
1068056420757712900,Thu Nov 29 08:17:48 +0000 2018,@_Sharraf üòç Thanks! Looking forward to including Arabic datasets.
1067887507344433200,Wed Nov 28 21:06:36 +0000 2018,RT @DeepIndaba: üé• See what the African AI community is like in our video summarising the #DLIndaba2018. We find "the courage in each other‚Ä¶
1067807760174252000,Wed Nov 28 15:49:43 +0000 2018,RT @cbeutenmueller: My favorite paper on top 1. This list cannot be wrong. https://t.co/FRHacroawr congrats to @seb_ruder  #ml #ai #NLProc
1067803940186669000,Wed Nov 28 15:34:32 +0000 2018,RT @_aylien: This AMAZING demo for the custom clickbait detector we built is so much fun it will SHOCK you! https://t.co/aldQQ740HF
1067701124994609200,Wed Nov 28 08:45:59 +0000 2018,@karthiktsaliki @jeremyphoward You should be fine if you use the scripts here: https://t.co/f0uJPW2pHX Not that the‚Ä¶ https://t.co/iykBzj5Sxl
1067699472384344000,Wed Nov 28 08:39:25 +0000 2018,RT @jennwvaughan: Reposting for those headed to #NeurIPS2018  Nine things I wish I had known the first time I came to NIPS  (my opening rem‚Ä¶
1067699240095363100,Wed Nov 28 08:38:30 +0000 2018,@karthiktsaliki @jeremyphoward Sure.
1067440893148495900,Tue Nov 27 15:31:55 +0000 2018,RT @scholarcy: Abstract, highlights, summaries and references from the 41 #NLProc papers accepted at this years #NeurIPS2018 @NipsConferenc‚Ä¶
1067404203608498200,Tue Nov 27 13:06:08 +0000 2018,@scholarcy @thembani_p Nice! Please share a link once you're done. :)
1067383963163992000,Tue Nov 27 11:45:42 +0000 2018,@supernlpblog FYI, this paper should answer the question in your intermezzo https://t.co/uJnSZlnZWx
1067380619146072000,Tue Nov 27 11:32:25 +0000 2018,All 41 #NeurIPS2018 papers on #NLProc. You can find the full papers in the proceedings here: https://t.co/PocAoUFCNF https://t.co/LcUSLuAJ0z
1067380244271767600,Tue Nov 27 11:30:55 +0000 2018,@thembani_p @Swayson @psiphire16 Those do seem to be the ones on NLP, though, right? @Swayson has selected NLP as the subject.
1067365905762533400,Tue Nov 27 10:33:57 +0000 2018,RT @thembani_p: Hey @seb_ruder üòä Do you have a list of all the NLP papers at #NeurIPS2018 or know where I can get one? Just want to get som‚Ä¶
1067365894509273100,Tue Nov 27 10:33:54 +0000 2018,@thembani_p Great question! Haven't had the time to go through them myself yet. Maybe someone else has?
1067343636579401700,Tue Nov 27 09:05:27 +0000 2018,Great post about the most common supervised and unsupervised sentence representation methods. https://t.co/8OtSmIeLLY
1067209084351537200,Tue Nov 27 00:10:48 +0000 2018,RT @Tim_Dettmers: Just published a blog post about advice for PhD applications. This comes a bit late, but I hope it is still helpful for s‚Ä¶
1067174089406730200,Mon Nov 26 21:51:44 +0000 2018,Montezuma‚Äôs Revenge Solved - Key insights : 1. Remember the exploration frontier. 2. First return to a state, then‚Ä¶ https://t.co/lzmfYtfPpG
1067025330064556000,Mon Nov 26 12:00:37 +0000 2018,RT @parsaghaffari: Good morning! We're looking for a *Site Reliability Engineer (SRE)* at @_aylien to join our engineering team in Dublin,‚Ä¶
1067013923013050400,Mon Nov 26 11:15:18 +0000 2018,@ppweni Thanks for the shout-out, Phillip! :)
1067007355408318500,Mon Nov 26 10:49:12 +0000 2018,@cgildea14 You can send me an email. See e.g. my latest paper for the email address.
1066638128167956500,Sun Nov 25 10:22:01 +0000 2018,@apertio_tech Thanks for the pointer! How do you retrieve datasets here? When I type in `question answering` for in‚Ä¶ https://t.co/O14bYoXubT
1066026947019452400,Fri Nov 23 17:53:24 +0000 2018,RT @CambridgeLTL: Do you want to do a PhD with @annalkorhonen @nigelhcollier¬†@licwu et al at @CambridgeLTL? We welcome applications from ou‚Ä¶
1065579800054636500,Thu Nov 22 12:16:36 +0000 2018,RT @jhuclsp: Thinking of applying to CS PhD programs? Interested in #nlproc ? Checkout our new FAQ with advice for applying to grad school!‚Ä¶
1065335451920343000,Wed Nov 21 20:05:39 +0000 2018,@JesseHVig I really have to thank all the amazing people who've contributed to the repo over the last months. We're‚Ä¶ https://t.co/wKG7YvY4fW
1065236179392577500,Wed Nov 21 13:31:10 +0000 2018,RT @nigelhcollier: Call for PhD students: we have an EPSRC studentship to work on NLP and Information Extraction with a focus on integratin‚Ä¶
1064908689440731100,Tue Nov 20 15:49:51 +0000 2018,RT @SanhEstPasMoi: One NLP model to rule them all üòâ  We've open sourced code &amp; demo of our latest Hierarchical Multi-Task Learning model. S‚Ä¶
1064640013017776100,Mon Nov 19 22:02:13 +0000 2018,RT @asiddhant1: Work (with Amazon) titled "Unsupervised Transfer Learning for SLU" is set to appear at #aaai2019.  We apply ELMo, ULMFiT (u‚Ä¶
1064547362985730000,Mon Nov 19 15:54:04 +0000 2018,RT @maria_antoniak: I started a blog! ‚úçÔ∏è My first post is about studying for data science / machine learning internship interviews, with a‚Ä¶
1064447901605249000,Mon Nov 19 09:18:50 +0000 2018,RT @jeremyphoward: Great summary in @nytimes from @CadeMetz discussing recent advances in NLP from @GoogleAI @OpenAI @allen_ai @fastdotai &amp;‚Ä¶
1064447789512515600,Mon Nov 19 09:18:24 +0000 2018,RT @EricTopol: Somewhere between a "1st thrust" and "explosive progress," the advances in natural language processing #AI #NLP https://t.co‚Ä¶
1064444964002824200,Mon Nov 19 09:07:10 +0000 2018,The New York Times discusses the current advances in #NLProc. "Explosive progress" but still a long way from "human‚Ä¶ https://t.co/HlM3aV1tT5
1063851350944088000,Sat Nov 17 17:48:22 +0000 2018,@divergent_ai Thank you! I really enjoyed reading your article. :)
1063765300804501500,Sat Nov 17 12:06:26 +0000 2018,RT @omarsar0: Wrote a short overview of HMTL (Multi-task learning for state of the art NLP). Paper written by @SanhEstPasMoi, @Thom_Wolf, a‚Ä¶
1063746950040903700,Sat Nov 17 10:53:31 +0000 2018,RT @divergent_ai: Inspired by @seb_ruder‚Äòs recent writing on @gradientpub: i wrote my first-ever blog post, about ImageNet, bias, and a mul‚Ä¶
1063485066028990500,Fri Nov 16 17:32:52 +0000 2018,RT @julien_c: New drop from the Hugging Face team, w/ @seb_ruder:  üí• State of the art NLP with HMTL (Hierarchical Multi-Task Learning model‚Ä¶
1063430276112625700,Fri Nov 16 13:55:10 +0000 2018,RT @KreutzerJulia: Our group entered the blogosphere!üöÄ First post on #ReinforcementLearning in #NMT: The Good, the Bad and the Ugly https:/‚Ä¶
1063129704843145200,Thu Nov 15 18:00:48 +0000 2018,RT @honnibal: Have been experimenting with an unsupervised pre-training technique for @spacy_io, similar to ULMFit/Elmo/BERT etc. Still ver‚Ä¶
1062761763517685800,Wed Nov 14 17:38:44 +0000 2018,RT @g_bekou: @seb_ruder @seb_ruder you can find them here: https://t.co/xqPBtT9ti7
1062761733448708100,Wed Nov 14 17:38:37 +0000 2018,@swaroopgrs Thank you!!
1062761700565442600,Wed Nov 14 17:38:29 +0000 2018,@g_bekou Thank you!!
1062753552823664600,Wed Nov 14 17:06:06 +0000 2018,@TaycirYahmed Thanks! I was looking for the slides on pretraining / multi-task learning that @PSH_Lewis mentioned h‚Ä¶ https://t.co/9ptFYGokdU
1062751023096348700,Wed Nov 14 16:56:03 +0000 2018,Does anyone know if the slides for the Joint Models for NLP tutorial at #emnlp18 are available somewhere? I can't s‚Ä¶ https://t.co/MIbjVUJS6z
1062696047196209200,Wed Nov 14 13:17:36 +0000 2018,@nasrinmmm @Forbes Congrats! üëè
1062442410922385400,Tue Nov 13 20:29:44 +0000 2018,RT @allen_ai: An overview of the current push to teach #AI common sense, an important research initiative for #AI2. Including some thoughts‚Ä¶
1062408020292972500,Tue Nov 13 18:13:05 +0000 2018,RT @asutoshsahoo_97: https://t.co/a75jtbGkQM  Slides of my seminar this semester on Universal Language Model Fine Tuning for Text Classific‚Ä¶
1062100262729453600,Mon Nov 12 21:50:10 +0000 2018,RT @supernlpblog: Check out our first post - a recap of EMNLP 2018 in Brussels: https://t.co/z71FO7EwwK.
1061938074467422200,Mon Nov 12 11:05:41 +0000 2018,This looks like a great task to submit to if you're interested in QA. In contrast to Jeopardy!, clues for a questio‚Ä¶ https://t.co/FZFxgZrYrx
1061921611794866200,Mon Nov 12 10:00:16 +0000 2018,New NLP News: ML on code, Understanding RNNs, Deep Latent Variable Models, Writing Code for NLP Research, Quo vadis‚Ä¶ https://t.co/uJK46fZ7nh
1061916764789968900,Mon Nov 12 09:41:00 +0000 2018,@minhle_r7 Awesome, thanks! :)
1060634607320789000,Thu Nov 08 20:46:10 +0000 2018,@srchvrs Good point! Translation invariance doesn't mean they don't have flaws. Besides not capturing pose, they al‚Ä¶ https://t.co/8DWFri8zGu
1060557633751523300,Thu Nov 08 15:40:18 +0000 2018,RT @PiotrCzapla: 15% reduction in error rate for Malay üá≤üáæ by @cedric_chee  using #ULMFiT architecture by @jeremyphoward &amp; @seb_ruder.  Have‚Ä¶
1060497674695258100,Thu Nov 08 11:42:03 +0000 2018,RT @PiotrCzapla: How about #ULMFiT getting 97% accuracy on Portuguese üáµüáπ text classification by @Fmelobr and @monilouise? https://t.co/uyQ5‚Ä¶
1060172342712778800,Wed Nov 07 14:09:18 +0000 2018,@tavoaguilar91 I assume you mean prediction is at the word level as most of the LMs use character-level features. A‚Ä¶ https://t.co/N63rIB6B33
1060164442757980200,Wed Nov 07 13:37:54 +0000 2018,New post on my #EMNLP2018 Highlights: Inductive bias, cross-lingual learning, and more https://t.co/IrqcZaQ8rM
1060155044161564700,Wed Nov 07 13:00:33 +0000 2018,RT @parsaghaffari: "Off-the-Shelf Unsupervised NMT" by @Mystical_Wiz, @seb_ruder and @anotherjohng explores unsupervised MT as a multi-task‚Ä¶
1060155044161564700,Wed Nov 07 13:00:33 +0000 2018,RT @parsaghaffari: "Off-the-Shelf Unsupervised NMT" by @Mystical_Wiz, @seb_ruder and @anotherjohng explores unsupervised MT as a multi-task‚Ä¶
1060097259428331500,Wed Nov 07 09:10:56 +0000 2018,RT @nigelhcollier: PhD funding just announced by The Alan Turing Institute @turinginst at https://t.co/UtEeHuFqw6. Happy to hear from highl‚Ä¶
1059859719924277200,Tue Nov 06 17:27:03 +0000 2018,RT @SanhEstPasMoi: Excited to announce that our paper ‚ÄúA Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks‚Äù has‚Ä¶
1059859152447647700,Tue Nov 06 17:24:47 +0000 2018,We have a new hierarchical multi-task learning model coming out at #AAAI2019. Great work and super interesting anal‚Ä¶ https://t.co/rTgvJQKJZ4
1059850610240118800,Tue Nov 06 16:50:51 +0000 2018,@minhle_r7 Thanks again for the pointer! Could you create an issue for this on https://t.co/j9HhK14fV5? That way, w‚Ä¶ https://t.co/FnpeujX1m2
1059848195063324700,Tue Nov 06 16:41:15 +0000 2018,@harshit048 @rasbt @TeachTheMachine Have a look here for pointers to state-of-the-art models and datasets: https://t.co/ZmAJrIlUU3
1059834441101688800,Tue Nov 06 15:46:36 +0000 2018,@AaronWu_0202 @gradientpub That's a good question. For some methods, adding the context vectors can be helpful. Lev‚Ä¶ https://t.co/Rd2IZQ4QAS
1059808196418773000,Tue Nov 06 14:02:18 +0000 2018,RT @PSH_Lewis: My #emnlp18 blog post is now live.  This post is a bit of a monster. My High level thoughts and trends at the start, then pa‚Ä¶
1059768139523731500,Tue Nov 06 11:23:08 +0000 2018,RT @PiotrCzapla: Application of #ULMFiT for üá´üá∑ by @tomsthom achieved 92.8% accuracy on sentiment analysis! https://t.co/LVmatTLfDC , anothe‚Ä¶
1059745252645441500,Tue Nov 06 09:52:12 +0000 2018,@PiotrCzapla @DangTLam Yes, I totally agree. We'd often like smaller and more efficient models. At the same time, f‚Ä¶ https://t.co/pS1uTKAVE9
1059489831133286400,Mon Nov 05 16:57:14 +0000 2018,RT @Thom_Wolf: Here is an op-for-op @PyTorch re-implementation of @GoogleAI's BERT model by @sanhestpasmoi, @timrault and I. We made a scri‚Ä¶
1059442009835008000,Mon Nov 05 13:47:13 +0000 2018,RT @jacobandreas: ATTENTION TWITTER I AM LOOKING FOR STUDENTS: Are you interested in doing things with language &amp; machine learning? Doing t‚Ä¶
1059439373396123600,Mon Nov 05 13:36:44 +0000 2018,The multilingual BERT model is out now (earlier than anticipated). It covers 102 languages and features an extensiv‚Ä¶ https://t.co/7j216nrsaq
1059424597211258900,Mon Nov 05 12:38:01 +0000 2018,@NirantK Sure. I don't think the talks were recorded (cc @yvespeirsman). What kind of notes would you be interested‚Ä¶ https://t.co/1i7k5Yz7Qm
1059417122651168800,Mon Nov 05 12:08:19 +0000 2018,Here are the slides of my talk on Transfer learning with language models at the Belgium NLP meetup last week. I tri‚Ä¶ https://t.co/vZuBd4CK0g
1059059571522703400,Sun Nov 04 12:27:32 +0000 2018,@minhle_r7 This is great! Thanks for pointing me to it. I'll take a look tomorrow.
1058759602995630100,Sat Nov 03 16:35:34 +0000 2018,RT @alexandraxron: Many thanks to @seb_ruder, we were inspired by ULMFiT for our submission model! Also great chatting with him at @emnlp20‚Ä¶
1058754688005357600,Sat Nov 03 16:16:02 +0000 2018,RT @katemargatina: Attending #emnlp2018 (my first conference ever) was an incredible experience as a new NLP researcher. I was beyond excit‚Ä¶
1058349458348539900,Fri Nov 02 13:25:48 +0000 2018,@MarekRei @dillonniederhut Will upload them soon and then share here.
1058347683193217000,Fri Nov 02 13:18:45 +0000 2018,@MarekRei @dillonniederhut That was at the Belgium NLP Meetup at the first day.
1058343638537777200,Fri Nov 02 13:02:41 +0000 2018,RT @anotherjohng: A reminder that today is the deadline for getting in touch with us if you are interested in collaborating under the SFI I‚Ä¶
1058343285914300400,Fri Nov 02 13:01:17 +0000 2018,RT @Oscar74176026: First half of the #emnlp2018 main conference is now over. Many of the big players are here (#google, #apple, #amazon, #s‚Ä¶
1058343255199359000,Fri Nov 02 13:01:09 +0000 2018,@Oscar74176026 @RichardSocher Being mentioned in the same sentence as @RichardSocher...ü§©
1058255847917150200,Fri Nov 02 07:13:50 +0000 2018,RT @nlpmattg: It's conference time, so here's some advice for new folks on approaching new people at conferences: don't introduce yourself‚Ä¶
1057751642349727700,Wed Oct 31 21:50:18 +0000 2018,RT @arnovicher: First day at #EMNLP2018 for the @streetbees data science team draws to a close with an absolutely awesome late night talk b‚Ä¶
1057751338539466800,Wed Oct 31 21:49:05 +0000 2018,RT @dillonniederhut: How to train your dÃ∂rÃ∂aÃ∂gÃ∂oÃ∂nÃ∂ language model 1. Use an encoder 2. Go bidirectional always ... 5. Buy 64 TPUs üòÖ  - @se‚Ä¶
1057703945714483200,Wed Oct 31 18:40:46 +0000 2018,@InglouriousBkof Sure. I'm also happy to meet at EMNLP so we can talk in person. Just send me a DM.
1057680973905780700,Wed Oct 31 17:09:29 +0000 2018,RT @gneubig: If you're at #EMNLP2018 and interested in faculty jobs in #nlproc or other areas of language technology, come talk to me about‚Ä¶
1057658604864585700,Wed Oct 31 15:40:36 +0000 2018,Code and pretrained weights for BERT are out now. Includes scripts to reproduce results. BERT-Base can be fine-tune‚Ä¶ https://t.co/9iuIjMBCqg
1057632022913404900,Wed Oct 31 13:54:58 +0000 2018,@hadyelsahar I think they do, but maybe not always.
1057630416763412500,Wed Oct 31 13:48:35 +0000 2018,@hadyelsahar Thanks for live-tweeting! One small suggestion: Maybe post as a thread next time (replies to the first‚Ä¶ https://t.co/Dklnf4bjvu
1057594880845844500,Wed Oct 31 11:27:23 +0000 2018,@LeonDerczynski @sleepinyourhat üëâ@poolio
1057578269049081900,Wed Oct 31 10:21:22 +0000 2018,RT @nishantiam: would be presenting our paper: Neural Drugnet which came 2nd in the shared task on Drug intake detection at @EMNLP2018  Sli‚Ä¶
1057525665052934100,Wed Oct 31 06:52:21 +0000 2018,RT @RomanFeiman: I'm looking for a PhD student and a postdoc for fall 2019. Come work at Brown in a truly multidisciplinary department, and‚Ä¶
1057359391727513600,Tue Oct 30 19:51:38 +0000 2018,@WilliamWangNLP @Airport_FRA Wow... I'm so so sorry for you and your student, William.
1057346105212571600,Tue Oct 30 18:58:50 +0000 2018,RT @samcharrington: I'm excited to share this super-interesting @twimlai conversation with #NLP researcher @seb_ruder. We cover a ton of gr‚Ä¶
1057223898402619400,Tue Oct 30 10:53:14 +0000 2018,@_dhruveshp Thanks for the kind words, Dhruvesh! :)
1057223773936599000,Tue Oct 30 10:52:44 +0000 2018,RT @_inesmontani: If you're in Brussels (e.g. for #emnlp2018), come hang out at the Belgium NLP Meetup tomorrow! We'll be speaking alongsid‚Ä¶
1057218513000108000,Tue Oct 30 10:31:50 +0000 2018,RT @eltimster: PSR: the University of Melbourne NLP Group has openings at all levels -- tenure-track faculty positions, postdocs, PhDs. Gra‚Ä¶
1057041754736812000,Mon Oct 29 22:49:27 +0000 2018,RT @mohitban47: Hi folks, we have renewed the postdoc link at https://t.co/BXMAGcUXWy to "Open Until Filled". Please apply and please keep‚Ä¶
1057008504198836200,Mon Oct 29 20:37:20 +0000 2018,RT @twimlai: Today we‚Äôre joined by @seb_ruder, PhD student studying NLP at @nuigalway and Research Scientist at text analysis startup @_ayl‚Ä¶
1057004787542179800,Mon Oct 29 20:22:34 +0000 2018,RT @chipro: We're building a new team at @NVIDIA and looking for NLP people with amazing engineering skills to bring NLP research into prod‚Ä¶
1056891333003096000,Mon Oct 29 12:51:44 +0000 2018,Check out the #conll2018 best papers: - Artetxe et al. reveal that word embeddings capture more information than ty‚Ä¶ https://t.co/7Fj5ScdJUM
1056882935801016300,Mon Oct 29 12:18:22 +0000 2018,@sarthak_1011 @ucddublin Hey, awesome that you're doing a Master's in ML! I'm not sure where you've seen that, but‚Ä¶ https://t.co/6eKMPJM8uV
1056880150309167100,Mon Oct 29 12:07:18 +0000 2018,@piesauce Looking forward to the post!
1055793444076093400,Fri Oct 26 12:09:07 +0000 2018,RT @microth: Job ads for the first two positions in my recently funded research group  are now online! Please feel to distribute to talente‚Ä¶
1055782460510875600,Fri Oct 26 11:25:28 +0000 2018,@icoxfog417 @tbsflk PRs are welcome! üòâ
1055776159512834000,Fri Oct 26 11:00:26 +0000 2018,@NirantK Thank you for supporting it, Nirant!
1055753484392370200,Fri Oct 26 09:30:20 +0000 2018,@sannikpatel The confusion might come from GloVe being called Global Vectors for Word Representation because they f‚Ä¶ https://t.co/fZ2Q2eJFTz
1055577708003053600,Thu Oct 25 21:51:51 +0000 2018,Are you interested in summarization? @tbsflk compiled the results on the most common datasets (CNN/DailyMail, Gigaw‚Ä¶ https://t.co/KtKEvR6MNm
1055544323754860500,Thu Oct 25 19:39:12 +0000 2018,The compute and data moats are dead This is a thoughtful article by @Smerity that makes a super important point peo‚Ä¶ https://t.co/a2iMrfHWSx
1055532778891034600,Thu Oct 25 18:53:19 +0000 2018,@burrsettles @duolingo This is great! Can we expect to see more open-access research from Duolingo?
1055531759796916200,Thu Oct 25 18:49:17 +0000 2018,@PSH_Lewis Nice! Would you mind adding the dataset here? https://t.co/3zP8JPZGHN
1055531418439352300,Thu Oct 25 18:47:55 +0000 2018,@mandarjoshi90 @eunsolc @omerlevy_ @dsweld @LukeZettlemoyer Really nice work! Thanks for sharing! :)
1055529776381857800,Thu Oct 25 18:41:24 +0000 2018,The GAN artwork sells for  a staggering $432,500, nearly 45x its high estimate! With such large sums, the question‚Ä¶ https://t.co/RyRDqvL5Gl
1055201577550651400,Wed Oct 24 20:57:15 +0000 2018,RT @PiotrCzapla: #ULMFIT rocks - 37% reduction in error rate for Indonesian document classification, model trained by Cahya Wirawan https:/‚Ä¶
1055191545543123000,Wed Oct 24 20:17:23 +0000 2018,RT @artetxem: We are releasing Monoses, an open source implementation of our Unsupervised Statistical Machine Translation system. 26.2 BLEU‚Ä¶
1055174236216680400,Wed Oct 24 19:08:36 +0000 2018,@ahammami0 @TryGhost Good idea! I have a look if I can add something like that.
1055174126959247400,Wed Oct 24 19:08:10 +0000 2018,@vykthur @TryGhost Thanks a lot for the feedback! Those are great points that I'll keep in mind!
1055173895223984100,Wed Oct 24 19:07:15 +0000 2018,@NirantK Thanks for the shout-out! ‚ò∫Ô∏è
1055161187011362800,Wed Oct 24 18:16:45 +0000 2018,Updated the look of my blog for the first time since I started 2 1/2 years ago (mainly finally upgraded to‚Ä¶ https://t.co/lSzlYUvUu8
1054996677512384500,Wed Oct 24 07:23:03 +0000 2018,RT @TomKenter: I just published my very first blogpost ever, about research I did (and really liked) on byte-level models.  "Why you should‚Ä¶
1054798472048795600,Tue Oct 23 18:15:27 +0000 2018,RT @PSH_Lewis: Just posted a new (the first real!) post on my #nlproc blog. An broad and incomplete survey of methods that use paraphrasing‚Ä¶
1054284663089520600,Mon Oct 22 08:13:45 +0000 2018,@m__dehghani @HAzarbonyad @jkamps @mdr Congrats! üëèüèª
1054036780226355200,Sun Oct 21 15:48:46 +0000 2018,@avin_regmi Yes, a siamese network is the first approach, where you feed in both into the same encoder and calculat‚Ä¶ https://t.co/TWqCqiHBhx
1053940723102441500,Sun Oct 21 09:27:04 +0000 2018,@avin_regmi There is no decoder. Either you use the representation of the encoder (e.g. last hidden state) and comp‚Ä¶ https://t.co/Gq30YAgmon
1053924251504382000,Sun Oct 21 08:21:37 +0000 2018,RT @jeremyphoward: I was lucky enough to see this @TEDxSFbay talk by @math_rachel live, and it took my breath away. Now you can watch it to‚Ä¶
1053430360674091000,Fri Oct 19 23:39:04 +0000 2018,@Make3333 @brianavecchione @math_rachel Sorry if this came out weird. Just wanted to say that I'd like to see more‚Ä¶ https://t.co/tEc0Ph7OdX
1053413712592298000,Fri Oct 19 22:32:55 +0000 2018,@brianavecchione @math_rachel Great! Would love to read more clear articles without jargon. Are you planning to wri‚Ä¶ https://t.co/yVnChU6mBV
1052821648200659000,Thu Oct 18 07:20:16 +0000 2018,RT @mohitban47: Please RT: I am looking for a POSTDOC in NLP/ML for @UNCNLP!! Broad focus, freedom to create research agenda, advise studen‚Ä¶
1052678689413050400,Wed Oct 17 21:52:12 +0000 2018,@uptownnickbrown @mattthemathman @MarkNeumannnn Briefly, morphology is captured at the word embedding layers, local‚Ä¶ https://t.co/aYpR8F0R0y
1052678336382730200,Wed Oct 17 21:50:47 +0000 2018,@uptownnickbrown Yeah, unfortunately NLP doesn't have as pretty images as CV. But we're definitely getting to a bet‚Ä¶ https://t.co/Xl44wsAz8H
1052658008847601700,Wed Oct 17 20:30:01 +0000 2018,RT @jeremyphoward: Thanks to @GuggerSylvain you can now use data augmentation in fastai v1 even with keypoints, bounding boxes, and much mo‚Ä¶
1052657866245468200,Wed Oct 17 20:29:27 +0000 2018,RT @sannikpatel: Well, if you need some tonic of motivation then jump straight here. @seb_ruder  @AnalyticsVidhya  https://t.co/YigX04OOx5
1052560217362354200,Wed Oct 17 14:01:26 +0000 2018,@ozansener Oh, wow! Thanks so much! I also really enjoyed your NIPS 2016 paper on domain adaptation.
1052544344169693200,Wed Oct 17 12:58:21 +0000 2018,@TalKachman Thanks! That's mostly advice from Patrick's new post, though. ;)
1052536398878662700,Wed Oct 17 12:26:47 +0000 2018,If you're doing research, consider starting a blog like Patrick a) to help organize yourself ‚è∞; b) for positive fee‚Ä¶ https://t.co/aM1zMcPIT8
1052534998123696100,Wed Oct 17 12:21:13 +0000 2018,@PSH_Lewis This is great! Looking forward to the posts, Patrick! ‚ò∫Ô∏è
1052235525958987800,Tue Oct 16 16:31:13 +0000 2018,Great effort by https://t.co/Uax3FRUddl to make important datasets more easily accessible in a single place, in a s‚Ä¶ https://t.co/JY44eW6bty
1052124791325495300,Tue Oct 16 09:11:12 +0000 2018,@Eardil @revue Thanks for spotting! Should be fixed now. :)
1051890976481058800,Mon Oct 15 17:42:06 +0000 2018,RT @AnalyticsVidhya: NLP is an intriguing but rather complex field. In this ep. of our Datahack Radio #podcast, Sebastian Ruder joins us to‚Ä¶
1051861607612080100,Mon Oct 15 15:45:24 +0000 2018,@NishaDalal3 Thanks for the support! :)
1051861543904796700,Mon Oct 15 15:45:09 +0000 2018,@cedric_chee @revue Thanks for spotting. Yep, that's a typo. :)
1051806360810197000,Mon Oct 15 12:05:52 +0000 2018,@MSripadarao Thanks for the shout-out!
1051778097173938200,Mon Oct 15 10:13:34 +0000 2018,It was a pleasure being interviewed by @AnalyticsVidhya for an episode of their DataHack Radio. We talked about all‚Ä¶ https://t.co/Hc9PR2zhPa
1051775909970231300,Mon Oct 15 10:04:52 +0000 2018,@razanmasood @revue Thank you!! ‚ò∫Ô∏è
1051759648859217900,Mon Oct 15 09:00:15 +0000 2018,New NLP News: TensorFlow 2.0, PyToch Dev Conference, DecaNLP, BERT, Annotated Encoder-Decoder, ICLR 2019 reading,‚Ä¶ https://t.co/rT0WpuIOLG
1051747467174309900,Mon Oct 15 08:11:51 +0000 2018,@PSH_Lewis @ucl @uclmr @facebookai @riedelcastro Congrats! Cheers to the years ahead! üëèüèª
1051566213170716700,Sun Oct 14 20:11:37 +0000 2018,@NirantK @salil_23 @fastdotai @jeremyphoward Great tips by Nirant! Also look at some of the posters of the course t‚Ä¶ https://t.co/5p25duHJZh
1051548745647775700,Sun Oct 14 19:02:12 +0000 2018,@guillaume_che @NirantK @apte_dhruv Very nice tutorial! Thanks for the sharing, Guillaume!
1051536955991830500,Sun Oct 14 18:15:21 +0000 2018,@avin_regmi Yep. You can use it as an encoder and then calculate cosine similarity between the representations.
1050787754651598800,Fri Oct 12 16:38:18 +0000 2018,@zacharylipton @emnlp2018 Ideally we'd just use it for evaluation. However, in the near term, we're still quite far‚Ä¶ https://t.co/Am9Dw73B3T
1050727451138150400,Fri Oct 12 12:38:40 +0000 2018,It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonse‚Ä¶ https://t.co/2Z1sDAEkD7
1050726109313212400,Fri Oct 12 12:33:20 +0000 2018,@bjh_ip Got it. Lifelong learning where you still want to do well on past tasks is closer to that.
1050710082252656600,Fri Oct 12 11:29:39 +0000 2018,@bjh_ip Yes, basically. Could you elaborate what you mean by a third synthesis model?
1050707446921084900,Fri Oct 12 11:19:11 +0000 2018,@KantadoAaveChe Totally. But RL for learning language is just getting started, so we might see more of this in the future.
1050707025867563000,Fri Oct 12 11:17:30 +0000 2018,@mmbollmann I don't think so. Before we used off-the-shelf word embeddings. While there were some approaches trying‚Ä¶ https://t.co/VoSLqXIu8C
1050698161814196200,Fri Oct 12 10:42:17 +0000 2018,@KantadoAaveChe Yep, agreed. I guess as in OpenAI's case, this simplistic formulation still gets you a long way, so‚Ä¶ https://t.co/lqCLAgb7zJ
1050696121855733800,Fri Oct 12 10:34:11 +0000 2018,@KantadoAaveChe Multiple reward structures as in StarCraft or Dota 2 (eg rewards for killing enemies vs winning the‚Ä¶ https://t.co/HV35xkEQQG
1050694435200278500,Fri Oct 12 10:27:28 +0000 2018,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions:‚Ä¶ https://t.co/pIYV9gkqZZ
1050689142848401400,Fri Oct 12 10:06:27 +0000 2018,Multi-Task Learning as Multi-Objective Optimization (NIPS 2018): This is a pretty cool paper that casts MTL as find‚Ä¶ https://t.co/rzaxRca9a1
1050664527019180000,Fri Oct 12 08:28:38 +0000 2018,RT @_florianmai: This is becoming more evident than ever considering the BERT results: https://t.co/uuaj8U1Mbo . https://t.co/uwGucuAN4J
1049954310677782500,Wed Oct 10 09:26:29 +0000 2018,@nicolaspanel I don't have bandwidth to port it to TF at the moment unfortunately. :-/
1049951381874970600,Wed Oct 10 09:14:51 +0000 2018,@GretchenAMcC @WIRED Thank you, Gretchen! This is a really beautiful article! Looking forward to reading more about‚Ä¶ https://t.co/5V6hi7cLLJ
1049598906202554400,Tue Oct 09 09:54:14 +0000 2018,RT @skynet_today: Skynet This Week #10: our latest bi-weekly quick takes on a bunch of the most important recent media stories about AI!  T‚Ä¶
1049377908387668000,Mon Oct 08 19:16:04 +0000 2018,"There were two pieces of [helpful] advice : 1. [What] gives you a once-in-a-lifetime opportunity? 2. What are you‚Ä¶ https://t.co/YnphamXPgQ
1049321505535008800,Mon Oct 08 15:31:57 +0000 2018,RT @samcharrington: Great chat with @seb_ruder this morning about all things neural NLP. Word embeddings, seq2seq, attention, memory and of‚Ä¶
1048299537381699600,Fri Oct 05 19:51:00 +0000 2018,RT @spacy_io: We've teamed up with @Kaggle for their new spaCy-themed Kernel Awards!  üí´ use spaCy on any NLP problem and dataset üè∑ publish‚Ä¶
1048234797439569900,Fri Oct 05 15:33:45 +0000 2018,@nicogontier üëã
1048205243799011300,Fri Oct 05 13:36:19 +0000 2018,RT @microth: I'm excited to announce that the IMS @Uni_Stuttgart will host my recently funded research group on computational models of mis‚Ä¶
1048163029207212000,Fri Oct 05 10:48:34 +0000 2018,RT @parsaghaffari: Our 2019 Research Fellowship program is now open. Get in touch if you have a background in Deep Learning and/or NLP and‚Ä¶
1048162998144180200,Fri Oct 05 10:48:27 +0000 2018,RT @teagermylk: Only 2 weekends left to watch @fastdotai videos for the London Data Science Journal Club meetup discussion of #ulmfit trans‚Ä¶
1047950347447230500,Thu Oct 04 20:43:27 +0000 2018,RT @ulrichpaquet: My first ever tweet: our attendees at #dlindaba2018! @DeepIndaba Welcome to the new world, Ulrich :-) https://t.co/qySli2‚Ä¶
1047895310566117400,Thu Oct 04 17:04:45 +0000 2018,SOTAWHAT - A script to keep track of state-of-the-art AI research. Good complement to structured repos such as‚Ä¶ https://t.co/DQk5bw3ylW
1047828330861539300,Thu Oct 04 12:38:36 +0000 2018,@J_MaestreVidal Thank you, Jorge!
1047534750204928000,Wed Oct 03 17:12:01 +0000 2018,RT @barbara_plank: Yay! Exciting to announce that our website is online - the first Women in Data Science conference in Copenhagen to be he‚Ä¶
1047476530870440000,Wed Oct 03 13:20:40 +0000 2018,RT @cocoweixu: I am looking for PhD students in natural language processing and machine learning: https://t.co/LFE3BUXRrO  Please RT! #nlpr‚Ä¶
1047373769977860100,Wed Oct 03 06:32:20 +0000 2018,@AlfredoCanziani @jeremyphoward Unfortunately not üòû
1047228981110485000,Tue Oct 02 20:57:00 +0000 2018,An Insider‚Äôs Guide to Keeping Up with the AI Experts: An extensive list of recommended AI researchers and pioneers‚Ä¶ https://t.co/VPVEStvGiw
1047228183974690800,Tue Oct 02 20:53:50 +0000 2018,fastai v1, built on @PyTorch v1 is now live (https://t.co/vkcy1YFySh). Awesome work by @jeremyphoward and‚Ä¶ https://t.co/9ZKw7Ww46z
1047206989699473400,Tue Oct 02 19:29:37 +0000 2018,@GuggerSylvain This is awesome, Sylvain! Well done! :)
1047206927523041300,Tue Oct 02 19:29:22 +0000 2018,RT @GuggerSylvain: Fastai v1 is now officially live. First library to provide a unified and simple API for applying DL to vision, text, tab‚Ä¶
1047186675154264000,Tue Oct 02 18:08:53 +0000 2018,RT @jeremyphoward: After 2 years of development, we've just launched fastai v1, the first deep learning library with a simple consistent AP‚Ä¶
1047152596539068400,Tue Oct 02 15:53:28 +0000 2018,@daniilmagpie Yeah, I've included them more for their potential rather than very strong results so far. I think hav‚Ä¶ https://t.co/kprr2aP1kz
1047062976161833000,Tue Oct 02 09:57:21 +0000 2018,@_josh_meyer_ @emilymbender @marzieh_saeidi @alicecoucke @deliprao @AnaValeriaGlez @yoavgo @zehavoc @bplank Yeah, b‚Ä¶ https://t.co/nXsFKzjxzv
1047061024099242000,Tue Oct 02 09:49:36 +0000 2018,@DeepIndaba @_aylien Thanks for all the feedback! I've changed the title and added another section referencing some‚Ä¶ https://t.co/qkX4BFdv6O
1047055532148822000,Tue Oct 02 09:27:46 +0000 2018,@_josh_meyer_ @emilymbender @marzieh_saeidi @alicecoucke @deliprao @AnaValeriaGlez @yoavgo @zehavoc 1) a) Frame the‚Ä¶ https://t.co/wYRqmWXFmK
1047025247344648200,Tue Oct 02 07:27:26 +0000 2018,RT @bhutanisanyam1: "Don‚Äôt let anyone tell you that you can‚Äôt do this (DL). [..]  Don‚Äôt think you need massive compute to work on meaningfu‚Ä¶
1047025032239693800,Tue Oct 02 07:26:35 +0000 2018,@fastdotai fellow Sanyam asked me a few questions about my background, doing research, &amp; writing articles. I hope t‚Ä¶ https://t.co/b1h85GDseJ
1047022948945670100,Tue Oct 02 07:18:18 +0000 2018,@redpony @DeepIndaba @_aylien Yeah. I'll add a large disclaimer in the beginning that this is heavily skewed toward‚Ä¶ https://t.co/ADhPxaoAlU
1047018577931452400,Tue Oct 02 07:00:56 +0000 2018,@zehavoc @shyamupa @DanielKhashabi @DeepIndaba @_aylien Yep, totally. I'll add another paragraph mentioning the thi‚Ä¶ https://t.co/vYAkhpfZbl
1047015030024261600,Tue Oct 02 06:46:50 +0000 2018,@shyamupa @DanielKhashabi @zehavoc @DeepIndaba @_aylien Totally agree with you. There's a lot that could have been‚Ä¶ https://t.co/cczYAXqUuy
1046791863896887300,Mon Oct 01 16:00:03 +0000 2018,New blog post: A Review of the Recent History of Natural Language Processing. The 8 biggest milestones in the last‚Ä¶ https://t.co/hp9uRHwwLX
1046790473719959600,Mon Oct 01 15:54:32 +0000 2018,RT @_aylien: Following on from a session at the recent @DeepIndaba, our Research Scientist @seb_ruder put together this review of the recen‚Ä¶
1046734797874303000,Mon Oct 01 12:13:17 +0000 2018,RT @AndrewYNg: Deep Learning is getting really good on Big Data/millions of images. But Small Data is important too. Am seeing many excitin‚Ä¶
1046438386293243900,Sun Sep 30 16:35:27 +0000 2018,@eagirre @haldaume3 @deliprao @Smerity @yoavgo @alvations @conll2018 @Aitor57 Similar to existing replies, I'd ment‚Ä¶ https://t.co/RJdAk5H9oc
1045681534689308700,Fri Sep 28 14:28:00 +0000 2018,A great series of interviews on the Pedagogy of NLP by @david__jurgens, @lucy3_li, starting with NLP experts‚Ä¶ https://t.co/B165Z21Zqi
1045634313369395200,Fri Sep 28 11:20:21 +0000 2018,RT @PiotrCzapla: 30 min of training to exceed SOTA of GermEval17 sentiment task. Using @fastdotai library, #ULMFiT(@jeremyphoward, @seb_rud‚Ä¶
1045634229013549000,Fri Sep 28 11:20:01 +0000 2018,@PiotrCzapla @fastdotai @jeremyphoward @misterkardas üëèüèª
1045519970116022300,Fri Sep 28 03:46:00 +0000 2018,If you're still undecided, here are 5 more reasons to attend the Deep Learning Indaba 2019 üåç https://t.co/vjBlntW3yj
1045284556771921900,Thu Sep 27 12:10:33 +0000 2018,cc @sivareddyg @danqi_chen @eunsolc @hhexiy @MohitIyyer @qi2peng2
1045283752069533700,Thu Sep 27 12:07:21 +0000 2018,There've been so many cool QA datasets recently. Dataset providers, it'd be awesome if you could add metadata so th‚Ä¶ https://t.co/ZFEM27YwQv
1045283411382992900,Thu Sep 27 12:06:00 +0000 2018,Good practices in Modern Tensorflow for NLP: A notebook of best practice code snippets covering Eager execution,‚Ä¶ https://t.co/lCdilDkEog
1045110272950759400,Thu Sep 27 00:38:00 +0000 2018,A Little Review of Domain Adaptation in 2017: Nice review of some of last year's most popular methods for aligning‚Ä¶ https://t.co/xagT7N2pL4
1045060815244513300,Wed Sep 26 21:21:29 +0000 2018,This looks like an awesome Intro to ML course for everyone with a coding background. Covers decision trees (includi‚Ä¶ https://t.co/PM5LC4Qmlp
1045059260072296400,Wed Sep 26 21:15:18 +0000 2018,How to visualize decision trees: A master class in visualization and a library for creating beautiful visualization‚Ä¶ https://t.co/8CV43W25lv
1044571186980671500,Tue Sep 25 12:55:52 +0000 2018,RT @GuggerSylvain: Fastai live replaces the old international fellowship and is open to anyone with one year experience of coding! Easiest‚Ä¶
1044191959252906000,Mon Sep 24 11:48:57 +0000 2018,Proof of the Riemann Hypothesis by Sir Michael Atiyah: https://t.co/dRw0bxn2G0 Uses the Todd function, an infinite‚Ä¶ https://t.co/KZhSvzqRbk
1044185466256412700,Mon Sep 24 11:23:09 +0000 2018,RT @PiotrCzapla: The @fastdotai  library and ULMFiT  (by @jeremyphoward  and @seb_ruder) is super powerful our first model for German gave‚Ä¶
1043495569703030800,Sat Sep 22 13:41:45 +0000 2018,RT @KirkDBorne: A Comprehensive Guide to Transfer Learning ‚Äî #MachineLearning's Next Frontier: https://t.co/z490BkbgMu by @seb_ruder   #Big‚Ä¶
1043182583742382100,Fri Sep 21 16:58:04 +0000 2018,RT @jeremyjordan: How to Write a üíØ Research Paper (shared in @seb_ruder's newsletter)  1. Don‚Äôt wait: write. 2. Identify your key idea. 3.‚Ä¶
1043165820136943600,Fri Sep 21 15:51:27 +0000 2018,@yoavgo @gchrupala Yeah, I guess it depends on your definition of MTL. I've seen MTL mostly refer to as "all task d‚Ä¶ https://t.co/oiRQYt4OQW
1043165091955462100,Fri Sep 21 15:48:33 +0000 2018,@gchrupala @yoavgo For (very recent) MTL for language, I'd say https://t.co/Mpi9XfhUHa, https://t.co/6BJy3VHl7l,‚Ä¶ https://t.co/X5QUvRDpap
1043163875024273400,Fri Sep 21 15:43:43 +0000 2018,@yoavgo @gchrupala Would you say that word2vec and ELMo are doing multi-task learning (MTL)? To me, pretraining + f‚Ä¶ https://t.co/D387xJa77u
1043036488240222200,Fri Sep 21 07:17:32 +0000 2018,@marijanzg1 My email is available on my website.
1042739337555787800,Thu Sep 20 11:36:45 +0000 2018,@ArkaSadhu29 Thanks! That wouldn't have been possible without the great support and editing from The Gradient team‚Ä¶ https://t.co/2fsl1h8KjJ
1042679835154427900,Thu Sep 20 07:40:19 +0000 2018,@GuillaumeLample @alex_conneau @LudovicDenoyer That's awesome! Well deserved!
1042410126509072400,Wed Sep 19 13:48:35 +0000 2018,@andymauro @jbrenier Maybe have a look at this article I wrote two months ago for some additional context: https://t.co/P1bdYY2UBA
1042409548768862200,Wed Sep 19 13:46:18 +0000 2018,@mihail_eric @GitHubEng @srcgraph Good question! @sourcedtech is maybe also doing something similar.
1042377483679674400,Wed Sep 19 11:38:53 +0000 2018,@GitHubEng @fastdotai Totally missed posting the link: https://t.co/YjXZewMkrJ
1042341543737282600,Wed Sep 19 09:16:04 +0000 2018,Towards Natural Language Semantic Code Search by @GitHubEng - Interestingly, they first learn separate vector space‚Ä¶ https://t.co/2HZ4ZteodR
1042046531308793900,Tue Sep 18 13:43:48 +0000 2018,@MendeleySupport Hey Mendeley, for the last couple of days, Mendeley Desktop freezes roughly every30 minutes. Force‚Ä¶ https://t.co/BSVvWSU32Y
1042002903840567300,Tue Sep 18 10:50:26 +0000 2018,@sgouws @juliaroz_ml @omojumiller @alienelf @timberners_lee It was "first come, first serve" üòÜ
1041984853984649200,Tue Sep 18 09:38:42 +0000 2018,@Sasha511 Haha üòúI unfortunately didn't get to try another one since the first day. Also didn't manage to get a bunn‚Ä¶ https://t.co/q2VYF5mLK4
1041718790789242900,Mon Sep 17 16:01:28 +0000 2018,RT @parsaghaffari: We're looking for backend engineers to build products that aggregate and analyze millions of pieces of content every day‚Ä¶
1041711284335661000,Mon Sep 17 15:31:38 +0000 2018,@linbo_pythoner üëçüèª
1041711095004782600,Mon Sep 17 15:30:53 +0000 2018,@ekandekha_comma @linbo_pythoner Very nice! üëèüèªOne small thing: I think instead of citing e.g. [4] it's nicer for th‚Ä¶ https://t.co/NTm5vgISLo
1041707219832660000,Mon Sep 17 15:15:29 +0000 2018,@nlpnoah @ai2_allennlp This is a beautiful tutorial! Thanks for sharing!
1041706292669296600,Mon Sep 17 15:11:48 +0000 2018,@vykthur @DeepIndaba @vukosi @dannimassi @shakir_za @nunuska @PythonKhaleesi @katjahofmann @omojumiller @Dr_Tempest‚Ä¶ https://t.co/LX9A5si9OJ
1041706198179799000,Mon Sep 17 15:11:26 +0000 2018,RT @vykthur: My thoughts and notes on the 2018 DL Indaba üëâ  https://t.co/1gQWInTJG5 . Thanks to the team! @DeepIndaba @vukosi @dannimassi @‚Ä¶
1041704058095059000,Mon Sep 17 15:02:56 +0000 2018,@RyanEloff üòçIt was a pleasure meeting you, Ryan. I hope to see lots of awesome work from you in the future. Let me‚Ä¶ https://t.co/d0Asauvudj
1041674709182697500,Mon Sep 17 13:06:18 +0000 2018,@NirantK @revue Good point! :)
1041628448735809500,Mon Sep 17 10:02:29 +0000 2018,New NLP Newsletter: Deep Learning Indaba 2018 edition üåçImpressions and highlights from the #DLIndaba2018‚Ä¶ https://t.co/dHBHCRzoov
1041208990280106000,Sun Sep 16 06:15:42 +0000 2018,RT @jeremyphoward: Congrats to @nirantk for his state of the art in Hindi language modeling and text classification using our ULMFit approa‚Ä¶
1040626655918796800,Fri Sep 14 15:41:43 +0000 2018,RT @juliaroz_ml: Big highlight for #NLProc enthusiasts yesterday at the #DLIndaba2018: succinct field overview by @seb_ruder and many insig‚Ä¶
1040589251967103000,Fri Sep 14 13:13:05 +0000 2018,RT @omojumiller: @math_rachel and @jeremyphoward look who I am wine tasting with in Stellenbosch? @seb_ruder!! Thanks to #DLIndaba2018 for‚Ä¶
1040550715310129200,Fri Sep 14 10:39:57 +0000 2018,@JeffDean gives the last talk at the #DLIndaba2018 on using Deep Learning to solve important problems. Lots of inte‚Ä¶ https://t.co/QrOuUCewtC
1040330294832889900,Thu Sep 13 20:04:05 +0000 2018,RT @thembani_p: From @kchonyc to @seb_ruder carrying the flag of NLP at @DeepIndaba #DLIndaba2018 https://t.co/0MiVYK2cg1
1040330226985840600,Thu Sep 13 20:03:49 +0000 2018,RT @Sasha511: Panel at the @DeepIndaba with @sgouws, @omojumiller, @seb_ruder and @HermanKamper on the Open Problems in NLP üó£ #DLIndaba2018‚Ä¶
1040241906066829300,Thu Sep 13 14:12:51 +0000 2018,Principle 10. Meta learning is the way to go. Not even the architecture is handcrafted anymore. Everything is learn‚Ä¶ https://t.co/JloH8eZBc6
1040241141382369300,Thu Sep 13 14:09:49 +0000 2018,Principle 9. Leverage strong function approximators. Algorithmic complexity can be pushed into neural network archi‚Ä¶ https://t.co/g9I5jkiHNo
1040240290215407600,Thu Sep 13 14:06:26 +0000 2018,Principle 8. Imagining the future (what will happen next) can be used for planning. Same RL algorithms can be appli‚Ä¶ https://t.co/ywgZvUVhmy
1040239566077263900,Thu Sep 13 14:03:33 +0000 2018,Principle 7. Value functions efficiently summarize the state of the world and the future. Multiple value functions‚Ä¶ https://t.co/zj1TJB7rtQ
1040238871798276100,Thu Sep 13 14:00:48 +0000 2018,Principle 6. Agents influence the stream of data and experiences. Agents should have access to features to control‚Ä¶ https://t.co/LvBAO0pAmt
1040238081633411100,Thu Sep 13 13:57:40 +0000 2018,Principle 5. State should be built as the state of the model, i.e. an RNN's hidden state and not defined in terms o‚Ä¶ https://t.co/KnXoiv8PKy
1040237357495144400,Thu Sep 13 13:54:47 +0000 2018,Principle 4. Use agent's experience rather than human expertise. Don't rely on engineered features or heuristics. H‚Ä¶ https://t.co/vZOYvNpT6V
1040236672913469400,Thu Sep 13 13:52:04 +0000 2018,Principle 3. Generality (how your algorithm performs on other tasks) is super important. Key is to design a diverse‚Ä¶ https://t.co/qWKDJF2byl
1040235985790017500,Thu Sep 13 13:49:20 +0000 2018,Principle 2. How an algorithm scales is more important than its starting point. Avoid performance ceilings. Deep Le‚Ä¶ https://t.co/dhvLcs6jlE
1040235236284670000,Thu Sep 13 13:46:21 +0000 2018,David Silver on Principles for Reinforcement Learning at the #DLIndaba2018. Important principles that are not only‚Ä¶ https://t.co/lWgh6Wa251
1040233749844246500,Thu Sep 13 13:40:27 +0000 2018,@linbo_pythoner Ah. I probably won't. I don't speak Chinese unfortunately.
1040232191782019100,Thu Sep 13 13:34:15 +0000 2018,@linbo_pythoner That's awesome! Congrats! üëè Would love if you could share the presentation afterwards!
1040230631249199100,Thu Sep 13 13:28:03 +0000 2018,RT @AlexLaterre: So proud to have the work I have led at @instadeepai be Cited by David Silver as Success Story in #RL in his lecture at  #‚Ä¶
1040229264472387600,Thu Sep 13 13:22:37 +0000 2018,@linbo_pythoner The left one is the softmax used for the language modeling loss that predicts a word from the vocab‚Ä¶ https://t.co/MGPkPu4NVf
1040228880689373200,Thu Sep 13 13:21:06 +0000 2018,@surafelml No live stream unfortunately, but maybe someone will live tweet. üòä
1040226610425868300,Thu Sep 13 13:12:05 +0000 2018,Slides and a video of the session will be made available afterwards for anyone who can't attend.
1040226406335217700,Thu Sep 13 13:11:16 +0000 2018,Join us at the Frontiers of Natural Language Processing session at 4.30 pm in the Van der Sterr building at the‚Ä¶ https://t.co/qIO2BN5A38
1040226007561760800,Thu Sep 13 13:09:41 +0000 2018,@linbo_pythoner Thanks for the question! Those are just to represent a binary classification task, e.g. predicting‚Ä¶ https://t.co/ZlxctCi4VG
1040216640229654500,Thu Sep 13 12:32:27 +0000 2018,David Silver on success stories in Reinforcement Learning at the #DLIndaba2018. Fascinating to hear about RL intuit‚Ä¶ https://t.co/SQm5TmE51p
1040212682035654700,Thu Sep 13 12:16:44 +0000 2018,Just presented ULMFiT at the #DLIndaba2018. Great to see lots of people excited about progress in #nlproc! https://t.co/7c3HQCbhFR
1040212682035654700,Thu Sep 13 12:16:44 +0000 2018,Just presented ULMFiT at the #DLIndaba2018. Great to see lots of people excited about progress in #nlproc! https://t.co/7c3HQCbhFR
1040212241252008000,Thu Sep 13 12:14:59 +0000 2018,RT @tymwol: I just found this exhaustive series of posts on word embeddings in #NLProc #DeepLearning by @seb_ruder (1) https://t.co/fT0FpwW‚Ä¶
1040212205713666000,Thu Sep 13 12:14:50 +0000 2018,RT @Sasha511: @DeepIndaba Posters: Universal Language Model Fine-tuning for Text Classification by @seb_ruder and @jeremyphoward ü§ì #DLIndab‚Ä¶
1039972389050835000,Wed Sep 12 20:21:53 +0000 2018,RT @thembani_p: At @CapitecBankSA event with @seb_ruder and @Swayson chatting about NLP for underserved languages. This is the beauty of #D‚Ä¶
1039947737574395900,Wed Sep 12 18:43:56 +0000 2018,RT @Sasha511: Great practicals at the @DeepIndaba this year üòé thanks to @sgouws, @avishkar58 and all the tutors for their hard work üíÉüèª #DLI‚Ä¶
1039856880485580800,Wed Sep 12 12:42:54 +0000 2018,Had lots of fun at yesterday's Google AI event at the #DLIndaba2018. Great people and authentic African food and mu‚Ä¶ https://t.co/eo9P1W3bQP
1039562512012853200,Tue Sep 11 17:13:11 +0000 2018,RT @julien_c: ‚ö†Ô∏è NLP community ‚ö†Ô∏è:  We're in the process of building an online demo for our new NLP multi-task model that reaches or beats‚Ä¶
1039562442685206500,Tue Sep 11 17:12:55 +0000 2018,@ksoonson @robert_thas @aniediudo @GoogleDevExpert @kbeguir @fatbutcher_sa üëã
1039552839880716300,Tue Sep 11 16:34:45 +0000 2018,RT @aniediudo: ML @GoogleDevExpert dinner with @ksoonson, @seb_ruder &amp; @kbeguir at the @fatbutcher_sa #DLIndaba2018 https://t.co/pnRmvesPQo
1039544451507658800,Tue Sep 11 16:01:25 +0000 2018,RT @Sasha511: Tutor @seb_ruder sharing his vast RNN knowledge during the @DeepIndaba prac ü§ì #DLIndaba2018 https://t.co/PuPjE6bCH0
1039059961597321200,Mon Sep 10 07:56:14 +0000 2018,"There are many other frontiers. This is an exciting time to do research in AI." https://t.co/IzNOSxGuhY
1039059601159794700,Mon Sep 10 07:54:48 +0000 2018,.@NandoDF: We need models that can perform clunterfactual reasoning. https://t.co/0wTgdYbi3c
1039057870346694700,Mon Sep 10 07:47:55 +0000 2018,Frontiers that excite @NandoDF at the #DLIndaba2018: 1. RL 2. Metalearning 3. Imitation learning 4. Robotics 5. Con‚Ä¶ https://t.co/XEYgbvu85Z
1038466534539255800,Sat Sep 08 16:38:10 +0000 2018,@hllo_wrld @uwnlp @chrmanning @RichardSocher @CaimingXiong @stanfordnlp @MetaMindIO @salesforce This is great! Cong‚Ä¶ https://t.co/KD44TQdB3T
1038174077884751900,Fri Sep 07 21:16:03 +0000 2018,Succinct and sober @WIRED article about recent work on pre-trained language models from @OpenAI, @allen_ai,‚Ä¶ https://t.co/ed2HN2IJtH
1038170828872015900,Fri Sep 07 21:03:08 +0000 2018,RT @allen_ai: #AI2's model #ELMo "swiftly toppled previous bests on a variety of challenging tasks ... In a field where progress tends to b‚Ä¶
1038170244706168800,Fri Sep 07 21:00:49 +0000 2018,@jeffrschneider Yep. Word sense disambiguation is hard. üòî cc @CamachoCollados
1038076965247434800,Fri Sep 07 14:50:09 +0000 2018,RT @_aylien: Busy day onboarding some new recruits here at AYLIEN HQ üëΩ https://t.co/FAs8jAjiQs
1037627636913197000,Thu Sep 06 09:04:41 +0000 2018,RT @YovaKem: Happy to announce that my first paper as a PhD fellow is now live! Generalizing Procrustes Analysis for Better Bilingual Dicti‚Ä¶
1037627116718837800,Thu Sep 06 09:02:37 +0000 2018,RT @nishantiam: System description paper for @emnlp2018 drug intake classification. We came 2nd w/ @MundraShivansh   This uses ULMFiT (@jer‚Ä¶
1037616721140633600,Thu Sep 06 08:21:18 +0000 2018,@YovaKem @coastalcph @conll2018 @sigmorphon üëèüèªCongrats!
1036554478554628100,Mon Sep 03 10:00:20 +0000 2018,New NLP News: Fake video, State of ML Code, Notebooks, and Software, Advice for MScing and PhDing, Ada Lovelace, 15‚Ä¶ https://t.co/YYzBab5ZyJ
1036165113856880600,Sun Sep 02 08:13:08 +0000 2018,@iamaidang @yaringal @yeewhye Congrats! üéâ
1035583723876565000,Fri Aug 31 17:42:54 +0000 2018,RT @HoseinFooladi7: Some very useful NLP resources that I have used for learning: https://t.co/yoY7QkOVzY https://t.co/Ja3A97HGMw https://t‚Ä¶
1035562472269393900,Fri Aug 31 16:18:27 +0000 2018,@radekosmulski Congrats! This is great! Thanks a lot also for talking more about the competition. That's super useful!
1034894765698035700,Wed Aug 29 20:05:14 +0000 2018,Our @emnlp2018 paper  A Discriminative Latent-Variable Model for Bilingual Lexicon Induction with @_shrdlu_,‚Ä¶ https://t.co/Nte1BeUeEd
1034515229424672800,Tue Aug 28 18:57:05 +0000 2018,@dino91068472 Hey, can you be a bit more specific?
1034477500129321000,Tue Aug 28 16:27:10 +0000 2018,Awesome Humble Bundle including up to 15 top ML books üìö https://t.co/2JiZc2bup2
1034405231294120000,Tue Aug 28 11:40:00 +0000 2018,@gwern @ludwigschubert @Miles_Brundage I think it's quite misleading not to mention in the abstract that the tests‚Ä¶ https://t.co/tIjhAawShp
1034132154412933100,Mon Aug 27 17:34:53 +0000 2018,RT @_aylien: Check out our intelligent @Slack bot named Mr. Jingles! Our office mascot, once a humble cat, can now fetch stories, create vi‚Ä¶
1033350008496504800,Sat Aug 25 13:46:55 +0000 2018,RT @surafelml: Its now official !!!  @ethionlp   is established, up &amp; running üèÉ   If you are interested to start working/proceed what you h‚Ä¶
1033349764232839200,Sat Aug 25 13:45:57 +0000 2018,RT @_aylien: We have an opening for a talented Business Development Representative to join our Sales team here in Dublin ‚òòÔ∏è. If you know an‚Ä¶
1032635854609469400,Thu Aug 23 14:29:07 +0000 2018,@shilpa15397 There hasn't been much work on meta-learning in NLP as far as I'm aware. These are the only two that I‚Ä¶ https://t.co/GAbMY8K7G6
1032485827887030300,Thu Aug 23 04:32:58 +0000 2018,RT @gradientpub: Deep learning has made enormous strides in NLP, but state-of-the-art models are still spurious and brittle. @anmarasovic b‚Ä¶
1032435317612011500,Thu Aug 23 01:12:16 +0000 2018,RT @misterkardas: ULMFiT + sentencepiece = state-of-the-art perplexity for polish language. Our (me and @PiotrCzapla) model (ppl. 118) easi‚Ä¶
1032204996912275500,Wed Aug 22 09:57:03 +0000 2018,@mdekuijper @revue @F_Vaggi It seems it's Amazon Secure Web Gateway Service. If you follow me, I can DM you the err‚Ä¶ https://t.co/3tZYq7PtlA
1032204102527316000,Wed Aug 22 09:53:30 +0000 2018,@marijanzg1 Hi Marijan, sure. Feel free to add me on LinkedIn.
1032203078701920300,Wed Aug 22 09:49:26 +0000 2018,RT @hengluchang: I just published a short post ‚ÄúVisualizing Gradient Descent with Momentum in Python‚Äù https://t.co/4CZ8mqXCX7  This post wa‚Ä¶
1032200096107122700,Wed Aug 22 09:37:34 +0000 2018,@jasonbaldridge @nkjemisin Thanks for the tip, Jason! Added to the reading list. :)
1031937733588074500,Tue Aug 21 16:15:02 +0000 2018,RT @Tim_Dettmers: I updated my guide with new GPU recommendations: RTX 2080 most cost-efficient choice. GTX 1080/1070 (+Ti) cards remain ve‚Ä¶
1031890535756296200,Tue Aug 21 13:07:29 +0000 2018,@microth Well deserved, Michael! Congrats! üéâ
1031800300733124600,Tue Aug 21 07:08:56 +0000 2018,@AlexSherstinsky Sure! You can find my email on https://t.co/U5moxQLuSK.
1031570974926401500,Mon Aug 20 15:57:40 +0000 2018,@revue Hey guys! getrevue seems to get flagged as a malicious link by the network at Amazon. Is there anything you‚Ä¶ https://t.co/LOppUojgL3
1031481047173750800,Mon Aug 20 10:00:20 +0000 2018,New NLP News: Learning Meaning in NLP, Recurrence in NNs, Q&amp;A with Yoshua Bengio, ML == pseudo science?, ImageNet &lt;‚Ä¶ https://t.co/Ozr1mDZ0qV
1030805661335085000,Sat Aug 18 13:16:35 +0000 2018,@alxcnwy @DeepIndaba Awesome! Me too! üòä
1030796417152036900,Sat Aug 18 12:39:51 +0000 2018,RT @bernardt_d: Do you want to know where #NLProc research is heading? Then don't miss this panel discussion on NLP @DeepIndaba https://t.c‚Ä¶
1029800415150391300,Wed Aug 15 18:42:06 +0000 2018,RT @skynet_today: Google Translate's 'Sinister Religious Prophecies', Demystified! A brand new article by @julia_gong  -  check it out!  ht‚Ä¶
1029478350732107800,Tue Aug 14 21:22:20 +0000 2018,RT @jeremyphoward: This is a terrific introduction to NLP transfer learning, showing how to match fasttext's results on 4 million Amazon re‚Ä¶
1029440004852994000,Tue Aug 14 18:49:57 +0000 2018,RT @IAugenstein: Would you like to do a PhD with me @uni_copenhagen? 28 Marie Curie PhD fellowships are available at our faculty. Apply by‚Ä¶
1029430622480150500,Tue Aug 14 18:12:40 +0000 2018,RT @PeterMartigny: How to do NLP classification if you don't have much labeled data? See how the @feedly team uses the ULMFit paper from @s‚Ä¶
1029293018103377900,Tue Aug 14 09:05:53 +0000 2018,RT @nsaphra: I wrote a brief lit review on modifying hyperparameters during DNN training. There's gotta be someone else is interested in ad‚Ä¶
1029290976895004700,Tue Aug 14 08:57:46 +0000 2018,@Opetunde_ @_tabbz @DeepIndaba Awesome! Me too! In the meantime, let me know if there's any way I can help (just shoot me an email).
1029129752681037800,Mon Aug 13 22:17:08 +0000 2018,RT @PeterMartigny: New ML blogpost from the @feedly team, play with CNNs and transfer learning with your own pics :) https://t.co/HgYBDI6MS‚Ä¶
1029103720854966300,Mon Aug 13 20:33:41 +0000 2018,@jkkummerfeld @PyTorch @TensorFlow This is really great! Thank you! :)
1029103690563678200,Mon Aug 13 20:33:34 +0000 2018,RT @jkkummerfeld: I made a page showing how to implement a good (97.2% acc) POS tagger in #DyNet, @PyTorch and @TensorFlow, visualised in a‚Ä¶
1029030055635284000,Mon Aug 13 15:40:58 +0000 2018,@Abdullah_MA_b @DeepIndaba üòå
1029028709016191000,Mon Aug 13 15:35:37 +0000 2018,Join us for the session on Frontiers of Natural Language Processing at the Deep Learning Indaba. We'll have panel d‚Ä¶ https://t.co/l7iB8K9TO9
1028903209488797700,Mon Aug 13 07:16:55 +0000 2018,@adamshamsudeen @Smerity @jeremyphoward We've actually been working on Wiki Text-like datasets and Language model f‚Ä¶ https://t.co/k5IZkN8Oun
1028663319018782700,Sun Aug 12 15:23:41 +0000 2018,@aijournalyt Very nice! Thanks for the video overview! Will mention it in the newsletter and point people to your channel. :)
1028629869280804900,Sun Aug 12 13:10:46 +0000 2018,RT @coastalcph: 7 papers at EMNLP, 3 at CoNLL, and first place in the ConLL/SIGMORPHON shared task (T2) - everyone at @coastalcph is excite‚Ä¶
1028594957278498800,Sun Aug 12 10:52:02 +0000 2018,RT @Tema_telewo: Hi everyone,  Please help me to take the AI's ambassadors of #Cameroon to DeepLearning Indaba 2018. https://t.co/rrMrQ3aqi‚Ä¶
1028204920682500100,Sat Aug 11 09:02:10 +0000 2018,@tpilehvar @emnlp2018 Congrats!
1028004789043585000,Fri Aug 10 19:46:55 +0000 2018,@natschluter @DanielVarab @NLPatITU Congrats! üëèüèª
1028004673284976600,Fri Aug 10 19:46:28 +0000 2018,@tallinzen @emnlp2018 Awesome! Congrats! :)
1028004564832841700,Fri Aug 10 19:46:02 +0000 2018,@varish @_shrdlu_ Soon. :) Will post the preprint in a couple of weeks.
1028001855333380100,Fri Aug 10 19:35:16 +0000 2018,Yay! Our paper "A Discriminative Latent-Variable Model for Bilingual Lexicon Induction" with @_shrdlu_, Yova Kement‚Ä¶ https://t.co/Uoh3PMs3ec
1027999696172789800,Fri Aug 10 19:26:41 +0000 2018,RT @Raspberry_Pi: In the UK, we call him Wally. In France, people ask "O√π est Charlie?". Germany calls him Walter, and in Croatia? "Gdje je‚Ä¶
1027998687400783900,Fri Aug 10 19:22:41 +0000 2018,@barbara_plank @zeljkoagic Congrats, Barbara! üòä
1027998621025951700,Fri Aug 10 19:22:25 +0000 2018,@artetxem @glabaka @eagirre Congrats! üëç
1027998538129711100,Fri Aug 10 19:22:05 +0000 2018,@manaalfar @emnlp2018 @Google @dipanjand @jasonbaldridge @iftenney Congrats!
1027966743162040300,Fri Aug 10 17:15:44 +0000 2018,RT @elihoole: Found the blog post on the level of maths required for ML particularly useful. Follow @seb_ruder's great monthly #machinelear‚Ä¶
1027966724816162800,Fri Aug 10 17:15:40 +0000 2018,@elihoole @revue Thank you, Elijah! :)
1027942208672915500,Fri Aug 10 15:38:15 +0000 2018,RT @dannimassi: Attending the Deep Learning Indaba? Have a project or work in computer vision to share? Then apply to present a spotlight t‚Ä¶
1027940107012198400,Fri Aug 10 15:29:54 +0000 2018,RT @jeremyphoward: A New Kind of Clinical Trial https://t.co/EnNde2NsQV My write-up of our new collaboration at @_doc_ai with great folks f‚Ä¶
1027911059217293300,Fri Aug 10 13:34:28 +0000 2018,RT @srchvrs: Sorry folks I can tolerate these alchemy and pseudo-science claims any more. Here is my take on this: https://t.co/ZngbEpnrsA
1027889300342534100,Fri Aug 10 12:08:01 +0000 2018,This is a very thoughtful Q&amp;A with Yoshua Bengio not only about starting a lab, but also about how to foster collab‚Ä¶ https://t.co/LGYOpIBqJd
1027174955979563000,Wed Aug 08 12:49:28 +0000 2018,@SeguiSanti Thanks, Santi! :)
1027125201094500400,Wed Aug 08 09:31:45 +0000 2018,RT @AleMas2100: The best ML newsletter I've seen so far  https://t.co/msrVXdI0VE  thanks @seb_ruder
1027106654805008400,Wed Aug 08 08:18:03 +0000 2018,@AleMas2100 Thanks! That means a lot!
1026584520563339300,Mon Aug 06 21:43:17 +0000 2018,@F_Vaggi Hey, I don't think I received your email, Federico.
1026578962858815500,Mon Aug 06 21:21:12 +0000 2018,RT @kdnuggets: Weren't at ACL 2018? Read the highlights by @seb_ruder: Understanding Representations and Evaluation in More Challenging Set‚Ä¶
1026499897367912400,Mon Aug 06 16:07:01 +0000 2018,@F_Vaggi Sure. You can find my email on https://t.co/U5moxQLuSK.
1026495067534630900,Mon Aug 06 15:47:50 +0000 2018,@F_Vaggi Hey! Thanks for pointing that out. I haven't heard about that yet. Is there anything I can do on my end?
1026429349099126800,Mon Aug 06 11:26:41 +0000 2018,@sksq96 @revue Thanks for spotting that! :)
1026407600299167700,Mon Aug 06 10:00:16 +0000 2018,New NLP News: Future of CV and NLP, Maths (Fields Medal, Maths for ML), AutoML Natural Language, Google Assistant,‚Ä¶ https://t.co/dJRFSbGdd1
1026166001568817200,Sun Aug 05 18:00:14 +0000 2018,@aijournalyt üëçüèª
1026139575121588200,Sun Aug 05 16:15:14 +0000 2018,@aijournalyt Hey! It's a bit hard to direct readers to a whole channel. If you have a video that focuses on a parti‚Ä¶ https://t.co/MI2O6j4GKM
1024624955143675900,Wed Aug 01 11:56:40 +0000 2018,RT @prajjwal_1: Universal Language Models Fine tuning (ULMFit) summed up!  https://t.co/KmeYkWllWs Cc:@jeremyphoward @seb_ruder
1024558741650399200,Wed Aug 01 07:33:34 +0000 2018,RT @shilpa15397: I cannot believe that it‚Äôs taken me this long to realise that something like the #nlphighlights podcast exists! It‚Äôs a bri‚Ä¶
1024246166748311600,Tue Jul 31 10:51:30 +0000 2018,RT @redpony: Doing a PhD in NLP or computational linguistics? Want to work with a world class interdisciplinary language team at @DeepMindA‚Ä¶
1023932936637345800,Mon Jul 30 14:06:50 +0000 2018,RT @dirk_hovy: Very excited to be part of this initiative! In this context, I am also looking for a postdoc who is interested in applying #‚Ä¶
1022987557016162300,Fri Jul 27 23:30:14 +0000 2018,@artetxem That's awesome! Congrats!
1022987513550594000,Fri Jul 27 23:30:04 +0000 2018,RT @artetxem: ...and to continue with the good news, I got the National Award to the best academic record of Spain. Not a bad day! https://‚Ä¶
1022948830143230000,Fri Jul 27 20:56:21 +0000 2018,@mariokostelac Thanks so much! I'm really glad it's helpful!
1022825494088167400,Fri Jul 27 12:46:15 +0000 2018,RT @lschmelzeisen: I'm really relieved that tracking popular tasks, datasets, and best techniques in NLP seems to become much easier in the‚Ä¶
1022824924866596900,Fri Jul 27 12:43:59 +0000 2018,@yoavgo Yay! Congrats! :)
1022530753794658300,Thu Jul 26 17:15:04 +0000 2018,RT @skynet_today: Has AI surpassed humans at translation? Not even close.   Present day NMT systems still fail at many essential aspects of‚Ä¶
1022472444119068700,Thu Jul 26 13:23:21 +0000 2018,Two themes were most prominent for me at #ACL2018: 1) Understanding representations. 2) Evaluating models in more c‚Ä¶ https://t.co/ZQfTGnH9c9
1022471702188634100,Thu Jul 26 13:20:25 +0000 2018,RT @_aylien: Last week, our Research Scientist @seb_ruder presented work at #ACL2018. Check out his summary of the conference here - https:‚Ä¶
1022116084172955600,Wed Jul 25 13:47:19 +0000 2018,RT @etzioni: One year a anniversary of the outstanding NLP Newsletter by @seb_ruder https://t.co/FemIXnpIn5
1022108974836015100,Wed Jul 25 13:19:04 +0000 2018,@sai_prasanna Awesome, thanks! :)
1021792042878464000,Tue Jul 24 16:19:41 +0000 2018,@YannFletBerliac @dennybritz @ch402 @karpathy @michael_nielsen I'm using @mendeley_com. I've tried others but have‚Ä¶ https://t.co/tfSxsqdEsj
1021759055705063400,Tue Jul 24 14:08:36 +0000 2018,RT @_aylien: Congratulations @seb_ruder on one year of NLP News üéâAnyone who isn't signed up to this great newsletter, check out the latest‚Ä¶
1021659625748344800,Tue Jul 24 07:33:30 +0000 2018,@PiotrZelasko @MarkNeumannnn @yoavgo Slides from Yejin Choi and Tim Baldwin are already available. https://t.co/maSBXd7RQR
1021659250387501000,Tue Jul 24 07:32:01 +0000 2018,@pmigdal Great! Just replied.
1021355566134976500,Mon Jul 23 11:25:17 +0000 2018,@pmigdal Thanks for the feedback! There's an issue on generating a progress chart. Also happy to accept PRs. üòâ
1021334188996415500,Mon Jul 23 10:00:20 +0000 2018,New NLP News: ACL 2018, Videos from NAACL 2018 and ICML 2018, Fanatic NMT, MojiTalk, CompareGAN, NLP is fun, Progra‚Ä¶ https://t.co/YtjpeuN9Fb
1020959447919689700,Sun Jul 22 09:11:15 +0000 2018,@al3hIshek Alexander Rush's lab at Stanford has done some work on this, e.g. see here: https://t.co/VoKzFbb1ai It's‚Ä¶ https://t.co/ABJvbfOt4H
1020896599336075300,Sun Jul 22 05:01:31 +0000 2018,RT @explosion_ai: Just added a "best practices" tag to the https://t.co/04W4hbjE9O forum. Here are some interesting discussions around defi‚Ä¶
1020896285707006000,Sun Jul 22 05:00:16 +0000 2018,@egrefen @YejinChoinka Would you like to add the dataset? ;)
1020893894848872400,Sun Jul 22 04:50:46 +0000 2018,RT @Abdullah_MA_b: We're group of sudanese undergrad who wish to take the next big step in their research career by attending Indaba AI sum‚Ä¶
1020843388889780200,Sun Jul 22 01:30:04 +0000 2018,@AlfredoCanziani Yep. It's already there under multi task learning.
1020809838643695600,Sat Jul 21 23:16:45 +0000 2018,@joshmeyerphd Probably DeepVoice (https://t.co/UeGIJ3RMBl), which jointly predicts phoneme duration and frequency profile.
1020809178888036400,Sat Jul 21 23:14:08 +0000 2018,@andreaskirkedal @syhw Good idea. Added a link to the repo. :)
1020498167861661700,Sat Jul 21 02:38:17 +0000 2018,RT @pymnpsbn: I couldn't attend #NAACL2018 this year but if you are interested in #NMT and translating morphologically complex languages wa‚Ä¶
1020494402953736200,Sat Jul 21 02:23:20 +0000 2018,Did your approach achieve a new state-of-the-art at #ACL2018? Did you propose a new dataset or task? Add it to this‚Ä¶ https://t.co/idIw4maGd9
1020471201641087000,Sat Jul 21 00:51:08 +0000 2018,@universeinanegg Thanks! :)
1020471136541335600,Sat Jul 21 00:50:53 +0000 2018,@NickFisherAU @yoavgo Totally. Language modelling will definitely not get us to true language understanding, but it‚Ä¶ https://t.co/2DuO3761QE
1020470816243212300,Sat Jul 21 00:49:36 +0000 2018,@ink__pad Arguably a more appropriate ending. :)
1020470654573793300,Sat Jul 21 00:48:58 +0000 2018,@ajinkyakale You're welcome. :)
1020470612030980100,Sat Jul 21 00:48:47 +0000 2018,@keratin7 Glad it's appreciated. :)
1020470557014310900,Sat Jul 21 00:48:34 +0000 2018,@ajinkyakale Thanks! Yeah, hope we'll manage to record these sometime in the future. :)
1020470448830623700,Sat Jul 21 00:48:09 +0000 2018,@PiotrZelasko @MarkNeumannnn @yoavgo Not yet as far as I know. Will ping the organizers and reply here once they upload them.
1020209959043096600,Fri Jul 20 07:33:03 +0000 2018,Jason Eisner: Parameters drive out theory. If you want more theory, reduce the number of parameters. Fin.
1020209788968362000,Fri Jul 20 07:32:22 +0000 2018,Graham: Lots of really good papers that use a lot of compute and lots of good papers that don't use a lot of comput‚Ä¶ https://t.co/O4rP3ESDCl
1020209474206756900,Fri Jul 20 07:31:07 +0000 2018,Yejin: Current era democratizes. Writing insightful papers or blogs is really useful to the community. Audience: Re‚Ä¶ https://t.co/2GgY5op7kM
1020208903248728000,Fri Jul 20 07:28:51 +0000 2018,Yejin: Did not deliver as much. Curious why in some cases a lot of theory does not deliver. Inference in such metho‚Ä¶ https://t.co/wYLFJVa44l
1020208760424345600,Fri Jul 20 07:28:17 +0000 2018,Meg: Interesting when theory can drive empirical work. More need for theory. Yejin (being controversial): Academia‚Ä¶ https://t.co/BPOXbowMCC
1020208208378441700,Fri Jul 20 07:26:06 +0000 2018,Ryan: RNNs are still very recently used models. Might take a few more years.  Audience: Will just building better m‚Ä¶ https://t.co/vtgHRhEL6x
1020207711525322800,Fri Jul 20 07:24:07 +0000 2018,Audience: Are we actually able to deliver. Should better describe what we don't do.  Graham: A lot of papers examin‚Ä¶ https://t.co/dJffOutGCw
1020207329365483500,Fri Jul 20 07:22:36 +0000 2018,Yejin: We are struggling with tasks that actually require understanding and reasoning. Audience: Doing really well‚Ä¶ https://t.co/m0E7ZT0xBr
1020206844160114700,Fri Jul 20 07:20:40 +0000 2018,Yejin: Can't parse entire image or understand whole image. Action recognition is still very poor. NLP tasks that wo‚Ä¶ https://t.co/Nw62XezaVh
1020206555499647000,Fri Jul 20 07:19:32 +0000 2018,Audience: Is there something fundamentally wrong with our current architectures, the hacks we're using? Yoav: They'‚Ä¶ https://t.co/DJulwA1MEi
1020206219372326900,Fri Jul 20 07:18:11 +0000 2018,Meg: Gains on constrained tasks are often confused with gains on the general task. That's where messier, more gener‚Ä¶ https://t.co/X3pDcatU4T
1020205898378035200,Fri Jul 20 07:16:55 +0000 2018,Yejin: Possibly better to transfer knowledge from elsewhere. Also relevant for NLP QA tasks, which require understa‚Ä¶ https://t.co/961UF9bGBp
1020205639887290400,Fri Jul 20 07:15:53 +0000 2018,Yejin: Already do that to some extent, e.g. VQA. Often deeper understanding of world and language is required. Inte‚Ä¶ https://t.co/Tz2mnJEHzf
1020205302682083300,Fri Jul 20 07:14:33 +0000 2018,Graham: Going back to previous answer, if we can prove that we are doing sth suboptimally, then psycholinguistics m‚Ä¶ https://t.co/OKxclcYV6D
1020204743056998400,Fri Jul 20 07:12:19 +0000 2018,Ryan: Psycholinguistics research does not help to build better NLP systems, i.e. does not enable practical models.‚Ä¶ https://t.co/k0WXUEdBPt
1020204090259730400,Fri Jul 20 07:09:44 +0000 2018,Yejin: Children learn from declarative explanations about how the world works. Robotics field will likely not evolv‚Ä¶ https://t.co/9kdp8D3qqh
1020203818502348800,Fri Jul 20 07:08:39 +0000 2018,Audience: People who are blind and/or deaf can learn language. Yejin: Groundedness matters a lot, but perhaps more‚Ä¶ https://t.co/Ufq7kOMmcV
1020203410140758000,Fri Jul 20 07:07:02 +0000 2018,Meg: History of feral children not exposed to language who fail to learn language later on in life. Part of human l‚Ä¶ https://t.co/si7Zxnd1VC
1020202985240973300,Fri Jul 20 07:05:20 +0000 2018,Ndapa: Background knowledge is useful. Collecting commonsense knowledge, though, is challenging. Meg: We're not wor‚Ä¶ https://t.co/EDGvP5K4SV
1020202665362436100,Fri Jul 20 07:04:04 +0000 2018,Q: Human brain is only known system for understanding language. Should we be more inspired by human language learni‚Ä¶ https://t.co/WsgubZfiIe
1020202448428752900,Fri Jul 20 07:03:12 +0000 2018,Yejin: Mainly better because LSTMs don't work very well across sentence boundaries. Will take several years to deve‚Ä¶ https://t.co/WdtoiBA7sR
1020202145490976800,Fri Jul 20 07:02:00 +0000 2018,Graham: As a reviewer, don't ask authors to evaluate on SQuAD if people propose a new discourse-driven model for QA‚Ä¶ https://t.co/bZ72y4PAwj
1020201741038436400,Fri Jul 20 07:00:24 +0000 2018,Ryan: State-of-the-art LMs don't reset hidden states between sentences, so can incorporate sentence dependencies. G‚Ä¶ https://t.co/NX1Mqe38Wt
1020201242520244200,Fri Jul 20 06:58:25 +0000 2018,Yejin: The community should not work on datasets for too long (datasets are getting solved very fast these days), s‚Ä¶ https://t.co/H8kMlLnJLu
1020201039121616900,Fri Jul 20 06:57:36 +0000 2018,Yejin: Community pays a lot of attention to easier tasks such as SQuAD or baBi, which are close to solved. Have to‚Ä¶ https://t.co/hTpT65T1xy
1020200711307472900,Fri Jul 20 06:56:18 +0000 2018,Meg: True if only done word-by-word. Other things to be done e.g. in generative models using latent variables. Yeji‚Ä¶ https://t.co/OMIYieWrhx
1020200290509733900,Fri Jul 20 06:54:38 +0000 2018,Meg: Depends on downstream task (some need attention/information flow, others don't). A little bit of both.  Audien‚Ä¶ https://t.co/J3IU7lbgLL
1020199991996923900,Fri Jul 20 06:53:27 +0000 2018,Yoav: Hard to do in end-to-end settings, but possibly easier to manipulate if we have modules, so maybe move away a‚Ä¶ https://t.co/87wOgH66xI
1020199810006040600,Fri Jul 20 06:52:43 +0000 2018,Audience: How do our current efforts match fare in terms of representation stability across time? Meg: Representati‚Ä¶ https://t.co/Hq1Xb5gGUx
1020199311282335700,Fri Jul 20 06:50:44 +0000 2018,Audience: Going towards more human-readable representations by e.g. automatically extracting discrete formal repres‚Ä¶ https://t.co/fmedWPMHss
1020198739418406900,Fri Jul 20 06:48:28 +0000 2018,Yejin: Distributional semantics leads to good performance because some contextual information is incorporated witho‚Ä¶ https://t.co/nqTxWh1hOP
1020198464813088800,Fri Jul 20 06:47:23 +0000 2018,Graham: Hard to answer. Different ACL papers isolate problems, e.g. context necessary for MT, number agreement, etc‚Ä¶ https://t.co/OL7kykfjrg
1020198285040992300,Fri Jul 20 06:46:40 +0000 2018,Audience: Should we keep working on models that incorporate distributional semantics given that they can only take‚Ä¶ https://t.co/TsN8R8fr6h
1020197928554586100,Fri Jul 20 06:45:15 +0000 2018,Yoav: Running a model on multiple GPUs for many weeks costs what annotating several treebanks would cost. Money see‚Ä¶ https://t.co/ZpWZevCkdE
1020197689160446000,Fri Jul 20 06:44:18 +0000 2018,Yoav: Training a language model does not require expensive data annotation. Audience: Web is much smaller in non-En‚Ä¶ https://t.co/9c4rAsf1z2
1020197589113757700,Fri Jul 20 06:43:54 +0000 2018,Graham: 20% percent error reduction on task is only 20% interesting. Lots of tasks that we care about that don‚Äôt ha‚Ä¶ https://t.co/utty4fNZ9S
1020197319105425400,Fri Jul 20 06:42:49 +0000 2018,Important to focus on what you really love. Work with people that have complimentary and different interests. Yoav:‚Ä¶ https://t.co/sFgnpwbMYw
1020197085793042400,Fri Jul 20 06:41:54 +0000 2018,Meg: Different people have different strengths. People say: ‚ÄúWe should all care about ethics‚Äù. Geek out about what‚Ä¶ https://t.co/vBKxJpkJ57
1020196710050455600,Fri Jul 20 06:40:24 +0000 2018,#Repl4NLP at #ACL2018 panel discussion: Q: Given that the amount of data and computing power is rapidly increasing,‚Ä¶ https://t.co/hLe3Q1ADSN
1020162246159130600,Fri Jul 20 04:23:27 +0000 2018,.@yoavgo on directions for language model pretraining and fine-tuning at #Repl4NLP workshop at #acl2018 https://t.co/WgsWKO6HZQ
1020077432731123700,Thu Jul 19 22:46:26 +0000 2018,@dhwaj_raj @jeremyphoward Working on it.
1019829639563305000,Thu Jul 19 06:21:48 +0000 2018,@johannestknd Not yet I think. Will tweet once I hear about them.
1019768157391605800,Thu Jul 19 02:17:29 +0000 2018,@GuggerSylvain @fastdotai @awscloud @jeremyphoward @math_rachel Congrats! üéâ
1019738459299123200,Thu Jul 19 00:19:29 +0000 2018,@Sohilnewa ACL 2019 is in Florence, Italy.
1019440753980796900,Wed Jul 18 04:36:30 +0000 2018,RT @haldaume3: wait really?  NAACL 2018: New Orleans NAACL 2019: Minneapolis ACL 2020: Seattle  i was surprised that '19 will be in the US,‚Ä¶
1019243561395425300,Tue Jul 17 15:32:56 +0000 2018,RT @_aylien: It's a big week here at AYLIEN - we're releasing a brand new endpoint and upgrading two more! Entity-level Sentiment Analysis‚Ä¶
1019130531349295100,Tue Jul 17 08:03:47 +0000 2018,Venue of ACL 2020 will be Seattle. Dates: July 6-11 #acl2018 https://t.co/Ll3lnIE7LI
1018775889306214400,Mon Jul 16 08:34:34 +0000 2018,RT @SriVidya_1729: @seb_ruder 's " Universal language model fine tuning for text classification" definitely was one of the interesting post‚Ä¶
1018723522334449700,Mon Jul 16 05:06:29 +0000 2018,RT @IAugenstein: Check out @seb_ruder's poster on "Universal Language Model Fine-Tuning for Text Classification" #acl2018 #NLProc @jeremyph‚Ä¶
1018665055380856800,Mon Jul 16 01:14:09 +0000 2018,RT @StevenBird: #DeepLearning in a remote community in @WestArnhem, my excuse for missing #ACL2018 Melbourne. It was there in 1985 that I d‚Ä¶
1018650426764226600,Mon Jul 16 00:16:02 +0000 2018,The newest addition to the *ACL family: AACL, the Asia-Pacific Chapter of the ACL. Looking forward to more NLP rese‚Ä¶ https://t.co/jjppQlCcR3
1017808312081973200,Fri Jul 13 16:29:46 +0000 2018,RT @m__dehghani: Check out "Universal Transformers". Work done during my internship at  @GoogleBrain w/ @sgouws, @OriolVinyalsML, @kyosu, a‚Ä¶
1017740711914074100,Fri Jul 13 12:01:09 +0000 2018,Videos of the talks and slides will be available online afterwards.
1017740347340927000,Fri Jul 13 11:59:42 +0000 2018,5pm (3B): "Strong Baselines for Neural Semi-Supervised Learning under Domain Shift" https://t.co/fWeJVZ1XTL
1017740231750111200,Fri Jul 13 11:59:14 +0000 2018,I'll be giving a poster presentation and 2 oral presentations at #ACL2018 on Monday: 12.30-2pm (1A): "Universal Lan‚Ä¶ https://t.co/vYLtVpD6g3
1017710175866818600,Fri Jul 13 09:59:48 +0000 2018,@Even_Oldridge @jeremyphoward I think it really depends on how you set up your experiments. If you can specify diff‚Ä¶ https://t.co/xo2Nw9POKK
1017390612683911200,Thu Jul 12 12:49:58 +0000 2018,RT @licwu: Our new survey on typological linguistics for NLP is available online. All that you ever wanted to know about modeling language‚Ä¶
1017344973254127600,Thu Jul 12 09:48:37 +0000 2018,@thomaskipf @MatthiasSBauer Yes, something like that would be great! üëçüèª
1017338749028466700,Thu Jul 12 09:23:53 +0000 2018,@thomaskipf @MatthiasSBauer Hey Thomas, any chance someone could take notes of the discussion for people who can't make it?
1017085540955574300,Wed Jul 11 16:37:44 +0000 2018,RT @vaibhaw_vipul: It is an amazing feeling when u get to talk to some1 who has been inspiring u. @seb_ruder his newsletter and blogs and r‚Ä¶
1016697348074999800,Tue Jul 10 14:55:11 +0000 2018,RT @bigdata: .@seb_ruder:  "it is very likely that in a year‚Äôs time NLP practitioners will download pretrained language models rather than‚Ä¶
1016632975205830700,Tue Jul 10 10:39:24 +0000 2018,RT @deliprao: New #nlproc blog post: "Design Patterns for Production NLP Systems"  https://t.co/m4a4MYgtfT https://t.co/GUdydhnHVy
1016631146979971100,Tue Jul 10 10:32:08 +0000 2018,@haldaume3 My hope is that it will be similar to how we've been using word2vec: Instead of using pretrained word em‚Ä¶ https://t.co/8ovKfotJlg
1016448335610335200,Mon Jul 09 22:25:42 +0000 2018,RT @cnancyxu: i'm excited we've launched Perspectives @gradientpub today. great first few pieces on wide spectrum of AI opinions by our wri‚Ä¶
1016328193970712600,Mon Jul 09 14:28:18 +0000 2018,RT @andrey_kurenkov: My far too long in the making essay about RL is finally out! Check it our, some juicy stuff in it (especially part two‚Ä¶
1016324879451385900,Mon Jul 09 14:15:08 +0000 2018,RT @gradientpub: ImageNet is a mainstay of modern computer vision research.  Can we build such a resource for natural language?  As @seb_ru‚Ä¶
1016273369476583400,Mon Jul 09 10:50:27 +0000 2018,RT @jeremyphoward: My collaborator @seb_ruder does a wonderful job here of explaining the implications of our (+ @allen_ai and @openai) rec‚Ä¶
1016267223948722200,Mon Jul 09 10:26:02 +0000 2018,@mously_ @gradientpub Thanks! Thanks for spotting. Will be fixed.
1016260757355483100,Mon Jul 09 10:00:20 +0000 2018,New NLP News, this time with a different format. Themes: RL and teamwork, Tracking the state of AI, Facebook and fa‚Ä¶ https://t.co/myOWHLVROc
1016244913929900000,Mon Jul 09 08:57:23 +0000 2018,@unsorsodicorda @gradientpub I'd say the works that I mention in the first paragraph: ELMo, ULMFiT, and the OpenAI Transformer.
1016230522786320400,Mon Jul 09 08:00:11 +0000 2018,RT @gradientpub: It‚Äôs time to complete the second half of our launch.  Introducing Perspectives: a platform for editorial opinions on AI.‚Ä¶
1016228686385860600,Mon Jul 09 07:52:54 +0000 2018,@gradientpub @andrey_kurenkov @tl0en @acganesh Do check out today's other perspectives: on AI regulation by‚Ä¶ https://t.co/NnL7pJy5gv
1016227864600088600,Mon Jul 09 07:49:38 +0000 2018,@gradientpub Huge thanks to the fantastic editors at @gradientpub @andrey_kurenkov, @tl0en, and @acganesh who helpe‚Ä¶ https://t.co/vyajWFfsik
1016226532363329500,Mon Jul 09 07:44:20 +0000 2018,New piece about a direction I'm super excited about: NLP's ImageNet moment has arrived @gradientpub https://t.co/Kb3szva7tC
1015256209891446800,Fri Jul 06 15:28:37 +0000 2018,@deliprao I guess a bit niche unless you want to crowd-source / create it yourself. Or: You can write to the startu‚Ä¶ https://t.co/w2WU3UXsws
1015140093387001900,Fri Jul 06 07:47:13 +0000 2018,@deliprao Sounds like this Cards Against Humanity for Startups might be what you're looking for. https://t.co/enIMuIjZLM
1014948783661224000,Thu Jul 05 19:07:01 +0000 2018,@realdoug @jeremyphoward We're working on applying it to some sequence labeling tasks at the moment. Feel free to t‚Ä¶ https://t.co/VTYUSKr4A2
1014948783661224000,Thu Jul 05 19:07:01 +0000 2018,@realdoug @jeremyphoward We're working on applying it to some sequence labeling tasks at the moment. Feel free to t‚Ä¶ https://t.co/VTYUSKr4A2
1014791285851459600,Thu Jul 05 08:41:11 +0000 2018,@alanyttian @_aixile @minjunli Very cool!
1014416482883817500,Wed Jul 04 07:51:51 +0000 2018,@uku_peter Thanks! Feel free to contribute if there's anything you'd like to add. :)
1013432919988240400,Sun Jul 01 14:43:31 +0000 2018,@romanthecap Cool! üëçüèª
1013015198062514200,Sat Jun 30 11:03:38 +0000 2018,@albertgarcia93 @sortses @pablo_2093 @sergiuop @iros_2018 @EPSAlicante @UA_Universidad Nice! Congrats! :)
1012608749465428000,Fri Jun 29 08:08:33 +0000 2018,@omarsar0 Yep. I quoted the same tweet as Devi in my tweet. ;) https://t.co/LfaZDXGMAa
1012607900039213000,Fri Jun 29 08:05:11 +0000 2018,@bartsmeets86 Thanks for sharing! :)
1012598103554347000,Fri Jun 29 07:26:15 +0000 2018,@IAugenstein I think paper writing and presenting, but I guess only if you submit early enough to be assigned a men‚Ä¶ https://t.co/zxWVAwfix7
1012332589229772800,Thu Jun 28 13:51:12 +0000 2018,RT @nlothian: So I spent a day and a half manually stepping through my data pipelines because I couldn't believe the results I'm getting.‚Ä¶
1012319460944998400,Thu Jun 28 12:59:02 +0000 2018,@anmarasovic üòú
1012314750963929100,Thu Jun 28 12:40:19 +0000 2018,@omarsar0 Yep, totally agree. I think slides and videos should definitely be made available.
1012314452841164800,Thu Jun 28 12:39:08 +0000 2018,@zehavoc Ideally yeah. But in many labs, you are not explicitly taught how to review, how to give a good talk, how‚Ä¶ https://t.co/35wg8XS8sp
1012302061629829100,Thu Jun 28 11:49:53 +0000 2018,RT @johnbreslin: Great initiative from @seb_ruder - "Repository to track the progress in Natural Language Processing (NLP), including the d‚Ä¶
1012301762370424800,Thu Jun 28 11:48:42 +0000 2018,This looks super useful! Can we have a workshop like this at an #NLProc conference next year: Good Citizen of *ACL?‚Ä¶ https://t.co/lSNyZBPcVl
1012264551482646500,Thu Jun 28 09:20:50 +0000 2018,@kdubovikov Thanks, Kirill! I appreciate it. :)
1012255091695513600,Thu Jun 28 08:43:15 +0000 2018,@NirantK Hi Nirant, that's a good point. That really detracts from the reading experience. :( I'll experiment with‚Ä¶ https://t.co/4CCJYzID4e
1011992805630206000,Wed Jun 27 15:21:01 +0000 2018,@jkkummerfeld @radevd @michigan_AI üéâ
1011913052315439100,Wed Jun 27 10:04:06 +0000 2018,RT @_aylien: There's a familiar face on GitHub's trending repos this morning ... Check out this repo our Research Scientist @seb_ruder has‚Ä¶
1011711895634628600,Tue Jun 26 20:44:47 +0000 2018,@fionntan_tech @jeremyphoward Cool! Yes, that should be definitely applicable.
1011670824204537900,Tue Jun 26 18:01:35 +0000 2018,If you care about European excellence in AI, consider signing the CLAIRE proposal: https://t.co/TD3jkRsCLx https://t.co/332n9gEEv9
1011657257803804700,Tue Jun 26 17:07:40 +0000 2018,@NicolasPapernot Fancy! :) Well done! üëèüèª
1011638719458283500,Tue Jun 26 15:54:00 +0000 2018,@fionntan_tech @jeremyphoward We haven't tried extreme multilabel classification yet, but I don't see why it wouldn‚Ä¶ https://t.co/s221SZpiZM
1011638449848414200,Tue Jun 26 15:52:56 +0000 2018,@jkkummerfeld @radevd @michigan_AI Thanks! :)
1011607387583799300,Tue Jun 26 13:49:30 +0000 2018,@jkkummerfeld @radevd @michigan_AI That's awesome! Any chance you could add the datasets here?  https://t.co/bXT6jQYMIc
1011548914883924000,Tue Jun 26 09:57:09 +0000 2018,RT @omarsar0: As I read through the ULMFit (transfer learning for NLP) paper by @jeremyphoward  and @seb_ruder all I kept saying to myself‚Ä¶
1011539496624689200,Tue Jun 26 09:19:44 +0000 2018,@Miles_Brundage As far as I'm aware, the methods described in this paper (https://t.co/cF1NZ374kF) are used in prod‚Ä¶ https://t.co/ZQdViQtEn0
1011352499679309800,Mon Jun 25 20:56:40 +0000 2018,Check out @Smerity's thread for some of the high-level implications of AI becoming superhuman at 'team work' of the‚Ä¶ https://t.co/FJZrPSfsGd
1011351248950431700,Mon Jun 25 20:51:42 +0000 2018,@_shrdlu_ @JYaoTheGunner @xtimv Well played. :) We need metrics, though, to measure progress. We just need to create better datasets.
1011252004226846700,Mon Jun 25 14:17:20 +0000 2018,@jeremyphoward @NirantK +1 Also search for InferSent and GenSent for state-of-the-art sentence embeddings.
1011204902754930700,Mon Jun 25 11:10:10 +0000 2018,@DaniaL_KH Very nice! üëèüèªThat'll go in the next newsletter. :) https://t.co/0h8zWma2dl
1011187365673685000,Mon Jun 25 10:00:29 +0000 2018,New NLP Newsletter w/ even more cool stuff this time! Transfer learning, Chris Olah, Software 2.0, NMT with attenti‚Ä¶ https://t.co/SHOCxWlK3g
1011162322881142800,Mon Jun 25 08:20:58 +0000 2018,@microth @iatitov @srchvrs @VeredShwartz Here's the SOTA on OntoNotes AFAIK: https://t.co/IHCAY7Yvye Michael, feel‚Ä¶ https://t.co/oHHBIIjTpO
1010912980668690400,Sun Jun 24 15:50:11 +0000 2018,@jeffrschneider That's a great idea! Will add that! :)
1010865446428528600,Sun Jun 24 12:41:18 +0000 2018,NLP-progress is trending on GitHub today! And already 4 PRs! This is so awesome! Thanks everyone for contributing!‚Ä¶ https://t.co/OQQfdd5yEK
1010479227324551200,Sat Jun 23 11:06:36 +0000 2018,@unsorsodicorda @prfsanjeevarora Yes, I think so. Feel free to submit a PR with the results. :)
1010445372035076100,Sat Jun 23 08:52:04 +0000 2018,@fjacquenet Thank you, Francois! That really means a lot. I was initially just trying to scratch my own itch and th‚Ä¶ https://t.co/Z1fpBFruOD
1010280822425964500,Fri Jun 22 21:58:12 +0000 2018,@zehavoc Agreed! Would be cool to reinvigorate that page. I don't really care where this lives. I just want an up-t‚Ä¶ https://t.co/uBk5XrSsgx
1010277334463770600,Fri Jun 22 21:44:21 +0000 2018,RT @SanhEstPasMoi: Everyone in the NLP research community should start watching/starring/bookmarking this awesome repo ! https://t.co/OVYJ1‚Ä¶
1010225396233711600,Fri Jun 22 18:17:58 +0000 2018,Here's the direct link to the repo: https://t.co/bXT6jQYMIc Let me know if you find this helpful. Submit a PR if I'‚Ä¶ https://t.co/2lJxwVVaQx
1010225031794888700,Fri Jun 22 18:16:31 +0000 2018,Do you often find it cumbersome to track down the best datasets or the state-of-the-art for a particular task in NL‚Ä¶ https://t.co/Y5Tl4kPRR2
1010113833309016000,Fri Jun 22 10:54:39 +0000 2018,RT @cnancyxu: if you aren't on the train yet, intro inverse RL piece on learning human actions through expert demonstration @gradientpub ht‚Ä¶
1009886943654699000,Thu Jun 21 19:53:04 +0000 2018,RT @kloeebt: I'm looking for a candidate for a 1-year postdoc on NLP, to work on discourse parsing at  @labo_Loria, @synalp_nancy in Nancy‚Ä¶
1009741552527212500,Thu Jun 21 10:15:20 +0000 2018,RT @the_antlr_guy: Launched a new website, https://t.co/WDCq9K3lwv, dedicated to giving deep but intuitive explanations of machine learning‚Ä¶
1009485671352950800,Wed Jun 20 17:18:34 +0000 2018,@iamaidang @hardmaru Thanks for sharing that observation! It's really interesting to hear what such deep experience‚Ä¶ https://t.co/C82AJQ60XE
1009484921679810600,Wed Jun 20 17:15:35 +0000 2018,It will be interesting to see what the trade-offs are. Is self-attention + a pointer-generator expressive enough to‚Ä¶ https://t.co/Vn9iZotKAd
1009484244060688400,Wed Jun 20 17:12:53 +0000 2018,The initial work on casting NLP tasks as QA (https://t.co/T1iioO5Hj1) was quite well received, but AFAIK most NLP t‚Ä¶ https://t.co/VyXFxITnBe
1009482838968856600,Wed Jun 20 17:07:18 +0000 2018,The Natural Language Decathlon: Great to see more research on multi-task / transfer learning coming out of Salesfor‚Ä¶ https://t.co/Y2jaubDML3
1009481230608425000,Wed Jun 20 17:00:55 +0000 2018,RT @parsaghaffari: We're looking for an amazing Data &amp; Search Engineer to reinforce our data aggregation and analysis pipelines, and help h‚Ä¶
1009014761969934300,Tue Jun 19 10:07:20 +0000 2018,@powooli @StoicTrader üòç
1008744943274295300,Mon Jun 18 16:15:10 +0000 2018,RT @yashuseth: Read the paper Universal Language Model Fine-Tuning for Text Classification by @jeremyphoward and @seb_ruder. Exciting times‚Ä¶
1007639458806751200,Fri Jun 15 15:02:22 +0000 2018,RT @riedelcastro: Interested in joining @UCL computer science as an assistant/associate professor in  #nlproc #MachineLearning? Get in touc‚Ä¶
1007382913627942900,Thu Jun 14 22:02:57 +0000 2018,RT @sleepinyourhat: Reading @seb_ruder and @barbara_plank's paper on domain shift. Great example of how to write an abstract for mixed posi‚Ä¶
1007302120721276900,Thu Jun 14 16:41:55 +0000 2018,@jacobandreas @MIT_CSAIL Congrats! üéâ
1007002944947085300,Wed Jun 13 20:53:05 +0000 2018,@winobes @lucie_nlp üëç
1006603747064152000,Tue Jun 12 18:26:49 +0000 2018,@Johannes_Welbl @riedelcastro üòÅüê®
1006595451439669200,Tue Jun 12 17:53:51 +0000 2018,@Johannes_Welbl @riedelcastro Congrats! Where will you be presenting the paper? :)
1006558654550724600,Tue Jun 12 15:27:38 +0000 2018,@vcheplygina @DrLukeOR Importantly, I don't suggest to put any position paper on arXiv. If it's an in-depth, well-a‚Ä¶ https://t.co/HGCMCidW0P
1006557147646320600,Tue Jun 12 15:21:39 +0000 2018,@vcheplygina @DrLukeOR I haven't heard any objections. The blog posts I put on arXiv were surveys, which people fou‚Ä¶ https://t.co/14RWA3QoFG
1006555542012289000,Tue Jun 12 15:15:16 +0000 2018,RT @_aylien: Check out @seb_ruder ' s blog on his week at  #NAACL2018 - Generalization, Test-of-time awards, &amp; Dialogue Systems https://t.c‚Ä¶
1006555488681635800,Tue Jun 12 15:15:04 +0000 2018,@verena_rieser @NAACLHLT @_aylien @alanathebot Thanks, Verena! I wasn't aware of this. I have updated article to reflect this.
1006535367317737500,Tue Jun 12 13:55:06 +0000 2018,My highlights of NAACL-HLT 2018: Generalization, Test-of-time, and Dialogue Systems @NAACLHLT @_aylien https://t.co/7FmYsCJn60
1006469699964735500,Tue Jun 12 09:34:10 +0000 2018,RT @_aylien: Really interesting new work from @openai on transfer learning for NLP, extends some great work by AYLIEN Research Scientist @s‚Ä¶
1006308045142528000,Mon Jun 11 22:51:48 +0000 2018,RT @jeremyphoward: This is exactly where we were hoping our ULMFit work would head - really great work from @OpenAI! üòä  If you're doing NLP‚Ä¶
1006306066924232700,Mon Jun 11 22:43:57 +0000 2018,RT @AlecRad: What I've been working on for the past year! https://t.co/CAQMYS1rR7  Inspired by CoVE, ELMo, and ULMFiT we show that a single‚Ä¶
1006242437659070500,Mon Jun 11 18:31:06 +0000 2018,Transfer learning with language models is getting hot! üî•New state-of-the-art results today by two different researc‚Ä¶ https://t.co/wmLCi53UV5
1006171444458479600,Mon Jun 11 13:49:00 +0000 2018,@deliprao Really glad you like it, Delip! Thanks for the pointer! Fixed! üôèüèª
1006141559178432500,Mon Jun 11 11:50:15 +0000 2018,@delliott @coastalcph Congrats! üéâ
1006137403873091600,Mon Jun 11 11:33:44 +0000 2018,@dries139 Thanks! Glad you like it!
1006117845758079000,Mon Jun 11 10:16:01 +0000 2018,@mElantkowski ‚ò∫Ô∏è
1006113884686684200,Mon Jun 11 10:00:17 +0000 2018,New NLP News - Conversational AI tutorial, RNNs for particle physics, InfoGAN, NLP Coursera, NLP book, killer robot‚Ä¶ https://t.co/c6SWVlQlFl
1005923012275818500,Sun Jun 10 21:21:49 +0000 2018,@manaalfar @NAACLHLT @yyaghoobzadeh @pymnpsbn
1005578614228836400,Sat Jun 09 22:33:19 +0000 2018,@ShubhamToshniw6 üëç
1005426349228642300,Sat Jun 09 12:28:16 +0000 2018,@ShubhamToshniw6 You're totally right. Not sure why I typed that. :-/
1005116087850106900,Fri Jun 08 15:55:24 +0000 2018,@jacobeisenstein This is so awesome! Thanks a lot for sharing this! :)
1005113500648304600,Fri Jun 08 15:45:07 +0000 2018,If you're a member of an underrepresented group in #NLProc or would like to support an initiative to broaden partic‚Ä¶ https://t.co/VZS75IVTci
1005110870706401300,Fri Jun 08 15:34:40 +0000 2018,@noecasas Hey Noe, thanks a lot for putting these notes together (for both EAMT and ICLR 2018). They are really use‚Ä¶ https://t.co/3eGPmrcHik
1005103254722707500,Fri Jun 08 15:04:24 +0000 2018,RT @IAugenstein: Slides of our #naacl2018 #NLProc paper (https://t.co/88FA4PdOX5) are available online: https://t.co/ioJrcfNs6B @seb_ruder‚Ä¶
1004738146033160200,Thu Jun 07 14:53:35 +0000 2018,DepthFirstLearning -- curriculums designed around foundational modern research papers with exercises and Colab note‚Ä¶ https://t.co/0UrmNMNgf6
1004405794937466900,Wed Jun 06 16:52:57 +0000 2018,Good thread on @barbara_plank's talk exploring cross-lingual character level models, multi task learning, and semi‚Ä¶ https://t.co/UxkfChAYZG
1004394807853281300,Wed Jun 06 16:09:17 +0000 2018,RT @nlpmattg: #nlphighlights 57: Now back after a bit of a break, @seb_ruder talks to us about his survey on cross-lingual word embeddings‚Ä¶
1004393739438248000,Wed Jun 06 16:05:02 +0000 2018,NLP ‚ù§Ô∏è Deep Learning: @barbara_plank at the #SCLeM workshop at @NAACLHLT https://t.co/mbRQOB5aIR
1004373037091819500,Wed Jun 06 14:42:47 +0000 2018,@chipro For traditional work, you can look at intros to recognizing textual entailment (RTE). Most of the work on n‚Ä¶ https://t.co/ESzPqmjA9l
1004346413193617400,Wed Jun 06 12:56:59 +0000 2018,@iamtrask @NAACLHLT Yep. üòÄ
1004182482932117500,Wed Jun 06 02:05:35 +0000 2018,@lousylinguist @NAACLHLT @chrmanning Chris had to get some extra energy to be the contrarian on the panel. :)
1004182197732028400,Wed Jun 06 02:04:27 +0000 2018,@InglouriousBkof @chrmanning I'm really glad it was helpful! Sorry that we didn't meet. :-/ Let me know when you're‚Ä¶ https://t.co/3TU5kp02fx
1004172228467068900,Wed Jun 06 01:24:50 +0000 2018,@Thom_Wolf üëå
1004134427100897300,Tue Jun 05 22:54:37 +0000 2018,RT @ybisk: Thank you @seb_ruder for documenting  (thread üëá) #GenDeep https://t.co/dahwXdnqVq
1004124954097483800,Tue Jun 05 22:16:59 +0000 2018,Fin.
1004124924175355900,Tue Jun 05 22:16:52 +0000 2018,Sam: Abstractiveness is used as measure in new Newsroom corpus (NAACL-HLT 2018).
1004124794177183700,Tue Jun 05 22:16:21 +0000 2018,Sam: Brittleness of models is not with regard to overfitting to genre. Q: How do we evaluate the abstractiveness of‚Ä¶ https://t.co/7aT4RZcj5l
1004124175924121600,Tue Jun 05 22:13:53 +0000 2018,Percy: Knowledge representations are quite different from current, task-oriented mentality. Worth investigating.  Q‚Ä¶ https://t.co/lsWVe4L4Ux
1004123594543288300,Tue Jun 05 22:11:35 +0000 2018,Q: Possible to have declarative representations to build NLU upon? Yejin: Possible. Shouldn't repeat what people ha‚Ä¶ https://t.co/z3XQSNCMTT
1004122507295764500,Tue Jun 05 22:07:16 +0000 2018,Percy: Numbers will look really low. Possibly tag examples with phenomenon/difficulty. Problem is that we don't hav‚Ä¶ https://t.co/WuJrDluLym
1004121892582813700,Tue Jun 05 22:04:49 +0000 2018,Still pretty far from language understanding. Q: Will pre-trained models will be used in all NLP tasks in the next‚Ä¶ https://t.co/CLg6L5tbw1
1004119742309859300,Tue Jun 05 21:56:16 +0000 2018,Sam: On SQuAD, performance drop even if students ask factoid questions (vs. MTurk workers in test data).  Chris: Hu‚Ä¶ https://t.co/cBXm7eY99l
1004118496979112000,Tue Jun 05 21:51:19 +0000 2018,Important to return to indigenous NLP tradition and ignore some things that ML has brought into NLP. Q: Other ways‚Ä¶ https://t.co/mXOvGSptMi
1004117373148909600,Tue Jun 05 21:46:52 +0000 2018,Requirement of i.i.d. data came through ML empiricism. Linguistic data is not i.i.d., any text is associated with o‚Ä¶ https://t.co/hA7SZd2Owl
1004117000392724500,Tue Jun 05 21:45:23 +0000 2018,@NAACLHLT @chrmanning Chris: Psychology is too obsessed with controlled experiments vs. more natural data in ML/NLP‚Ä¶ https://t.co/Mn89vcLUk3
1004116475219759100,Tue Jun 05 21:43:17 +0000 2018,@NAACLHLT @chrmanning Sam: Cannot break out of cycle of experimental design (RQ --&gt; experiments --&gt; results). We ca‚Ä¶ https://t.co/VCBjpjlpBz
1004115924570509300,Tue Jun 05 21:41:06 +0000 2018,@NAACLHLT @chrmanning Hard to rule out bias in general. With 100% accuracy, you would get everything correct anyway‚Ä¶ https://t.co/Xj8kuQUPKP
1004115220174917600,Tue Jun 05 21:38:18 +0000 2018,@NAACLHLT @chrmanning Yejin: Not clear whether to maintain natural distribution of data, balance and modify distrib‚Ä¶ https://t.co/E2RoojoL2O
1004114498221953000,Tue Jun 05 21:35:26 +0000 2018,@NAACLHLT @chrmanning Standard CS degree does not have a single lecture on how to design an experiment. In grad sch‚Ä¶ https://t.co/9puCMSQnMd
1004114145682362400,Tue Jun 05 21:34:02 +0000 2018,@NAACLHLT @chrmanning Hard to define tasks precisely enough to break out of cycle.  Yejin: Revisit how dataset is c‚Ä¶ https://t.co/KLhH9Rdgcf
1004113775899955200,Tue Jun 05 21:32:34 +0000 2018,@NAACLHLT @chrmanning Percy: Cycle of new paradigms, which throw out previous extensions and start from scratch (ru‚Ä¶ https://t.co/JFhdUNvwj8
1004113101346787300,Tue Jun 05 21:29:53 +0000 2018,@NAACLHLT @chrmanning Yejin: World models applicable to other parts, but for the moment, developing models for lang‚Ä¶ https://t.co/rlZ70M0Igo
1004112627742765000,Tue Jun 05 21:28:00 +0000 2018,@NAACLHLT @chrmanning A promising direction in this line: Memory networks, entity networks. Chris: Inductive bias i‚Ä¶ https://t.co/aTS2mLVVUK
1004112171356360700,Tue Jun 05 21:26:11 +0000 2018,@NAACLHLT @chrmanning Yejin Choi: Language specific inductive bias is necessary to push NLG. Inductive bias as arch‚Ä¶ https://t.co/lKcJ5hPxxM
1004111334483660800,Tue Jun 05 21:22:52 +0000 2018,@NAACLHLT @chrmanning: "We should have more inductive biases. We are clueless about how to add inductive biases so‚Ä¶ https://t.co/5bDQ2NWwUl
1004110419563286500,Tue Jun 05 21:19:14 +0000 2018,All-star panel at the generalization in deep learning workshop at @NAACLHLT #Deepgen2018 https://t.co/gbxuDDubP0
1004086882572042200,Tue Jun 05 19:45:42 +0000 2018,@InglouriousBkof I'll probably post something, but it's not gonna be as comprehensive as past summaries. I hope oth‚Ä¶ https://t.co/NeqVwL2ucp
1004043083028353000,Tue Jun 05 16:51:39 +0000 2018,It was super fun to chat with @nlpmattg and @waleed_ammar about cross-lingual word embeddings. The survey can be fo‚Ä¶ https://t.co/sqeCfTo9O2
1003717749858848800,Mon Jun 04 19:18:54 +0000 2018,RT @IAugenstein: I'll be presenting our #naacl2018 #NLProc paper on Multi-task Learning over Disparate Labels at 14:18 in Empire A https://‚Ä¶
1003687334678810600,Mon Jun 04 17:18:02 +0000 2018,@InglouriousBkof Me too! @IAugenstein is gonna give the talk.
1003577459172880400,Mon Jun 04 10:01:26 +0000 2018,New NLP News - ML Practica, I/O CodeLabs, All About NLP, Retro games, Dark side of AI, Computer vs. Doctors, Misspe‚Ä¶ https://t.co/syKi0C7Yf0
1003397075780997100,Sun Jun 03 22:04:39 +0000 2018,@pfrcks Yep.
1002961613631184900,Sat Jun 02 17:14:17 +0000 2018,RT @yuvalpi: Peyman Passban, Qun Liu, Andy Way (presented by @seb_ruder ): Improving Character-based Decoding Using Target-Side Morphologic‚Ä¶
1002106269254250500,Thu May 31 08:35:27 +0000 2018,@lotass_42 @jeremyphoward @karpathy @AndrewYNg @ylecun @math_rachel The self-driving car crash says more about the‚Ä¶ https://t.co/J2cgTiChJz
1002105759281352700,Thu May 31 08:33:25 +0000 2018,@lotass_42 @jeremyphoward @karpathy @AndrewYNg @ylecun @math_rachel While some arguments can be made that DL is ove‚Ä¶ https://t.co/wIrusGeUxP
1002096195127345200,Thu May 31 07:55:25 +0000 2018,@radekosmulski @fastdotai That's awesome! Huge congrats!
1001892204695183400,Wed May 30 18:24:50 +0000 2018,@prabhatjha That's awesome! Really glad those have been helpful to you. :)
1001889059931918300,Wed May 30 18:12:20 +0000 2018,RT @GuggerSylvain: Just finished adapting the QRNN (https://t.co/WXUNi86wpO) pytorch implementation of @smerity in the fastai library. Two‚Ä¶
1001572929765888000,Tue May 29 21:16:09 +0000 2018,@EmilioLapiello @jeremyphoward @fastdotai Thanks a lot, Emilio! That helps putting the results in context. :)
1001489770453852200,Tue May 29 15:45:42 +0000 2018,@damianborth @LernendeSysteme @snv_berlin @faznet @Georg_Schuette Is there any way we can help encourage that?
1001431752806723600,Tue May 29 11:55:10 +0000 2018,@damianborth @LernendeSysteme @snv_berlin @faznet @Georg_Schuette Any key takeaways?
1001101942955929600,Mon May 28 14:04:37 +0000 2018,RT @MatthewTeschke: Nice list of some useful NLP resources. If you want to stay up to date on NLP news, definitely skip to the bottom of th‚Ä¶
1001101899410657300,Mon May 28 14:04:27 +0000 2018,@MatthewTeschke Thanks for the shout-out, Matthew! :)
1001024261279100900,Mon May 28 08:55:56 +0000 2018,RT @_aylien: Interested in learning about #NLP? We compiled some of our favorite (free) NLP &amp; #machinelearningeducational resources here: h‚Ä¶
1000743715466555400,Sun May 27 14:21:09 +0000 2018,@lnsmith613 @zachlandes Yes, that explains why fine-tuning &gt; using fixed features; do you have any intuition why Re‚Ä¶ https://t.co/uzWroWHQ0T
1000743181015814100,Sun May 27 14:19:01 +0000 2018,@sajalkaushik17 @fchollet Popily (https://t.co/Q0qn1VaaJb) did something like that. They pivoted to focus on fake n‚Ä¶ https://t.co/U1orPXmuNK
1000337933726048300,Sat May 26 11:28:43 +0000 2018,@williamleif @ryan_t_lowe Congrats! Well done!
1000331008049336300,Sat May 26 11:01:12 +0000 2018,@EmilioLapiello @jeremyphoward @fastdotai Hey Emilio, thanks for writing about this! I think that's a super importa‚Ä¶ https://t.co/KchAxwaBb8
1000328958750220300,Sat May 26 10:53:03 +0000 2018,@irhumshafkat @PyTorch Well done, Irhum!
1000328320989450200,Sat May 26 10:50:31 +0000 2018,Can't find a download link; here's a website to scrape: https://t.co/8Zq8rd5qsb
1000327950749917200,Sat May 26 10:49:03 +0000 2018,This looks like a fun dataset to play with: A cross-lingual corpus of Pok√©mon names to study sound symbolism across‚Ä¶ https://t.co/6xsLcepWfp
1000143434144014300,Fri May 25 22:35:51 +0000 2018,@IntelAIDev open-sources NLP Architect, a dedicated Python library for Deep Learning for NLP. At first glance, look‚Ä¶ https://t.co/chBnwOftbD
1000139516097761300,Fri May 25 22:20:16 +0000 2018,@G_Auss That's fantastic! Thanks, Xiang! :)
1000139014404427800,Fri May 25 22:18:17 +0000 2018,If you don't want to read the whole paper, skip to Section 5, which contains lots of fun examples: e.g. Germans use‚Ä¶ https://t.co/krs8EnGK4l
1000137890905673700,Fri May 25 22:13:49 +0000 2018,As a non-native English speaker, I'm always interested in studies that seek to better understand l2 acquisition. Th‚Ä¶ https://t.co/f8W0uD7Ced
1000134148856664000,Fri May 25 21:58:57 +0000 2018,@zachlandes I think it just shows that training on fixed features can't leverage all the information that has been‚Ä¶ https://t.co/7I3XMHCH4N
1000074082317226000,Fri May 25 18:00:16 +0000 2018,@tuzhucheng @G_Auss üëç
1000059725218795500,Fri May 25 17:03:13 +0000 2018,@tuzhucheng @G_Auss That'd be awesome and super useful!! :)
1000032147539943400,Fri May 25 15:13:38 +0000 2018,Fine-tuning is also always better than training on fixed features. By fine-tuning pre-trained ImageNet models, the‚Ä¶ https://t.co/27Bt7CVAL0
1000030284484694000,Fri May 25 15:06:14 +0000 2018,This is a super useful paper that we need more of: Better ImageNet models are not necessarily better feature extrac‚Ä¶ https://t.co/OPklY09zd0
999966274942853100,Fri May 25 10:51:53 +0000 2018,@tuzhucheng Thanks, Michael! The problem with that corpus is that it's already transliterated to pinyin, so it does‚Ä¶ https://t.co/1zgcgXv2jU
999963045072748500,Fri May 25 10:39:03 +0000 2018,@stjaco @zehavoc @GuggerSylvain @eldams @EVALITAcampaign Thanks for the reference!
999909440512966700,Fri May 25 07:06:02 +0000 2018,RT @emnlp2018: #emnlp2018 received a record-breaking 2,137 valid submissions, a 46% increase over EMNLP 2017! Thanks to everyone for submit‚Ä¶
999733537808011300,Thu May 24 19:27:04 +0000 2018,Nice article on how to use transfer learning to classify the room type (living room, bedroom, etc.) of Airbnb listi‚Ä¶ https://t.co/yw4qhD26zK
999722220111974400,Thu May 24 18:42:05 +0000 2018,@michaelcapizzi @jeremyphoward Hey Michael, it seems like your model is not learning anything (50% acc is random on‚Ä¶ https://t.co/z9FU3aJuQs
999645386456354800,Thu May 24 13:36:47 +0000 2018,#NLProc peeps, are any of you aware of benchmark text classification datasets for non-English languages that have b‚Ä¶ https://t.co/6kjDJrAzLj
999302864416714800,Wed May 23 14:55:43 +0000 2018,Kaggle's 2nd video understanding competition challenges you to learn a model &lt; 1 GB. Will be interesting to see the‚Ä¶ https://t.co/O7IeaJQRln
999197910754582500,Wed May 23 07:58:40 +0000 2018,@tallinzen @_shrdlu_ üëèüèª
999051319259353100,Tue May 22 22:16:10 +0000 2018,@radevd This looks great! Can't wait to play around with it. :)
999050505388249100,Tue May 22 22:12:56 +0000 2018,RT @radevd: Learn NLP the smart way, from the best.  AAN ("All About NLP"), the search engine for NLP tutorials, is out now, after years of‚Ä¶
998890701835432000,Tue May 22 11:37:56 +0000 2018,@vlachos_nlp Totally agree! Found that interview super interesting and full of useful tidbits.
998881397099188200,Tue May 22 11:00:58 +0000 2018,RT @TechCrunch: Uizard raises funds for its AI that turns design mockups into source code https://t.co/PQDRZx1rYT by @mikebutcher https://t‚Ä¶
998833088821170200,Tue May 22 07:49:00 +0000 2018,@gyrodiot @GoAbiAryan @hardmaru @Miles_Brundage @MIRIBerkeley Thanks for those references, J√©r√©my! I'm not followin‚Ä¶ https://t.co/0jogmRTwqe
998627529211744300,Mon May 21 18:12:11 +0000 2018,Slides for my keynote this morning on Successes and Frontiers of Deep Learning are now online. Mainly a high-level‚Ä¶ https://t.co/WRwxxDnBEr
998594314690285600,Mon May 21 16:00:12 +0000 2018,@alexip Thank you, Alexis!
998567839756931100,Mon May 21 14:15:00 +0000 2018,New NLP News - https://t.co/L1Y9bxoWL0; Google IO; Semantic segmentation, object detection, network graph overviews‚Ä¶ https://t.co/10QivkrzqX
998501833814560800,Mon May 21 09:52:43 +0000 2018,RT @kcimc: apparently all the westworld hosts are running @tensorflow in node? https://t.co/Gt6dY8wLUI
998496792969928700,Mon May 21 09:32:41 +0000 2018,RT @DocXavi: Better and faster text classification models by transfer learning presented by Sebastian Ruder @seb_ruder from @insight_centre‚Ä¶
998496781762748400,Mon May 21 09:32:38 +0000 2018,RT @oconnorn: @seb_ruder kicking off the @insight_centre Deep Learning workshop in @DublinCityUni explaining recent advances in machine tra‚Ä¶
998468092744425500,Mon May 21 07:38:38 +0000 2018,Rules of Machine Learning: Best Practices for ML Engineering. Based on the Rules of ML pdf file (‚Ä¶ https://t.co/k4cIe4lfzY
998455463195889700,Mon May 21 06:48:27 +0000 2018,@johnbreslin Thanks, John! :)
998170153400586200,Sun May 20 11:54:44 +0000 2018,RT @DocXavi: In Dublin and interesting in #deeplearning ? Join this Monday our 1.5 day tutorial @insight_centre @DublinCityUni, we've been‚Ä¶
997803280834138100,Sat May 19 11:36:55 +0000 2018,@davidandrzej Do you mean generating memes or analyzing existing ones from a digital humanities perspective? For 1)‚Ä¶ https://t.co/biVflvStJU
997600045015978000,Fri May 18 22:09:19 +0000 2018,@sannikpatel @_aylien @fastdotai @jeremyphoward It's already available here: https://t.co/f0uJPW2pHX
997221126056153100,Thu May 17 21:03:38 +0000 2018,@apiltamang Haha, yeah. That photo was a bit back. :)
997169718006984700,Thu May 17 17:39:21 +0000 2018,@eagirre Very good point. It's really interesting that seed lexicons don't seem that useful. Are we just bad at cre‚Ä¶ https://t.co/F8SJqDpCWR
997167462675345400,Thu May 17 17:30:24 +0000 2018,I'll be giving a keynote at the Insight@DCU Deep Learning Workshop on Mon, May 21 in Dublin. Mostly high-level abou‚Ä¶ https://t.co/1Z2Yj5d0MO
997116886394589200,Thu May 17 14:09:25 +0000 2018,@ZoeyC17 This is great, Zoey! Thanks a lot for writing this. :)
997116817020727300,Thu May 17 14:09:09 +0000 2018,RT @ZoeyC17: I am very proud to publish my first blog post about deep learning in object detection, which I cover the evolution of importan‚Ä¶
997110692015689700,Thu May 17 13:44:49 +0000 2018,RT @_aylien: Research posted on Tuesday by AYLIEN research scientist @seb_ruder and @fastdotai founder @jeremyphoward on Universal Language‚Ä¶
997099672660475900,Thu May 17 13:01:01 +0000 2018,@moreymat Yep, just realized. :) Thanks for reading it so attentively!
997095949779505200,Thu May 17 12:46:14 +0000 2018,@moreymat You're totally right! Thanks for spotting that. That error somehow made it from the preprint to  the final version. Will fix it.
997008902607786000,Thu May 17 07:00:20 +0000 2018,RT @artetxem: Check out our @acl2018 paper on "A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddin‚Ä¶
996816052716736500,Wed May 16 18:14:01 +0000 2018,RT @jeremyphoward: If you're looking for an interesting collaborative research project in computer vision, here's one that I think could be‚Ä¶
996774519149924400,Wed May 16 15:28:59 +0000 2018,@PhilippBayer @jeremyphoward @gedankenstuecke This is awesome! Thank you for compiling that, Philipp!
996702312751091700,Wed May 16 10:42:03 +0000 2018,@PhilippBayer @jeremyphoward @gedankenstuecke üòç
996680058147393500,Wed May 16 09:13:37 +0000 2018,@PhilippBayer @jeremyphoward @gedankenstuecke Would love to chat about this! Are there other papers / posts you'd r‚Ä¶ https://t.co/5w8Ua2aAsj
996487635953705000,Tue May 15 20:29:00 +0000 2018,@ben_trevett Thanks! üòä
996486286478278700,Tue May 15 20:23:39 +0000 2018,@ben_trevett Darn. Thanks for noticing. That should be a 5.57. Not sure how that slipped through. We'll fix that an‚Ä¶ https://t.co/AbrFMsm6xt
996485511685517300,Tue May 15 20:20:34 +0000 2018,@sandstep1 @jeremyphoward Hey, we're aware of the paper. Not sure how relevant that is here as we use standard spli‚Ä¶ https://t.co/sohcYip4iT
996477244733378600,Tue May 15 19:47:43 +0000 2018,RT @jeremyphoward: @drsxr @seb_ruder Nor was I until very recently. It's a good time to become an NLP guy - and extremely important in radi‚Ä¶
996476000153239600,Tue May 15 19:42:46 +0000 2018,@sandstep1 @jeremyphoward In the beginning of the post. Or here https://t.co/evuqDyHU4v
996459565796548600,Tue May 15 18:37:28 +0000 2018,@karthiktsaliki @fastdotai @jeremyphoward The final draft is online. We updated it today. üòä https://t.co/evuqDyHU4v
996454576176992300,Tue May 15 18:17:38 +0000 2018,Really excited that our @acl2018 paper on Universal Language Model Fine-tuning for Text Classification is now onlin‚Ä¶ https://t.co/z2wFdSgpch
996452851143540700,Tue May 15 18:10:47 +0000 2018,RT @math_rachel: Transfer learning has already led to huge advances in computer vision. Now @jeremyphoward &amp; @seb_ruder have applied transf‚Ä¶
996452829542912000,Tue May 15 18:10:42 +0000 2018,RT @jeremyphoward: New work with @seb_ruder: "Universal Language Model Fine-tuning for Text Classification", with code and pre-trained mode‚Ä¶
995983069025833000,Mon May 14 11:04:02 +0000 2018,@NalKalchbrenner Congrats! Really important contributions! :)
995784178502393900,Sun May 13 21:53:43 +0000 2018,@vnfrombucharest Congrats!
995611043614330900,Sun May 13 10:25:45 +0000 2018,RT @asmeaton: We ran this 2-day workshop on machine learning last year we had to cap registrations at 175 as that was the room limit, with‚Ä¶
995310990723027000,Sat May 12 14:33:26 +0000 2018,@yoavgo @gail_w @yahave @DannyKarmon @zdanielz Congrats! :)
995292349898031100,Sat May 12 13:19:22 +0000 2018,@nikos3388 Thanks for the mention! Glad you like it. :)
994877689919168500,Fri May 11 09:51:39 +0000 2018,@CamachoCollados @tpilehvar Congrats (to the survey and your sons)! Looking forward to reading the survey. :)
994877482548584400,Fri May 11 09:50:50 +0000 2018,RT @CamachoCollados: After many months working on it, including the birth of our respective first  sons in-between :), @tpilehvar and I are‚Ä¶
994637358770327600,Thu May 10 17:56:40 +0000 2018,@artetxem Awesome! Looking forward to reading your paper! :)
994542073993924600,Thu May 10 11:38:02 +0000 2018,@Shahnawazgrewal @emnlp2018 Well, you can send me a draft if it's related to my research areas (transfer learning,‚Ä¶ https://t.co/ylGIM4xl4z
994541172793200600,Thu May 10 11:34:27 +0000 2018,@Shahnawazgrewal Thanks! üòä
994493726138519600,Thu May 10 08:25:55 +0000 2018,RT @parsaghaffari: Stance detection is an important tool for highlighting bias in media, and reflecting it to consumers. Read about how we‚Ä¶
994493726138519600,Thu May 10 08:25:55 +0000 2018,RT @parsaghaffari: Stance detection is an important tool for highlighting bias in media, and reflecting it to consumers. Read about how we‚Ä¶
994493636388819000,Thu May 10 08:25:34 +0000 2018,Instead of going completely unsupervised, we can leverage identically spelled words as weakly supervised seeds, whi‚Ä¶ https://t.co/Ps5Y9AH8ya
994492978793197600,Thu May 10 08:22:57 +0000 2018,@acl2018 @licwu We analyze the limitations of current unsupervised x-lingual embedding models, which are one of the‚Ä¶ https://t.co/5fbPbGEnP4
994492492635615200,Thu May 10 08:21:01 +0000 2018,Our @acl2018 paper  On the Limitations of Unsupervised Bilingual Dictionary Induction with Anders S√∏gaard and‚Ä¶ https://t.co/fRAbSigGSF
994234338421215200,Wed May 09 15:15:12 +0000 2018,@sly_clank @scholarcy Thanks! :)
994209790359474200,Wed May 09 13:37:40 +0000 2018,@sly_clank @scholarcy No, haven't heard of her. The article looks really interesting! Do you have a link to the vis‚Ä¶ https://t.co/TawydnjLQL
994179907038187500,Wed May 09 11:38:55 +0000 2018,@sly_clank @scholarcy Yep, I'd expect a similar amount. I wonder how they determine when a human should step in; co‚Ä¶ https://t.co/Ud1biDoz8f
994167411162173400,Wed May 09 10:49:16 +0000 2018,@sly_clank @scholarcy For each domain, you still need to collect lots of real-world training data, which costs a lo‚Ä¶ https://t.co/zrnuTZeHQj
994143400118181900,Wed May 09 09:13:51 +0000 2018,@theolivenbaum Yep, totally agree. I don't think an RNN would be able to handle the entire conversation.
994138375425810400,Wed May 09 08:53:53 +0000 2018,@theolivenbaum Yeah. I'm also still a bit unsure how the model looks like: From the blog post, it seems that they u‚Ä¶ https://t.co/P5wu2cMyd3
994137627908599800,Wed May 09 08:50:55 +0000 2018,@scholarcy I guess there's still a large number of restaurants that don't do online booking, so in order to be able‚Ä¶ https://t.co/t1qZzjVNNp
994129562090004500,Wed May 09 08:18:52 +0000 2018,Really like the focus on narrow domains with Google Duplex. That's where ML/NLP can make the most difference in the‚Ä¶ https://t.co/8smosSefdo
994125901452496900,Wed May 09 08:04:19 +0000 2018,@omerlevy_ Is evaluation on diverse datasets a good way to test generalization? How can we standardize which datase‚Ä¶ https://t.co/ESZ6oAYGq8
993955557504667600,Tue May 08 20:47:26 +0000 2018,@JaeDukSeo Thank you, Jae! Great to see your implementations. :)
993911123224203300,Tue May 08 17:50:52 +0000 2018,RT @GuggerSylvain: Finished my implementation of the Deep Painterly Harmonization paper. Blog post is here: https://t.co/22wlmLuRYd  And no‚Ä¶
993826600645951500,Tue May 08 12:15:00 +0000 2018,RT @TheShubhanshu: Love this table from @seb_ruder and @barbara_plank 's paper on identifying model performance on out of vocab (OOV) and U‚Ä¶
993805181925158900,Tue May 08 10:49:54 +0000 2018,@TheShubhanshu @barbara_plank Thanks for the shout-out! :)
993743973331230700,Tue May 08 06:46:40 +0000 2018,@pmbaumgartner This is the previous version. We'll put the updated paper on arXiv at the end of this week.
993430312892977200,Mon May 07 10:00:18 +0000 2018,New NLP News: Intrinsic dimension video; Ilya Sutskever meta-learning; ICLR 2018 presentations; Pointer Sentinel Mi‚Ä¶ https://t.co/p2dz0UHtgC
992458283846905900,Fri May 04 17:37:48 +0000 2018,@gneubig @stanfordnlp Great paper! Really like that you've re-implemented (and improved) BiAF and that you're evaluating on 20 languages! :)
992456775482904600,Fri May 04 17:31:48 +0000 2018,@indicoData Cool! Will you make available the slides?
992032874823110700,Thu May 03 13:27:23 +0000 2018,@NLPatITU @barbara_plank @ITUkbh üëèüèª
991782449284776000,Wed May 02 20:52:16 +0000 2018,@captn_head Maybe a bit tongue-in-cheek. üòâ Though I think just having better pretraining tasks potentially has huge trickle-down effects.
991759604467028000,Wed May 02 19:21:30 +0000 2018,We know that transfer learning from a large dataset such as ImageNet is successful. Turns out if we pre-train on an‚Ä¶ https://t.co/D5FQ08wZEq
991725694496444400,Wed May 02 17:06:45 +0000 2018,RT @jeremyphoward: Training Imagenet in 3 hours for $25; and CIFAR10 for $0.26 ¬∑ https://t.co/GEOZuodrZj https://t.co/NkeUAUZ7Lk
991699924340396000,Wed May 02 15:24:21 +0000 2018,RT @socialsemantics: Researcher @seb_ruder @socialsemantics @insight_centre @nuigalway and @_aylien will give an invited talk at this forth‚Ä¶
991328125941370900,Tue May 01 14:46:57 +0000 2018,@cnancyxu @gradientpub Great initiative! Looking forward to reading and helping where I can.
991327821715988500,Tue May 01 14:45:45 +0000 2018,RT @cnancyxu: so @gradientpub launched today üéâ  Weekly publications on cutting-edge topics in #AI by academic and industry researchers. Wou‚Ä¶
991064435098898400,Mon Apr 30 21:19:09 +0000 2018,@feedmari EMNLP is in Brussels this year, so should be worthwhile. More ML-related: ICML is in Stockholm. Have a lo‚Ä¶ https://t.co/bguAfrWd0A
990947093404667900,Mon Apr 30 13:32:52 +0000 2018,The amazing thing about machine learning is the openness of the community, the venues, and the publications. Anyone‚Ä¶ https://t.co/wHUnteETW0
990944873560924200,Mon Apr 30 13:24:03 +0000 2018,Active Learning: Optimization != Improvement - insightful blog post about the pitfalls of Active Learning, when it‚Ä¶ https://t.co/Nx0PANtXqy
990548334988361700,Sun Apr 29 11:08:21 +0000 2018,@Tim_Dettmers That's awesome news, Tim!! Congrats!!
989611973066350600,Thu Apr 26 21:07:35 +0000 2018,@shrimai_ @acl2018 @rsalakhu Congrats! Great to see more work on this challenging task! :)
989610731166462000,Thu Apr 26 21:02:39 +0000 2018,RT @mElantkowski: It's a nice feeling when you run your implementation of a particular paper, and it just works with the first try.  Pointe‚Ä¶
989593820731007000,Thu Apr 26 19:55:27 +0000 2018,RT @sgouws: Get those applications in! :) https://t.co/lk3k7mMoLQ
989527729916833800,Thu Apr 26 15:32:50 +0000 2018,@rushoislam Thanks, Rusho! :)
989527197831565300,Thu Apr 26 15:30:43 +0000 2018,@lopusz Thank you, Micha≈Ç! Glad you found it helpful! :)
989487385045012500,Thu Apr 26 12:52:31 +0000 2018,@gyrodiot Awesome! Really happy to hear that! :)
989472415200809000,Thu Apr 26 11:53:01 +0000 2018,There's been an increasing interest in semi-supervised learning (SSL) algorithms and some cool results recently. In‚Ä¶ https://t.co/wtRvKayquY
989435162965545000,Thu Apr 26 09:25:00 +0000 2018,New blog post: An overview of proxy-label approaches for semi-supervised learning https://t.co/Ks7aexcdwu
989417353497759700,Thu Apr 26 08:14:14 +0000 2018,@sridatta @jeremyphoward We're working on releasing it. :)
989416257580564500,Thu Apr 26 08:09:52 +0000 2018,Strong Baselines for Neural Semi-supervised Learning under Domain Shift with @barbara_plank is now on arXiv‚Ä¶ https://t.co/zlpJr1rki6
989075908060504000,Wed Apr 25 09:37:27 +0000 2018,RT @coastalcph: @CoastalCPH is looking for PhD applicants with a &lt;3 for reinforcement learning and #NLProc to work on project $'ed by @goog‚Ä¶
989030994794643500,Wed Apr 25 06:38:59 +0000 2018,@chausler @barbara_plank @jeremyphoward @licwu @acl2018 Yep! üòä
988777324144406500,Tue Apr 24 13:50:59 +0000 2018,@elikiper Congrats, Eli and Miguel! Nice work! üëèüèªI've played around in the past with adjusting the sampling probabi‚Ä¶ https://t.co/SbxYCgOLnE
988703382968918000,Tue Apr 24 08:57:10 +0000 2018,@lili_vs Thank you, Lilia!
988692140170645500,Tue Apr 24 08:12:29 +0000 2018,RT @iamtrask: The difference between one optimizer vs another (SGD vs Adam?) can be pretty vague. @seb_ruder does a nice job unpacking them‚Ä¶
988678832906481700,Tue Apr 24 07:19:37 +0000 2018,RT @lawrennd: Call for European lab in AI. https://t.co/COJIyPtIuV
988422134841167900,Mon Apr 23 14:19:35 +0000 2018,@NirantK @ProjectJupyter Thanks for doing such great work!
988412880860143600,Mon Apr 23 13:42:49 +0000 2018,RT @mohitban47: Thanks @seb_ruder for the paper-pick shout-outs to two of our @NAACLHLT papers by students @ramakanth1729 and @SwetaKarleka‚Ä¶
988405272174571500,Mon Apr 23 13:12:35 +0000 2018,@ujjawalpathak2 Thanks, Kumar! Really appreciate it! üòä
988380158250897400,Mon Apr 23 11:32:47 +0000 2018,RT @YannFletBerliac: Ready! Thanks @seb_ruder https://t.co/MoqYAUCv43
988380087983755300,Mon Apr 23 11:32:30 +0000 2018,@YannFletBerliac üòç
988361125292052500,Mon Apr 23 10:17:09 +0000 2018,@roopalgarg Thanks! üôèüèª
988356883890262000,Mon Apr 23 10:00:18 +0000 2018,New NLP Newsletter: NLP PyTorch libraries; GAN tutorial; Jupyter tricks; TensorFlow things; Representation Learning‚Ä¶ https://t.co/J5wgkdevM5
987759210946596900,Sat Apr 21 18:25:22 +0000 2018,@jacobandreas Is it just the reviewing or are you also tired of reading reading comprehension papers? ;)
987650241762603000,Sat Apr 21 11:12:21 +0000 2018,@sleepinyourhat The paper is long and detailed enough as it is, though, so you don't necessarily need to include th‚Ä¶ https://t.co/pNBhDDJJ2b
987649918893375500,Sat Apr 21 11:11:04 +0000 2018,@sleepinyourhat Often, training on all tasks at the same time gives worse performance than training on two tasks th‚Ä¶ https://t.co/7jfgGfmTER
987649623387988000,Sat Apr 21 11:09:54 +0000 2018,@sleepinyourhat Great work! It's great to see an increased focus on generalization across NLP tasks. Another intere‚Ä¶ https://t.co/pp9SYZ1t9M
987638570520399900,Sat Apr 21 10:25:59 +0000 2018,@licwu @acl2018 Wow! Congrats, Ivan! üëèüèª
987628493008666600,Sat Apr 21 09:45:56 +0000 2018,@Thom_Wolf @acl2018 @julien_c @ClementDelangue @SanhEstPasMoi Nice! Congrats! :) See you there!
987442147422036000,Fri Apr 20 21:25:28 +0000 2018,@nicklovescode @barbara_plank @jeremyphoward @licwu @acl2018 Not yet. We'll post soon (as well as release the code)‚Ä¶ https://t.co/6tHPi6Cljh
987439247228588000,Fri Apr 20 21:13:56 +0000 2018,@barbara_plank @jeremyphoward @licwu @acl2018 On the Limitations of Unsupervised Bilingual Dictionary Induction wit‚Ä¶ https://t.co/OfLBBEBq96
987439024972423200,Fri Apr 20 21:13:03 +0000 2018,@barbara_plank @jeremyphoward @licwu @acl2018 Universal Language Model Fine-tuning for Text Classification with‚Ä¶ https://t.co/0z3aRDgo4z
987438900158369800,Fri Apr 20 21:12:34 +0000 2018,@barbara_plank @jeremyphoward @licwu @acl2018 Strong Baselines for Neural Semi-supervised Learning under Domain Shi‚Ä¶ https://t.co/RC9guQEwTt
987438708365451300,Fri Apr 20 21:11:48 +0000 2018,Yay!! 3 papers with awesome collaborators accepted to ACL! Thanks to @barbara_plank, @jeremyphoward, Anders S√∏gaard‚Ä¶ https://t.co/8TPk2bmOEh
987367933910437900,Fri Apr 20 16:30:34 +0000 2018,@sleepinyourhat Nice! Looking forward to reading. :)
987279572365439000,Fri Apr 20 10:39:27 +0000 2018,RT @_aylien: We're growing again! We're looking for a talented Dev Lead to join our team in sunny Dublin - if you know anyone suitable, ple‚Ä¶
987062174131552300,Thu Apr 19 20:15:35 +0000 2018,RT @TensorFlow: Solve text classification tasks using custom TensorFlow estimators. @seb_ruder &amp; @eisenjulian use estimators to identify po‚Ä¶
987023318825029600,Thu Apr 19 17:41:11 +0000 2018,RT @jeremyphoward: Why yes, we did just train CIFAR10 ~2000% faster than the previous best on DAWNBench, using fastai and @pytorch. :) Blog‚Ä¶
986644383394132000,Wed Apr 18 16:35:26 +0000 2018,RT @gdb: Important challenge in ML: generalizing outside the training distribution. One potential approach: https://t.co/Jv8qdInvBf
986627268708532200,Wed Apr 18 15:27:26 +0000 2018,@lordOFawkward Some words, like common nouns occur very often in texts. For these, we would like to perform smaller‚Ä¶ https://t.co/mc4AGK6nSc
985977274154602500,Mon Apr 16 20:24:35 +0000 2018,@Smerity @chrisjrn @MetaMindIO @salesforce Wow! That's big news! Good luck!
985961553412403200,Mon Apr 16 19:22:07 +0000 2018,@varish @jeremyphoward We've been waiting for a conference notification with the paper, so did not manage to releas‚Ä¶ https://t.co/mLjVT88dje
985960082520277000,Mon Apr 16 19:16:16 +0000 2018,@EmilStenstrom @eisenjulian Should be fixed now. Let me know if they work for you.
985957201889153000,Mon Apr 16 19:04:49 +0000 2018,@EmilStenstrom @eisenjulian Argh. Thanks for pointing that out. Will fix shortly.
985951206945079300,Mon Apr 16 18:41:00 +0000 2018,New blog post: Text Classification with TensorFlow Estimators https://t.co/mbGIwjNYcF with @eisenjulian
985863055304163300,Mon Apr 16 12:50:43 +0000 2018,RT @Tbeltramelli: Join our team! We are hiring ML and CV engineers and interns ü§ñ https://t.co/IQPGDDrYIY https://t.co/ifMDRPLPmj
985116183329493000,Sat Apr 14 11:22:55 +0000 2018,The Search for AlphaMystica: Interesting post about the challenges of applying AlphaGo to board games with more com‚Ä¶ https://t.co/AdsDLv0gh0
984755281287663600,Fri Apr 13 11:28:49 +0000 2018,RT @sgouws: Only a few weeks left to apply for this year's Deep Learning Indaba in September. Come for a jam-packed week of networking, lea‚Ä¶
984350679123849200,Thu Apr 12 08:41:04 +0000 2018,@NAACLHLT Congrats @mattthemathman, @MarkNeumannnn, @MohitIyyer, @nlpmattg! Well deserved! :)
983697266744725500,Tue Apr 10 13:24:39 +0000 2018,@NirantK @apte_dhruv You already have some common models listed throughout the list, but having them in one place i‚Ä¶ https://t.co/b3UAdD6rCT
983697013014433800,Tue Apr 10 13:23:38 +0000 2018,@NirantK @apte_dhruv Thanks! :) I'm not entirely sure myself. I had in mind maybe having a Table with links to prot‚Ä¶ https://t.co/pGldLtBkiN
983624087112167400,Tue Apr 10 08:33:51 +0000 2018,@NirantK @apte_dhruv It'd be super useful to have links to implementations of popular models in Tensorflow and Pyto‚Ä¶ https://t.co/fQ66t8bAgD
983622987734831100,Tue Apr 10 08:29:29 +0000 2018,@NirantK @apte_dhruv Some references are more relevant than others, e.g. in Research and Review Articles,  "Online‚Ä¶ https://t.co/aHeWxpmofI
983308388816818200,Mon Apr 09 11:39:23 +0000 2018,RT @Thom_Wolf: @seb_ruder's latest newsletter is out w/ more visual "Paper Picks" including our animated Meta-Learning intro and @hardmaru'‚Ä¶
983283449317519400,Mon Apr 09 10:00:17 +0000 2018,NLP News Issue #20 üéâ: Tensorflow DevSummit 2018; Annotated Transformer; TF-Hub; CS224n reports; Awesome-NLP; Meta-l‚Ä¶ https://t.co/HW8T73Sqnh
982947085623414800,Sun Apr 08 11:43:42 +0000 2018,@ovrdr Yeah, speech also. Also the Merck competition in 2012. Just mentioned CV for brevity and because AlexNet arg‚Ä¶ https://t.co/kCjZR3jWSb
982017468498620400,Thu Apr 05 22:09:44 +0000 2018,RT @deliprao: @mostafabenh @math_rachel @jeremyphoward Interesting problems are everywhere. If you are interested in NLP, see this list by‚Ä¶
982007235126710300,Thu Apr 05 21:29:04 +0000 2018,Retro Contest: A new benchmark &amp; competition on transfer learning for RL by @OpenAI. Great to see new benchmarks th‚Ä¶ https://t.co/lCM1wf46uH
982005816378232800,Thu Apr 05 21:23:26 +0000 2018,@mostafabenh Fair point. I guess curation might be more useful in the beginning to get to a critical mass of compel‚Ä¶ https://t.co/uiI1Yraehv
982001253671669800,Thu Apr 05 21:05:18 +0000 2018,@mostafabenh Maybe you can also take a page out of the playbook of https://t.co/kwK8xjCpLF in terms of presentation of projects.
982000912490262500,Thu Apr 05 21:03:56 +0000 2018,@mostafabenh Very cool! Not sure if a blockchain is totally necessary, but totally like the idea of crowd-sourcing‚Ä¶ https://t.co/qj6vX3NK11
981851614465142800,Thu Apr 05 11:10:41 +0000 2018,@dennybritz Sounds also like a good fit for a German beer garden. üçª
981631851667353600,Wed Apr 04 20:37:25 +0000 2018,@joshmeyerphd I've tried different weights in initial experiments, but hadn't seen too much of a difference. Since‚Ä¶ https://t.co/9wZgvoLoPt
981573911115698200,Wed Apr 04 16:47:11 +0000 2018,@joshmeyerphd Here are two recent papers, which come up with new ways to weight the task losses:‚Ä¶ https://t.co/UqFwh3Gmnk
981555216070987800,Wed Apr 04 15:32:54 +0000 2018,@ankur_rice @anotherjohng @afshinmeh @parsaghaffari @_aylien Hi Ankur, we determine the stance of the author of a n‚Ä¶ https://t.co/ZpEgVD4IAL
981452959665066000,Wed Apr 04 08:46:34 +0000 2018,RT @Thom_Wolf: To introduce the work we'll be presenting at ICLR, I tried to draft the most visual &amp; intuitive introduction to Meta-Learnin‚Ä¶
981452613710475300,Wed Apr 04 08:45:12 +0000 2018,RT @parsaghaffari: Our demo paper on Stance Detection in news has been accepted to NAACL-HLT 2018. We explore automatic detection of a jour‚Ä¶
981446214016290800,Wed Apr 04 08:19:46 +0000 2018,@Thom_Wolf @anotherjohng @afshinmeh @parsaghaffari @_aylien Thanks for the note, Thomas! We'll do that. :)
981440658811641900,Wed Apr 04 07:57:42 +0000 2018,Our demo paper on 360¬∞ Stance Detection, using stance detection to provide different perspectives on controversial‚Ä¶ https://t.co/LFypU8cOLr
981180883049877500,Tue Apr 03 14:45:26 +0000 2018,@cadarsh88 There's definitely a balance. Most days I don't really tweet much. :)
981175926053658600,Tue Apr 03 14:25:44 +0000 2018,It has been great and tremendously stimulating to read the thoughts of so many ML and NLP practitioners. I really a‚Ä¶ https://t.co/LRR5F4v1oy
981148638838689800,Tue Apr 03 12:37:19 +0000 2018,@joabingel Good luck, Joachim! :)
981124930145259500,Tue Apr 03 11:03:06 +0000 2018,RT @_rockt: I have the hope this gets @AngelaMerkeICDU @BMBF_Bund @BMWi_Bund thinking beyond digitalization and work on a long-term artific‚Ä¶
980400306268835800,Sun Apr 01 11:03:42 +0000 2018,@joshmeyerphd @ylecun @JeffDean Don't think that's true. In NLP, we've had dense representations (LSA, PCA, etc.) f‚Ä¶ https://t.co/awSaCKvn1t
980216723122028500,Sat Mar 31 22:54:12 +0000 2018,@earnmyturns üòÄ Totally agree that there are a lot of companies with very mature NLP tech. I guess my focus here was‚Ä¶ https://t.co/5OeArAU3hq
980211537473306600,Sat Mar 31 22:33:36 +0000 2018,@earnmyturns Thanks for those insights, Fernando! Could you elaborate a bit more on how your Theory of learning fro‚Ä¶ https://t.co/2bDRXPASq2
980208417628393500,Sat Mar 31 22:21:12 +0000 2018,RT @mundt_martin: I recently joined Twitter &amp; was at first very skeptical. I thought it was mainly for generating hype &amp; contained too much‚Ä¶
980202632689127400,Sat Mar 31 21:58:13 +0000 2018,@zehavoc True. Didn't mean to discredit our shared tasks. I guess what I was trying to get at here was that Imagene‚Ä¶ https://t.co/SxhhvKejPQ
980198771698143200,Sat Mar 31 21:42:53 +0000 2018,@rajhans_samdani Yeah, totally agree. This thread is very Deep Learning focused, just because DL has provided a lot‚Ä¶ https://t.co/Yh8kCNFl4y
980198278548598800,Sat Mar 31 21:40:55 +0000 2018,@zehavoc Yeah, I guess WMT fulfills that role for MT. Maybe it's also a factor that NLP tasks seem to be more diverse.
980197510441586700,Sat Mar 31 21:37:52 +0000 2018,@peratham Totally agree that there are a lot of useful vision datasets. Just wanted to mention those as examples th‚Ä¶ https://t.co/BmMbke3Lj8
980197162909929500,Sat Mar 31 21:36:29 +0000 2018,@EmmanuelAmeisen Thinking about it, I haven't seen word2vec applied to images. I guess I was thinking more in terms of visual words.
980117551610060800,Sat Mar 31 16:20:08 +0000 2018,@sandersted @jeremyphoward Yeah, just read this a couple of times on Twitter, so not sure what fraction. Mainly wan‚Ä¶ https://t.co/gQDd7ofi3p
980115863188791300,Sat Mar 31 16:13:26 +0000 2018,@honnibal Yep, definitely agree re the value &amp; potential of NLP. Continuous inputs is one factor that I mentioned,‚Ä¶ https://t.co/XUXiWVTVQT
980111842306658300,Sat Mar 31 15:57:27 +0000 2018,@dennybritz Yeah, correlation with performance of metrics such as BLEU, ROUGE, perplexity, etc. is an issue. Beside‚Ä¶ https://t.co/m1KMNVN9bR
980111110077591600,Sat Mar 31 15:54:32 +0000 2018,@joelgrus @sir_deenicus Yeah, that came to my mind, too. I wanted to see if there are also other issues that we can do something about.
980084095211528200,Sat Mar 31 14:07:12 +0000 2018,@sleepinyourhat @random_forests Yeah, the results look surprisingly good, but the paper is quite underwhelming.
980064606738579500,Sat Mar 31 12:49:45 +0000 2018,@sleepinyourhat @random_forests I assume this is the one: https://t.co/4Xm5o5lm2a
980043447422898200,Sat Mar 31 11:25:40 +0000 2018,@DataMappist Yeah, that's been true in my experience at least. E.g. for classification, classes are---subjectively-‚Ä¶ https://t.co/wAy8cWw5Vw
980038715870466000,Sat Mar 31 11:06:52 +0000 2018,@microth Having said that, AngelList lists more NLP (https://t.co/ixDY5qK34O) than CV startups (‚Ä¶ https://t.co/IMVRk1lAYP
980038469698359300,Sat Mar 31 11:05:54 +0000 2018,@microth Totally agree. I meant more that to me it seems applying 'pure' CV methods, e.g. object recognition to a n‚Ä¶ https://t.co/pLv97ZvFRw
980036772867473400,Sat Mar 31 10:59:09 +0000 2018,@microth Ah, actually wasn't aware that the license was free! Agreed. The XML format is not that user-friendly, tho‚Ä¶ https://t.co/ByMkr3FoTF
980031335518687200,Sat Mar 31 10:37:33 +0000 2018,16/ Anyway, these were just my thoughts. I‚Äôm looking forward to hearing what you think.
980031286017392600,Sat Mar 31 10:37:21 +0000 2018,15/ Another thing is to make it easier for people to discover datasets and existing baselines for their task. Many‚Ä¶ https://t.co/EpdpEfXrWh
980031230728200200,Sat Mar 31 10:37:08 +0000 2018,14/ The recent kaggle Jigsaw competition (https://t.co/3G9GIBpGoc) saw participation of 4,551 teams, but was largel‚Ä¶ https://t.co/UtZ1eE9pMH
980031167809376300,Sat Mar 31 10:36:53 +0000 2018,13/ Novel datasets are developed at a regular pace and shared tasks and competitions are frequently hosted. One fac‚Ä¶ https://t.co/pAotUArmeg
980031119792984000,Sat Mar 31 10:36:41 +0000 2018,12/ What does it take to catch up? Large publicly available datasets and competitions seem to have enabled a large‚Ä¶ https://t.co/3vO8Oi027u
980031070803513300,Sat Mar 31 10:36:30 +0000 2018,11/ Perhaps NLP should mostly focus on itself, e.g. on developing new methods that incorporate linguistic structure‚Ä¶ https://t.co/ORpyMzke07
980031016059449300,Sat Mar 31 10:36:16 +0000 2018,10/ Besides 1), these seem like things we can‚Äôt do anything about and the NLP community has made progress on 4). Do‚Ä¶ https://t.co/D1R0jHRQxJ
980030945590988800,Sat Mar 31 10:36:00 +0000 2018,9/ 2) The CV community is larger and seemingly more competitive, making the development of novel techniques more li‚Ä¶ https://t.co/cESIGIOHY2
980030896983113700,Sat Mar 31 10:35:48 +0000 2018,8/ Second, why does CV seem to be ahead? I can think of four reasons: 1) Deep Learning, the main driver of current‚Ä¶ https://t.co/2JN5Gr2NLW
980030849629515800,Sat Mar 31 10:35:37 +0000 2018,7/ CV is also ahead in terms of commercialization: Current CV methods enable applications such as self-flying drone‚Ä¶ https://t.co/ps4dsOlEdv
980030797775343600,Sat Mar 31 10:35:24 +0000 2018,6/ In contrast, NLP methods that made their way to CV seem to be a lot rarer. Word2vec, attention and the recent Tr‚Ä¶ https://t.co/SdKG5il6TY
980030752044744700,Sat Mar 31 10:35:14 +0000 2018,5/ Techniques such as residual or dense connections, GANs, adversarial examples, domain-adversarial loss, few-shot‚Ä¶ https://t.co/LDgtE5xTVg
980030698202558500,Sat Mar 31 10:35:01 +0000 2018,4/ However, for tasks where more general ML techniques are applicable, the knowledge transfer seems to be CV-&gt;NLP r‚Ä¶ https://t.co/h2udvTwFoU
980030645031366700,Sat Mar 31 10:34:48 +0000 2018,3/ First, is that really true? For many specialized applications, where task or domain-specific tools are required,‚Ä¶ https://t.co/l1vGgx42A2
980030581932286000,Sat Mar 31 10:34:33 +0000 2018,2/ I can‚Äôt speak about other application areas, so I will mostly compare CV vs. NLP. This is just a braindump, so f‚Ä¶ https://t.co/IbcPA79wtd
980030523660791800,Sat Mar 31 10:34:19 +0000 2018,1/ People (mostly people working with Computer Vision) say that CV is ahead of other ML application domains by at l‚Ä¶ https://t.co/15sYtt44pX
978719061898158100,Tue Mar 27 19:43:02 +0000 2018,@EdwardDixon3 @salesforce Thanks for the pointer, Edward!
978594410261819400,Tue Mar 27 11:27:43 +0000 2018,RT @coastalcph: Anders S√∏gaard is looking to fill 2 PhD positions and 3 postdocs to work on a Google Focused Research Award project on mult‚Ä¶
978374532070289400,Mon Mar 26 20:54:00 +0000 2018,The father of GANs discusses what to look for in a good GAN paper: Good evaluation (Inception distance, samples on‚Ä¶ https://t.co/YR9AByn6ci
978225231671300100,Mon Mar 26 11:00:44 +0000 2018,@mElantkowski Thanks for the shout-out! ‚ò∫Ô∏è
978210007379529700,Mon Mar 26 10:00:14 +0000 2018,New NLP News - ML Systems challenges; Multilingual Coref; Unsupervised NMT; Human MT performance; Neural Lattice La‚Ä¶ https://t.co/p2FdEmnHzy
977607013638312000,Sat Mar 24 18:04:09 +0000 2018,For everyone going to New Orleans for @NAACLHLT, it's now time to start familiarizing yourself with the geographic‚Ä¶ https://t.co/EuoKRx1nhT
977604831161540600,Sat Mar 24 17:55:29 +0000 2018,RT @jeremyphoward: This very interesting new draft paper from @timnitGebru @haldaume3 @katecrawford et al proposes creating "datasheets" fo‚Ä¶
977602753152340000,Sat Mar 24 17:47:13 +0000 2018,The next billion $ industry: Rent out your GPU compute to AI researchers; ~2x more profitable than mining Bitcoin üî•üí∞ https://t.co/uoTk0LnIMD
977212660767961100,Fri Mar 23 15:57:08 +0000 2018,@unlp_insight @insight_centre @ResearchatNUIG @nuigalway @sap_negi @roman_klinger @MichaelSchukat @EdwardACurry Wohoo, congrats! üéâ
977171793227276300,Fri Mar 23 13:14:45 +0000 2018,@yaringal @Smerity @StrongDuality @RichardSocher Here's a (noncomprehensive) collection of best practices for DL fo‚Ä¶ https://t.co/iQAq88q63T
977081641196752900,Fri Mar 23 07:16:31 +0000 2018,@KKumar_ üéâ
976773276235173900,Thu Mar 22 10:51:11 +0000 2018,@sbmaruf @artetxem @PyTorch Try clicking on the GitHub link. You'll find the reference information there.
975818069107990500,Mon Mar 19 19:35:32 +0000 2018,RT @artetxem: We have just released UNdreaMT, a @PyTorch implementation of our Unsupervised Neural Machine Translation system. Train your o‚Ä¶
974525233125244900,Fri Mar 16 05:58:16 +0000 2018,@syho0426 Hi Susanna, it's predefined based on the most common attributes that our clients in those industries are interested in.
974524797882310700,Fri Mar 16 05:56:32 +0000 2018,@lordOFawkward @Pocket @MathJax Yes, please! üòä
974399213265195000,Thu Mar 15 21:37:30 +0000 2018,RT @barbara_plank: Open call for PhD positions at ITU https://t.co/533zqNs3px - deadline: April 3 #cph @ITUkbh https://t.co/UUtqeE3wgD
974380185259860000,Thu Mar 15 20:21:53 +0000 2018,RT @trang_tran_203: @seb_ruder Best part of this week's newsletter... https://t.co/PnWSopezLM
974031778620104700,Wed Mar 14 21:17:27 +0000 2018,@EdwardDixon3 Oh, wow. Thanks a lot, Edward! :)
974003570495418400,Wed Mar 14 19:25:21 +0000 2018,RT @dirk_hovy: My new university has written a pretty nice profile of my research and me (in English and Italian). https://t.co/BGlrSpbjwf
973997198743613400,Wed Mar 14 19:00:02 +0000 2018,NLP News just cracked the 2,000 subscriber mark. Thanks to everyone subscribed for your support. If you would like‚Ä¶ https://t.co/oYb7PIeUQw
973950336296738800,Wed Mar 14 15:53:49 +0000 2018,@gustavoveronese üòÇ
973942541379932200,Wed Mar 14 15:22:51 +0000 2018,@unsorsodicorda @drevicko It's the most common automatic metric for evaluating machine translation systems. https://t.co/L05GLdpzhe
973925334100922400,Wed Mar 14 14:14:28 +0000 2018,Microsoft reports that they've achieved human parity on Chinese-to-English translation (27.40 BLEU; 1 BLEU better t‚Ä¶ https://t.co/nfGDYzWlSz
973190928553906200,Mon Mar 12 13:36:13 +0000 2018,@CreateMoMo No problem. I'll include your article in the next newsletter. :)
973173624558575600,Mon Mar 12 12:27:27 +0000 2018,@dkastaniotis @rasbt @soumithchintala @CSProfKGD @kfoynt Good luck! Looking forward to the new results.
973167688729153500,Mon Mar 12 12:03:52 +0000 2018,@CreateMoMo Hi Maolin, sure. Just send me the post and I'll include it if the topic is relevant and interesting to the newsletter audience.
973151677028106200,Mon Mar 12 11:00:14 +0000 2018,New NLP News - Coursera Deep Learning course notes; VAE explainer; Metalearning Symposium videos; ML crash crouse;‚Ä¶ https://t.co/5dPM9NLdkG
972611083361013800,Sat Mar 10 23:12:07 +0000 2018,@ameliovr There've been some advances around better dealing with learning rates and weight decay, understanding gen‚Ä¶ https://t.co/eQ7sDKsUbW
972571575789785100,Sat Mar 10 20:35:07 +0000 2018,RT @seanjtaylor: Incredibly useful reference on gradient descent optimization algorithms: https://t.co/UrXXvGwHRw  I've been reading up on‚Ä¶
972157944136597500,Fri Mar 09 17:11:30 +0000 2018,@kaggle, I'm trying to accept your TOS from 8 December 2017 and get the following error. Same thing happens when tr‚Ä¶ https://t.co/NCMvEGNlrT
972073317212532700,Fri Mar 09 11:35:13 +0000 2018,Given accuracy, data is likely more similar to sentiment analysis (SOTA on IMDb is ~95%) or might contain spurious correlations (e.g. URLs).
972073021082165200,Fri Mar 09 11:34:03 +0000 2018,"I trained fake news detection AI with &gt;95% accuracy, and almost went crazy"; Medium article claiming &gt;95% accuracy‚Ä¶ https://t.co/bTNfEnYpU7
971759323675332600,Thu Mar 08 14:47:31 +0000 2018,RT @_rockt: Calling a paper The Building Blocks of Interpretability and then only covering computer vision is misleading. @distillpub you w‚Ä¶
971444358267498500,Wed Mar 07 17:55:58 +0000 2018,@drthorn Awesome! Please share your experience when you've experimented with finetuning it!
971368056432877600,Wed Mar 07 12:52:46 +0000 2018,@rasbt Awesome, congrats! :)
971357459565736000,Wed Mar 07 12:10:39 +0000 2018,@NirantK Usually whatever task you care about. There are some issues with using text classification for evaluation‚Ä¶ https://t.co/XuB3TgslHk
971040942626099200,Tue Mar 06 15:12:56 +0000 2018,@NirantK Pretty much. For intrinsic evaluation, people mostly translated similarity datasets into different languag‚Ä¶ https://t.co/1qAW2aEHji
971038621250805800,Tue Mar 06 15:03:42 +0000 2018,RT @_aylien: For anyone curious about doing a PhD or Masters in #NLP/#machinelearning with us, consider this blog on research directions th‚Ä¶
970985034915164200,Tue Mar 06 11:30:47 +0000 2018,@cyrilzakka Yep.
970980736399487000,Tue Mar 06 11:13:42 +0000 2018,Let us know if you're interested in doing a PhD/MSc in ML, NLP and transfer learning with us. https://t.co/aD6wTt6Fir
970968956440928300,Tue Mar 06 10:26:53 +0000 2018,@arueckle Nice work! Particularly like the cross-lingual evaluation! üëçüèª
970951370512715800,Tue Mar 06 09:17:00 +0000 2018,Guide to Visual Question Answering: Datasets, Approaches and Evaluation; nice overview of VQA by @tryolabs https://t.co/r77dyJRDyV
970704964250144800,Mon Mar 05 16:57:52 +0000 2018,@microth Haha, not yet, but good idea! :)
970406820815822800,Sun Mar 04 21:13:10 +0000 2018,@sir_deenicus Could you elaborate a bit? You mean data augmentation in general is flawed or are you referring to a particular method?
970397303608574000,Sun Mar 04 20:35:20 +0000 2018,@JupyterAI Yes! That's definitely important and people are starting to work more on that. I just didn't feel like I‚Ä¶ https://t.co/hLUA9TTTEA
970367757517762600,Sun Mar 04 18:37:56 +0000 2018,@EmmanuelAmeisen @InsightDataAI Cool! The site is just down for maintenance. Looking forward to check it out when it's up again. :)
970364972898320400,Sun Mar 04 18:26:52 +0000 2018,@Allen_A_N Thanks for the pointers, Allen! üòÄ
970352129994055700,Sun Mar 04 17:35:50 +0000 2018,@NirantK Thanks! I've seen the paper. I don't think it's really zero-shot learning, though, as they require to map‚Ä¶ https://t.co/bSiYIYRjmH
970351007963246600,Sun Mar 04 17:31:23 +0000 2018,@Mr_ANich Thanks! :)
970350958730514400,Sun Mar 04 17:31:11 +0000 2018,@NDimensionData Thanks! I don't know of any other than openAI and AI-ON, but hope to see more of these.
970316682479722500,Sun Mar 04 15:14:59 +0000 2018,New blog post: Requests for research. A collection of interesting research directions around transfer learning and‚Ä¶ https://t.co/QgIxVAGN49
969946613488783400,Sat Mar 03 14:44:28 +0000 2018,@syho0426 Hi Susanna, our sentiment analysis algorithms use Deep Learning, with a combination of LSTMs and CNNs. Se‚Ä¶ https://t.co/8IWHs9b5e9
968873123096481800,Wed Feb 28 15:38:47 +0000 2018,RT @anotherjohng: If you're interested in doing a PhD or Masters in machine learning with my team at @_aylien, get in touch soon: https://t‚Ä¶
968760569078698000,Wed Feb 28 08:11:33 +0000 2018,RT @IAugenstein: Preprint of our #naacl2018 paper on multi-task learning with label transfer now online: https://t.co/A5jHhFWrdw #NLProc ht‚Ä¶
968078253645811700,Mon Feb 26 11:00:16 +0000 2018,New NLP News - Yann LeCun vs. Chris Manning; UMAP; Soft Cosine Measure; Convolution Visualizer; NNs on iOS Tutorial‚Ä¶ https://t.co/JmhStTWZyT
968062756544045000,Mon Feb 26 09:58:41 +0000 2018,@Lenasnews Send me a DM and we can talk more.
966366571814965200,Wed Feb 21 17:38:39 +0000 2018,RT @RadimRehurek: üõãüóû Newsletter bump, proud of this one: Cloud #GPU benchmarks, pretrained #NLP models, Count-Min Sketch algo, Poincar√© Emb‚Ä¶
966249517187240000,Wed Feb 21 09:53:31 +0000 2018,Great to see transfer learning with language model fine-tuning used to achieve SOTA on Thai text classification! Tr‚Ä¶ https://t.co/KxtrwIdpkr
965193716150472700,Sun Feb 18 11:58:09 +0000 2018,RT @Sigmoidal_io: On our blog, we have published a new article about Natural Language Processing with a fresh 2018 insight on the topic. #N‚Ä¶
964876181043601400,Sat Feb 17 14:56:22 +0000 2018,@TMills @jeremyphoward @amirieb Looking forward to reading your paper! :) I'll also be at NAACL. See you there!
964875928529723400,Sat Feb 17 14:55:22 +0000 2018,@marcus__ This is great, thanks! :)
964122845151400000,Thu Feb 15 13:02:53 +0000 2018,Deep Reinforcement Learning Doesn't Work Yet: Super comprehensive blog post on the Difficulty of Deep Reinforcement‚Ä¶ https://t.co/qMhk5G7CXd
964122130433609700,Thu Feb 15 13:00:03 +0000 2018,RT @coastalcph: 4 #NAACL2018 long papers accepted - congrats @IAugenstein, Maria Barrett, @johannesbjerva, @AnaValeriaGlez, @soegaarducph,‚Ä¶
964035519645732900,Thu Feb 15 07:15:53 +0000 2018,RT @IAugenstein: #naacl2018 paper "Multi-task Learning of Pairwise Sequence Classification Tasks Over Disparate Label Spaces" (IA, @seb_rud‚Ä¶
963115473293455400,Mon Feb 12 18:19:57 +0000 2018,@vlachos_nlp Thanks for creating such a helpful tutorial!
963115333912551400,Mon Feb 12 18:19:24 +0000 2018,@_rockt @vlachos_nlp No problem! Thanks for open-sourcing the implementation! :)
963115237569417200,Mon Feb 12 18:19:01 +0000 2018,@sivareddyg They are great slides! Thanks for making them available!
963114941795401700,Mon Feb 12 18:17:50 +0000 2018,@Even_Oldridge Thank you! Really appreciate it! üòä
963077601664421900,Mon Feb 12 15:49:28 +0000 2018,RT @m__dehghani: Read about our work (going to be presented at #ICLR2018) on how to learn from samples of variable quality: "Fidelity-weigh‚Ä¶
963025248789594100,Mon Feb 12 12:21:26 +0000 2018,@PyVale3 @CamachoCollados @Ale_Raganato üéâüëç Congrats!
963004829131006000,Mon Feb 12 11:00:17 +0000 2018,New NLP News - SPINN, ‚àÇ4, Nested LSTMs, Capsule Networks, Minigo, Matrix Calculus, Past Kaggle Comps, Private Image‚Ä¶ https://t.co/UuSlnfk4Nz
962654695704719400,Sun Feb 11 11:48:59 +0000 2018,Great, comprehensive blog post on performing image analysis and transfer learning on encrypted data by @mortendahlcs https://t.co/WmqW4frktg
962621030585028600,Sun Feb 11 09:35:12 +0000 2018,RT @dennybritz: Decided to write up something a little different from my usual posts: Introduction to Learning to Trade with Reinforcement‚Ä¶
962484279497412600,Sun Feb 11 00:31:48 +0000 2018,@jeremyphoward @Rajesh23MD @phanisrikanth33 @fastdotai As a way to generate English paraphrases. People have been u‚Ä¶ https://t.co/20hH3AYmtb
962268777453891600,Sat Feb 10 10:15:29 +0000 2018,RT @honnibal: @jekbradbury @seb_ruder Actually while we're talking about these optimizers...You should try parameter averaging --- I think‚Ä¶
962268633551589400,Sat Feb 10 10:14:54 +0000 2018,RT @jekbradbury: @seb_ruder That's not the latest adaptive learning rate method any more üòâ, the latest adaptive learning rate method is Ada‚Ä¶
962264672589303800,Sat Feb 10 09:59:10 +0000 2018,@antiformant üòÇ That was an... interesting slip. Fixed it now. üòÖ (for the curious: I misspelled Reddi as Reddit.)
962235163924226000,Sat Feb 10 08:01:55 +0000 2018,@jekbradbury Cool! Thanks for the pointer!
962085288079167500,Fri Feb 09 22:06:22 +0000 2018,@jeremyphoward Thanks for the note! I've added the caveat to the post. :)
962055250105561100,Fri Feb 09 20:07:00 +0000 2018,Updated overview of SGD optimization algorithms: Now includes AMSGrad (ICLR 2018), the latest adaptive learning rat‚Ä¶ https://t.co/0M9hs6piDz
961934762993705000,Fri Feb 09 12:08:14 +0000 2018,RT @dublinAI: What a GREAT NIGHT!!!  Thanks to our speakers, sponsor @PROWLER_IO venue @dogpatchlabs and of course the awesome, rapidly gro‚Ä¶
961168371294318600,Wed Feb 07 09:22:52 +0000 2018,@culurciello That might go away over time as we better understand its benefits. In the meantime, different parts li‚Ä¶ https://t.co/PDq2OkTcTb
961167685445996500,Wed Feb 07 09:20:08 +0000 2018,@culurciello It's a really promising architecture. I'm sure people will try it on different NLP transduction tasks.‚Ä¶ https://t.co/akPqCSAuK1
961165992373243900,Wed Feb 07 09:13:24 +0000 2018,@j_gauthier That'd be awesome, thanks!
961147750502068200,Wed Feb 07 08:00:55 +0000 2018,RT @EdinburghNLP: Interested in a PhD with @EdinburghNLP? Approximately 15 studentships at ILCC available for UK and EU/EEA nationals. Appl‚Ä¶
961017278140768300,Tue Feb 06 23:22:28 +0000 2018,@j_gauthier Are any slides of the class available?
958797838321930200,Wed Jan 31 20:23:12 +0000 2018,RT @thinkmariya: Got one too many trolls fooling with your customer support interfaces? @parsaghaffari of @_aylien shows you how to out-tro‚Ä¶
958716077780938800,Wed Jan 31 14:58:19 +0000 2018,@viiitdmj @jeremyphoward Perplexity (https://t.co/dR9jDcNml1) is standard for this. Perplexity is just the exponent‚Ä¶ https://t.co/hc5QAVdGQq
958451696367292400,Tue Jan 30 21:27:46 +0000 2018,@kroscoo @ICLR18 @aggielaz @sharky6000 @jzl86 @karl_tuyls Nice work, Kris! üëèüèª
958269452201742300,Tue Jan 30 09:23:35 +0000 2018,@GuillaumeLample @alex_conneau @LudovicDenoyer Congrats! Well deserved!
958261448857317400,Tue Jan 30 08:51:47 +0000 2018,@artetxem @eagirre Darn, sorry. I can't edit the newsletter anymore I'm afraid. Will remember it for next time. :)
958248310283948000,Tue Jan 30 07:59:35 +0000 2018,@JamieMurdoch2 @ICLR18 Congrats! üéâ
958111994103652400,Mon Jan 29 22:57:54 +0000 2018,@_ambodi Hey! I'm not aware of any survey on stochastic optimization in particular unfortunately.
958094702695321600,Mon Jan 29 21:49:12 +0000 2018,@m__dehghani @mehrjouarash @jkamps @sgouws @bschoelkopf Woohoo! Congrats! üéâ
958092028755300400,Mon Jan 29 21:38:34 +0000 2018,@sim0nsays @yoavgo This is also something @jeremyphoward and I had been talking about recently. Since negative samp‚Ä¶ https://t.co/JWN1Afars5
958053545110134800,Mon Jan 29 19:05:39 +0000 2018,@artetxem Haha :) I unfortunately didn't find a blog post about designing an app for learning Basque, so Spanish had to do. :-/
958047624195706900,Mon Jan 29 18:42:07 +0000 2018,@sim0nsays Hey Simon, that basically comes down to the derivation of the negative sampling equation, which is quite‚Ä¶ https://t.co/AiMEdkULUz
958012942271877100,Mon Jan 29 16:24:19 +0000 2018,@jonathonmorgan @NewKnowledgeAI Awesome! Congrats, Jonathon! üëèüèª
957997011432869900,Mon Jan 29 15:21:00 +0000 2018,RT @jeremyphoward: I've just launched Practical Deep Learning for Coders 2018! If you think it newsworthy please consider adding your vote‚Ä¶
957931381748887600,Mon Jan 29 11:00:13 +0000 2018,New NLP News - Poincar√© embeddings, trolling trolls, A2C comic, General AI Challenge, heuristics for writing, year‚Ä¶ https://t.co/qHxlfpkwcr
957184864704454700,Sat Jan 27 09:33:50 +0000 2018,@shilpa15397 Thank you, Shilpa! Means a lot to me. :)
956551629276418000,Thu Jan 25 15:37:34 +0000 2018,RT @dublinAI: We're very excited to have one of the UK's most exciting AI companies join us in Dublin for the first of our 2018 events. Spe‚Ä¶
956256982222164000,Wed Jan 24 20:06:45 +0000 2018,RT @parsaghaffari: Are trolls abusing your support channels? Here's a quick hack to troll them back using NLP and Machine Learning üòÅ https:‚Ä¶
956100281116262400,Wed Jan 24 09:44:05 +0000 2018,@radekosmulski @Smerity Wow! Thanks for the shout-out! üòä
955863221008785400,Tue Jan 23 18:02:05 +0000 2018,@jeremyphoward @Maciej_Kula @honnibal @goodfellow_ian @kevindewalt @Smerity @yoavgo I'm only aware of this paper by‚Ä¶ https://t.co/1Kc4rDe8Jr
955750635051405300,Tue Jan 23 10:34:43 +0000 2018,RT @LarsHulstaert: When solving a problem as a data scientist, it is crucial to optimally leverage the available data. In my next blog post‚Ä¶
955465826416676900,Mon Jan 22 15:42:59 +0000 2018,@nlpmattg @microth @haldaume3 How about ablations? These should be done on dev data, right?
955420298014281700,Mon Jan 22 12:42:04 +0000 2018,@dhruvghulati @ForbesUnder30 Yay, congrats! üëèüèªüéâ
955415290686378000,Mon Jan 22 12:22:10 +0000 2018,RT @CamachoCollados: A few days ago I wrote a short post on the application and general implicit benefits of using neural networks and word‚Ä¶
955360633477849100,Mon Jan 22 08:44:59 +0000 2018,@jeremyphoward @AMP_SV Yep. :)
954644196744364000,Sat Jan 20 09:18:07 +0000 2018,@xiangrenUSC @jeremyphoward @mattthemathman Here's the link to the datasets from Zhang et al. (2015): https://t.co/1LwqsxCjZk
954441516432445400,Fri Jan 19 19:52:44 +0000 2018,@jeremyphoward @mattthemathman We'll include more ablations to make this point clearer and compare it vs. other pre-trained approaches.
954441336303816700,Fri Jan 19 19:52:01 +0000 2018,@jeremyphoward @mattthemathman Arguably, one of the main benefits of fine-tuning is that you can use the same model‚Ä¶ https://t.co/Xh8bbQhPqQ
954408178023886800,Fri Jan 19 17:40:16 +0000 2018,@JamieMurdoch2 @dan_s_becker @jeremyphoward Hi Jamie, I didn't take it as such. That's really useful feedback. :) Y‚Ä¶ https://t.co/8h6hFqZJKF
954407450807021600,Fri Jan 19 17:37:22 +0000 2018,@shgidi @jeremyphoward Not yet. On the to-do list. :)
954373448075284500,Fri Jan 19 15:22:16 +0000 2018,@art_sobolev @jeremyphoward Their sentence embeddings are mainly useful for textual simlarity datasets, since this‚Ä¶ https://t.co/gRGLSf8mhq
954257869557436400,Fri Jan 19 07:43:00 +0000 2018,@dan_s_becker @JamieMurdoch2 @jeremyphoward Thanks for the feedback, Jamie! Missed that paper. While fine-tuning an‚Ä¶ https://t.co/o6oncNjVQ8
954021853680545800,Thu Jan 18 16:05:09 +0000 2018,@serrjoa @santty128 Thanks, Joan! :)
953743727587221500,Wed Jan 17 21:39:59 +0000 2018,RT @Dasha_Siberia: My team at LinkedIn Dublin is looking for an engineering intern to work on ML/#NLProc this summer! If you are a student,‚Ä¶
953730721960603600,Wed Jan 17 20:48:18 +0000 2018,@lukede0 @jeremyphoward Haha, I wouldn't normally use it. ^^ The only reason why I think it makes sense it because‚Ä¶ https://t.co/2nDV8VIPYE
953644030629503000,Wed Jan 17 15:03:49 +0000 2018,Google Cloud AutoML: Making AI accessible to every business - great to see transfer learning as a key component in‚Ä¶ https://t.co/tC2vDjzwCR
952970993311584300,Mon Jan 15 18:29:24 +0000 2018,@KaiLashArul Thanks! That's exactly what I was looking for! :)
952970900902678500,Mon Jan 15 18:29:02 +0000 2018,@sarafianosn @bousmalis @karol_kurach @mattzeiler @lethienhoavn Both papers are great, thanks! :)
952962938775724000,Mon Jan 15 17:57:24 +0000 2018,@bousmalis @karol_kurach @mattzeiler @lethienhoavn Thanks for all of the helpful links and pointers! I was mainly l‚Ä¶ https://t.co/W4wPljuQ5y
952907444124094500,Mon Jan 15 14:16:53 +0000 2018,Dear ML &amp; CV Twitter, can you point me to any representative papers that fine-tune different layers of pre-trained‚Ä¶ https://t.co/cquOZ6Hm6w
952873068589969400,Mon Jan 15 12:00:17 +0000 2018,New NLP News - 2017 Year in Review, 2018 Prognoses, Semi-supervised learning, CTC networks, random forests tutorial‚Ä¶ https://t.co/R742b5LkAs
952555964288393200,Sun Jan 14 15:00:14 +0000 2018,@maria_antoniak Looks cool! I'm also using Mendeley and was considering switching to Paperpile, but no offline vers‚Ä¶ https://t.co/u1m5XrWJbI
951989547277037600,Sat Jan 13 01:29:29 +0000 2018,RT @fchollet: Deep learning models are chains of differentiable blocks. Let's just call them "block chains" and start selling tokens https:‚Ä¶
951988180084928500,Sat Jan 13 01:24:03 +0000 2018,@microth Cool! Enjoy Barcelona! :)
951841013231861800,Fri Jan 12 15:39:16 +0000 2018,Do algorithms reveal sexual orientation or just expose our stereotypes? Great post and analysis debunking the misco‚Ä¶ https://t.co/avZ70acLVc
951727566850728000,Fri Jan 12 08:08:28 +0000 2018,@microth Cool! Congrats! :) Who will you be working with?
951241197246332900,Wed Jan 10 23:55:49 +0000 2018,RT @chibisi: Awesome Arxiv paper for introducing gradient descent optimization algorithms by @seb_ruder. https://t.co/AIx4bYljZm thanks a l‚Ä¶
951150647092551700,Wed Jan 10 17:56:00 +0000 2018,COTA: Improving Uber Customer Care with NLP &amp; Machine Learning - Detailed article about Uber's Customer Obsession T‚Ä¶ https://t.co/PkREyuguVM
951117176391524400,Wed Jan 10 15:43:00 +0000 2018,This AI Hunts Poachers: Cool article about PAWS, a system that uses ML to predict poaching and game theory to rando‚Ä¶ https://t.co/I96Vb3NUpE
951107489444933600,Wed Jan 10 15:04:30 +0000 2018,@chibisi üëç
951099664153026600,Wed Jan 10 14:33:25 +0000 2018,@chibisi Thank you, Chibisi! üòä
950630214307950600,Tue Jan 09 07:27:59 +0000 2018,@hardmaru @GECCO2018 Awesome! Congrats!
950607053835259900,Tue Jan 09 05:55:57 +0000 2018,@jeremyphoward @dennybritz @yoavgo @Smerity The Automated Knowledge Base Construction (AKBC) at NIPS 2017 hosted by‚Ä¶ https://t.co/04WEbBUMQ9
950598258044256300,Tue Jan 09 05:21:00 +0000 2018,@dennybritz @deliprao @jeremyphoward @yoavgo @Smerity Yep, agreed.
950597280226095100,Tue Jan 09 05:17:07 +0000 2018,@dennybritz @jeremyphoward @yoavgo @Smerity IMO sequence labelling goes a bit under the radar. Tasks like semantic‚Ä¶ https://t.co/j2LK32DrgO
949602995074875400,Sat Jan 06 11:26:11 +0000 2018,RT @_aylien: You may have missed it over Christmas but @seb_ruder's #NLP Newsletter is packed with the latest and most interesting NLP pape‚Ä¶
949274670930391000,Fri Jan 05 13:41:32 +0000 2018,@tw_killian Important contribution in making these kinds of approaches more efficient and scalable! Thanks for shar‚Ä¶ https://t.co/HabCjxVt3S
949208084320145400,Fri Jan 05 09:16:57 +0000 2018,RT @TensorFlow: TensorFlow Dev Summit will be on March 30th, 2018 in the Bay Area!   Sign up now to save the date and stay updated on the l‚Ä¶
949184822504599600,Fri Jan 05 07:44:31 +0000 2018,RT @Miles_Brundage: Review of my 2017 forecasts: https://t.co/cvr1gO0cB3
948900387355963400,Thu Jan 04 12:54:16 +0000 2018,@tw_killian Thanks for pointing me to your paper, which I had missed! I'll have a read through it and get back to you. :)
948869490938269700,Thu Jan 04 10:51:30 +0000 2018,@tw_killian @chelseabfinn @pabbeel I agree. I forgot to mention MAML and other meta-learning advances. I particular‚Ä¶ https://t.co/S4dtseRRBI
948868271842848800,Thu Jan 04 10:46:39 +0000 2018,@artix41 @bousmalis Important highlights and great summary, Arthur! Thanks for pointing me to it. :)
948868123951644700,Thu Jan 04 10:46:04 +0000 2018,RT @artix41: @seb_ruder @bousmalis Thanks for this summary, transfer learning for RL and robotics looks definitely promising! Concerning do‚Ä¶
948790018046242800,Thu Jan 04 05:35:42 +0000 2018,RT @boydgraber: UMD is looking to hire someone at the intersection of information / data science and journalism on the tenure track but has‚Ä¶
948423804132376600,Wed Jan 03 05:20:30 +0000 2018,@jackclarkSF @Miles_Brundage @bousmalis I think that domain randomization is also a form of adversarial examples /‚Ä¶ https://t.co/DGo1xwCGAp
948423368629411800,Wed Jan 03 05:18:46 +0000 2018,@Miles_Brundage @bousmalis I think that most would agree that there was significant progress, but as that's true fo‚Ä¶ https://t.co/KKAuqvO04L
948422663059357700,Wed Jan 03 05:15:58 +0000 2018,@nicklovescode Not really. I didn't find time to play around with them myself yet unfortunately. I'd expect mini wo‚Ä¶ https://t.co/tfO5F7NXSW
948404838273986600,Wed Jan 03 04:05:08 +0000 2018,@bousmalis 8/ There were probably many highlights that don't come to my mind right now. I'd really love to hear wha‚Ä¶ https://t.co/AHquZnR5w7
948404560984289300,Wed Jan 03 04:04:02 +0000 2018,7/ In sum, if I had to choose one achievement, it‚Äôd probably be the work by @bousmalis et al. That work, though, al‚Ä¶ https://t.co/QquDWRVXFK
948404339353124900,Wed Jan 03 04:03:09 +0000 2018,6/ Regarding NLP, there have been a few papers exploring the benefit of language modelling for transfer learning; I‚Ä¶ https://t.co/zXAyOYp7pX
948404270772117500,Wed Jan 03 04:02:53 +0000 2018,5/ I‚Äôve personally gotten more interested in multi-task learning (MTL) in 2017: MTL had been used in many cool appl‚Ä¶ https://t.co/I7j3A1CXHk
948404160424104000,Wed Jan 03 04:02:26 +0000 2018,4/ I don‚Äôt remember any advances in transfer learning for non-robotic RL that I was particularly excited about. I h‚Ä¶ https://t.co/shdnxbYzDr
948403923445997600,Wed Jan 03 04:01:30 +0000 2018,3/ Regarding transfer learning for RL, I was most impressed by self-supervised imitation learning, in particular th‚Ä¶ https://t.co/iN5kcK3fp3
948403768801902600,Wed Jan 03 04:00:53 +0000 2018,2/ Many interesting developments in domain adaptation in vision from people at Berkeley, OpenAI, Google and others‚Ä¶ https://t.co/7hjpYriO8g
948403531765067800,Wed Jan 03 03:59:57 +0000 2018,1/ @MilesBrundage asked me to give my thoughts on his prediction whether there has been an impressive transfer lear‚Ä¶ https://t.co/Y5R6WgwMpV
948383447730470900,Wed Jan 03 02:40:08 +0000 2018,@Miles_Brundage Could you send the link again to your 2017 predictions?
948366134125912000,Wed Jan 03 01:31:20 +0000 2018,@Miles_Brundage @marcgbellemare üëç
947689461290229800,Mon Jan 01 04:42:29 +0000 2018,Semi-supervised image classification explained: Nice overview of state-of-the-art approaches for semi-supervised im‚Ä¶ https://t.co/ZTDvIEfZIe
945247815139233800,Mon Dec 25 11:00:15 +0000 2017,New NLP News - Cat ML Papers, Multi-agent RL tool, TFGAN, MUSE, Intro to GPs, Word Mover's Distance tutorial, Gradi‚Ä¶ https://t.co/JxGeHYGw3O
944321648878084100,Fri Dec 22 21:40:00 +0000 2017,Multilingual Unsupervised and Supervised Embeddings (MUSE): state-of-the-art multilingual embeddings that enable Un‚Ä¶ https://t.co/BH4FfD5usb
944290947789213700,Fri Dec 22 19:38:00 +0000 2017,Nice tutorial on finding tutorials with word2vec and Word Mover's Distance by @gensim_py https://t.co/cFgb1LfAGM
944234575449833500,Fri Dec 22 15:54:00 +0000 2017,Deep Learning Hardware Limbo: The state of Deep Learning hardware at the end of 2017 and the competition between NV‚Ä¶ https://t.co/q3MJGbPHmv
944199846965186600,Fri Dec 22 13:36:00 +0000 2017,Deep Learning Achievements Over The Past Year: Nice review of some of the highlights of Deep Learning applications‚Ä¶ https://t.co/W5LAJqPxPI
943985433985372200,Thu Dec 21 23:24:00 +0000 2017,Gradient descent vs. neuroevolution: Nice visual explainer blog post on the differences between SGD and neuroevolut‚Ä¶ https://t.co/3LTJUi2qlN
943954982805442600,Thu Dec 21 21:23:00 +0000 2017,Welcoming the Era of Deep Neuroevolution: Detailed blog post that presents genetic algorithms as a competitive alte‚Ä¶ https://t.co/LknnNwTfN8
943922267544801300,Thu Dec 21 19:13:00 +0000 2017,Putting the Linguistics in Computational Linguistics: 4 things you can do to make your paper more linguistically in‚Ä¶ https://t.co/ltBLu7R4hy
943866903940001800,Thu Dec 21 15:33:00 +0000 2017,Artwork Personalization at Netflix: Cool blog post on how Netflix uses contextual bandits to personalize the artwor‚Ä¶ https://t.co/7yeDoEARIn
943784263614214100,Thu Dec 21 10:04:37 +0000 2017,What the SATs Taught Us about Finding the Perfect Fit: How to model clothes size with Item Response Theory by‚Ä¶ https://t.co/tCp0Je4Lrs
941330944237228000,Thu Dec 14 15:36:00 +0000 2017,RT @dirk_hovy: I am looking for a candidate for a three-year research postdoc position on NLP/Comp. Soc. Science, starting March 2018 at Bo‚Ä¶
941310533097218000,Thu Dec 14 14:14:54 +0000 2017,@manaalfar Please share when you've found some üòä
941062658198986800,Wed Dec 13 21:49:56 +0000 2017,Deep Learning for NLP, advancements and trends in 2017: some nice paper highlights with a focus on transfer learnin‚Ä¶ https://t.co/hkrGLhZSVe
940625691334402000,Tue Dec 12 16:53:35 +0000 2017,RT @_aylien: It's been quite a year in AI - a good reason to have a look through 2017's AI Index report (via @seb_ruder's NLP News) https:/‚Ä¶
940362834482999300,Mon Dec 11 23:29:05 +0000 2017,RT @nlppeople: #NLProc Landscape: #Ireland https://t.co/khE11aQp0y #nlppeople
940174393438883800,Mon Dec 11 11:00:17 +0000 2017,New NLP Newsletter - Recurrent Highway Hypernetworks, New Multimodal Environments, LDA2vec, DL for Structured Data,‚Ä¶ https://t.co/X9vvotpLW5
940157616927526900,Mon Dec 11 09:53:37 +0000 2017,The Case for Learned Index Structures: Indexing learned with NNs is up to 70% faster and uses 10x less storage than‚Ä¶ https://t.co/W27SvpgbvJ
939621838812078100,Sat Dec 09 22:24:38 +0000 2017,RT @IAugenstein: 1+ tenure-track assistant professorships in #NLProc #ML at @DIKU_Institut @uni_copenhagen Excellent opportunity to work wi‚Ä¶
939562401627861000,Sat Dec 09 18:28:27 +0000 2017,@seamuslawless @marievonboran @IrishTimes @AdaptCentre @tcddublin Hey Seamus, would be great to have you speak at a‚Ä¶ https://t.co/LuQlNH1uU3
939444740579315700,Sat Dec 09 10:40:54 +0000 2017,@johannesbjerva üéâ Congrats! üëè
938480474976149500,Wed Dec 06 18:49:16 +0000 2017,RT @jeremyphoward: If you're not sure about SGD, momentum, and nesterov, studying this notebook from @KeremTurgutlu might be just what you‚Ä¶
938128962760429600,Tue Dec 05 19:32:28 +0000 2017,RT @kdnuggets: Optimization for #DeepLearning Highlights in 2017 https://t.co/cQx4hi6jqm @seb_ruder https://t.co/HBmlQLZjNO
938067519079899100,Tue Dec 05 15:28:19 +0000 2017,RT @_aylien: In his spare time, our engineer built an experimental neural net in Rust and named it after a Dota 2 hero. Read about Juggerna‚Ä¶
937814490845990900,Mon Dec 04 22:42:53 +0000 2017,RT @Johannes_Welbl: Pushing Machine Comprehension forward ‚Äî two multi-hop datasets for text understanding across document boundaries, based‚Ä¶
937736918346944500,Mon Dec 04 17:34:38 +0000 2017,@_rockt Yeah, totally. Highlights the importance of finding methods that are robust and work with different learning rates/schedules.
937633834014793700,Mon Dec 04 10:45:01 +0000 2017,Search ICLR 2018 by @Smerity - now with Top 100 papers function: super useful to make sense of the ICLR reviews and‚Ä¶ https://t.co/FDyokMptYy
937611750320623600,Mon Dec 04 09:17:15 +0000 2017,RT @RadimRehurek: Awesome write up! I feel these TL;DR syntheses will become an industry of their own‚Äîquite valuable üèÜ https://t.co/GF7NQL9‚Ä¶
937445025079799800,Sun Dec 03 22:14:45 +0000 2017,This is so awesome! Really glad to see this getting more attention! This is the way forward for advancing AI resear‚Ä¶ https://t.co/q2GLGCG00X
937443490794963000,Sun Dec 03 22:08:39 +0000 2017,RT @jeremyphoward: Great discussion from @seb_ruder of recent practical advances in optimization (most of which are covered in our upcoming‚Ä¶
937416428122189800,Sun Dec 03 20:21:07 +0000 2017,New blog post: Optimization for Deep Learning Highlights in 2017 https://t.co/gnc3IueS8r
937298826687406100,Sun Dec 03 12:33:49 +0000 2017,Embodied Question Answering - Navigation + Visual QA! Cool new task &amp; 3D environments that push the boundaries of c‚Ä¶ https://t.co/0mVn18HWMP
936654835386118100,Fri Dec 01 17:54:49 +0000 2017,@gchrupala üëçüèª
936652583443271700,Fri Dec 01 17:45:52 +0000 2017,@gchrupala Apparently, the origin is a Russian word meaning "any foreigner who cannot speak Russian" :D
936340708344127500,Thu Nov 30 21:06:35 +0000 2017,Neural Text Generation: A Practical Guide - a useful overview of encoder-decoder model with helpful heuristics and‚Ä¶ https://t.co/sGfYlFllAp
936311474271727600,Thu Nov 30 19:10:25 +0000 2017,@jasonbaldridge @linghacks This looks fantastic! Do you know of any similar events in other countries, e.g. in Europe?
936287385150738400,Thu Nov 30 17:34:42 +0000 2017,RT @parsaghaffari: We are looking for a Search and Data Engineer to join us and help evolve our pipelines for retrieving, analyzing and ind‚Ä¶
936215562388230100,Thu Nov 30 12:49:18 +0000 2017,RT @Tbeltramelli: Very excited to share some of our progress! Teaching Machines to Understand User Interfaces https://t.co/ViIfCvGH2c
935865040489631700,Wed Nov 29 13:36:27 +0000 2017,@risi1979 This is really cool! Sounds like a great event!
935133459659489300,Mon Nov 27 13:09:25 +0000 2017,Structured Deep Learning: nice blog post on utilizing Deep Learning with structured data using entity embeddings https://t.co/lrdUV0On9U
935118646149599200,Mon Nov 27 12:10:33 +0000 2017,@VeredShwartz I think we're primed in a certain way. ;)
935109106460954600,Mon Nov 27 11:32:39 +0000 2017,@mohammad_d1993 Unfortunately not. Sorry about that. :(
935107992550330400,Mon Nov 27 11:28:13 +0000 2017,@marian_nmt Sure! Great library! Thanks for open-sourcing it! :)
935100967326044200,Mon Nov 27 11:00:18 +0000 2017,New NLP News - Flies smell word vectors, MarianNMT, Distributed Learning with keras, NLP workshops, NLP for net neu‚Ä¶ https://t.co/ebDHJkaXnS
934802118367744000,Sun Nov 26 15:12:47 +0000 2017,@dov_rahul Glad you like it! :) Don't think the talk was recorded unfortunately. Sorry. :(
934774910899118100,Sun Nov 26 13:24:40 +0000 2017,Slides of a talk on Optimization for Deep Learning I gave this Friday. Content is mainly based on my SGD blog post,‚Ä¶ https://t.co/ZXLYSSeJQk
934745122817957900,Sun Nov 26 11:26:18 +0000 2017,@m__dehghani @jkamps Cool! Really excited about this research direction! Looking forward to the preprint! :)
934383917330612200,Sat Nov 25 11:31:00 +0000 2017,Understanding objective functions in Neural Networks - nice overview of the connection between maximum likelihood a‚Ä¶ https://t.co/yHT0i82Jre
934142073992818700,Fri Nov 24 19:30:00 +0000 2017,Cool blog post on using multi-scale CNNs and density maps to count people standing in line by @dimroc https://t.co/EbFqLZnNjW
934099794926084100,Fri Nov 24 16:42:00 +0000 2017,High-fidelity speech synthesis with WaveNet: Training a small, parallel WaveNet using a novel form of distillation‚Ä¶ https://t.co/RP20Gtb0js
933807871250149400,Thu Nov 23 21:22:00 +0000 2017,An overview of the creative AI landscape by @elluba https://t.co/N36lzV4zC3
933777421781913600,Thu Nov 23 19:21:00 +0000 2017,3 simple rules for user experience design for APIs: 1. Deliberately design end-to-end workflow; 2. Reduce cognitive‚Ä¶ https://t.co/77v8TcxM7b
933109905854271500,Tue Nov 21 23:08:32 +0000 2017,A Berkeley View of Systems Challenges for AI - Berkeley's AI luminaries propose open research directions in systems‚Ä¶ https://t.co/MTjmga8Fqv
933020182125207600,Tue Nov 21 17:12:00 +0000 2017,Understanding the Mixture of Softmaxes - a digest of the most recent innovation in language modeling and in dealing‚Ä¶ https://t.co/ApUlC11SLF
932968286567800800,Tue Nov 21 13:45:47 +0000 2017,@ishita13316 Yep, just saw your email. üòä
932893787038408700,Tue Nov 21 08:49:45 +0000 2017,Expressivity, Trainability, and Generalization in Machine Learning - great blog post by @ericjang11 on understandin‚Ä¶ https://t.co/SR0HVO0w4x
932717070302793700,Mon Nov 20 21:07:33 +0000 2017,@joabingel @uclmr @libreoffice Worst case maybe upload on Google Drive? Not sure I'll be able to read LibreOffice w‚Ä¶ https://t.co/0ERMcXDSRP
932716613647896600,Mon Nov 20 21:05:44 +0000 2017,@jeremyphoward @Smerity @sleepinyourhat @Miles_Brundage Cool! Are the slides/video already available anywhere?
932660681710633000,Mon Nov 20 17:23:29 +0000 2017,@joabingel @uclmr Are the slides available anywhere?
931547918829588500,Fri Nov 17 15:41:45 +0000 2017,FigureQA, a new dataset by @MaluubaInc for Visual Question Answering on graphs and pie charts; consists of 1.3M que‚Ä¶ https://t.co/7qJP7orCoX
930917218325581800,Wed Nov 15 21:55:35 +0000 2017,@ehfo0 Cheers! I don't know that much about AVEB yet. I'll make one once I know more. :)
930729004562337800,Wed Nov 15 09:27:41 +0000 2017,@Smerity @jeremyphoward @yoavgo Gaussian noise is sometimes used; more intelligent perturbations (e.g. adversarial)‚Ä¶ https://t.co/8F5kVkfW8C
930577522491842600,Tue Nov 14 23:25:45 +0000 2017,Great and super important example of multi-task learning in practice: Jointly predicting hypertension, sleep apnea,‚Ä¶ https://t.co/vT8NHPVE6p
930178482134507500,Mon Nov 13 21:00:06 +0000 2017,@data_beth Thanks for the shout-out, Anna! The talks are great to follow along with the book.
930126510647889900,Mon Nov 13 17:33:35 +0000 2017,@EmmanuelAmeisen Thank you, Emmanuel! Glad you like it!
930096773560926200,Mon Nov 13 15:35:25 +0000 2017,@jvtastic Thank you, Jason! üòä
930035018893840400,Mon Nov 13 11:30:02 +0000 2017,New NLP News - GAN Playground, 2 Big ML Challenges, Pytorch NLP models, Linguistics in *ACL, mixup, Feature Visuali‚Ä¶ https://t.co/f0ifPruNbr
929982253807136800,Mon Nov 13 08:00:22 +0000 2017,@RangarajanRohit @parsaghaffari Hey! Thanks for your interest! Just follow our accounts and we'll keep you posted.
929110692904521700,Fri Nov 10 22:17:05 +0000 2017,Insightful, beautiful article on using feature visualization to make models interpretable by @ch402 @ludwigschubert‚Ä¶ https://t.co/uwvgilwEZK
929088002046951400,Fri Nov 10 20:46:55 +0000 2017,@barbara_plank @ITUkbh Awesome! Congrats, Barbara! :)
929044294966620200,Fri Nov 10 17:53:15 +0000 2017,RT @Tim_Dettmers: Our knowledge graph link prediction paper about ConvE was accepted at #AAAI2018 @riedelcastro @PMinervini https://t.co/at‚Ä¶
928745842319745000,Thu Nov 09 22:07:18 +0000 2017,RT @parsaghaffari: We hardly fit in one frame anymore, and I'm more grateful and proud than ever to have the opportunity to work with these‚Ä¶
928731782559780900,Thu Nov 09 21:11:26 +0000 2017,@NathanBenaich @PointNineCap Congrats, Nathan! Awesome news!!
928593965594693600,Thu Nov 09 12:03:48 +0000 2017,@joostbastings Thanks! :)
928297282700808200,Wed Nov 08 16:24:53 +0000 2017,@eoinhurrell Thanks! üòä
928296421345939500,Wed Nov 08 16:21:28 +0000 2017,@born2data Yeah, just get in touch with us via the email posted on the bottom of the call announcement and we can t‚Ä¶ https://t.co/Ow5sSenKfW
928291015680720900,Wed Nov 08 15:59:59 +0000 2017,@deliprao Thanks! :)
928290962077569000,Wed Nov 08 15:59:46 +0000 2017,@born2data The best way to do a postdoc with us would be through a Science Foundation Ireland Fellowship (see here‚Ä¶ https://t.co/hpA1Vpa8aq
928289906073448400,Wed Nov 08 15:55:35 +0000 2017,@IAugenstein Thanks! :)
928289874301644800,Wed Nov 08 15:55:27 +0000 2017,@microth Haha, thanks! :) But the more people with diverse backgrounds we can learn from, the better.
928274705097863200,Wed Nov 08 14:55:10 +0000 2017,Happy that we're announcing ‚Ç¨2m investment today! We're also hiring. If you want to work or do research with us (al‚Ä¶ https://t.co/RrX1ySqSde
928256895978221600,Wed Nov 08 13:44:24 +0000 2017,RT @Entirl: Deep Learning &amp; advanced Natural Language Processing are the core technologies at @_aylien - @Entirl's HPSU Divisional Manager‚Ä¶
928021783659327500,Tue Nov 07 22:10:09 +0000 2017,RT @DashingD3js: Juggernaut: Neural Networks in a web browser (Demo visualized in #D3js) https://t.co/o6h36LZlKG https://t.co/wOO59bYfWn
927891969019433000,Tue Nov 07 13:34:19 +0000 2017,RT @_aylien: Introducing Juggernaut, a neural net in Rust that trains a model from the browser. No JS, no server. https://t.co/KJzIQUbEQh #‚Ä¶
927867927528919000,Tue Nov 07 11:58:47 +0000 2017,@LeonDerczynski @ITUkbh Awesome! Congrats, Leon! :)
927541070212358100,Mon Nov 06 14:19:58 +0000 2017,@emilymbender @haldaume3 @_shrdlu_ @evanmiltenburg @LeonDerczynski @Laplace_wdd @robmalouf @adveisner @sjmielke‚Ä¶ https://t.co/UJyY9cCeOV
926398619825115100,Fri Nov 03 10:40:17 +0000 2017,@earnmyturns @deliprao Ivan Vulic, @soegaarducph, and I have also recently surveyed work on creating a shared cross‚Ä¶ https://t.co/fOvWyLAmAQ
926226423358124000,Thu Nov 02 23:16:02 +0000 2017,Whodunnit? Crime Drama as a Case for NLG - always great to see NLP applied to creative new tasks by @EdinburghNLP https://t.co/5WnS5DxJ9j
926221588353179600,Thu Nov 02 22:56:49 +0000 2017,@RadimRehurek @gensim_py Awesome! Congrats, Radim! Well deserved! Any way to stay up-to-date on your progress, eave‚Ä¶ https://t.co/yTUZyMqnMU
926214211163877400,Thu Nov 02 22:27:30 +0000 2017,Prime example that context is hard: Google Home confuses temperature inside with temperature in Side, Turkey https://t.co/akjIAbk2kS
926129607958585300,Thu Nov 02 16:51:19 +0000 2017,@amazonmturk Hey, I'd love to talk about using your recent APIs for sentiment analysis and other tasks. Care to send me a DM?
925742705958932500,Wed Nov 01 15:13:55 +0000 2017,RT @_rockt: I am very excited to share our paper on Differentiable Tree Planning for Deep RL w/ @greg_far @MaxiIgl &amp; @shimon8282 https://t.‚Ä¶
925440739810480100,Tue Oct 31 19:14:00 +0000 2017,The 2017 State of Data Science @kaggle survey with tidbits such as "avg data scientist is 30 years &amp; has a Master's" https://t.co/3buGDfNsQY
925403434588803100,Tue Oct 31 16:45:46 +0000 2017,RT @GalaxyKate: OK, to announce my big announcement for the day:  Google opensourced the bot-making language/platform I built there! https:‚Ä¶
925402683602763800,Tue Oct 31 16:42:47 +0000 2017,Cool, detailed blog post on closing the Simulation-to-Reality Gap for Deep Robotic Learning by @bousmalis https://t.co/97aBCnCEne
924986212330410000,Mon Oct 30 13:07:53 +0000 2017,New NLP News - Paperclip maximizer, Generative models, Debugging ML, ICLR, Interpretability (with link this time) https://t.co/YTEAxuzdUg
924985545700380700,Mon Oct 30 13:05:14 +0000 2017,@naivebayesian Thanks! :)
924938999122276400,Mon Oct 30 10:00:16 +0000 2017,NLP News - Paperclip maximizer, Generative models, Debugging ML, Evolution Strategies, Interpretability, arXiv, ICLR 2018, Multi-hop QA
924938999122276400,Mon Oct 30 10:00:16 +0000 2017,NLP News - Paperclip maximizer, Generative models, Debugging ML, Evolution Strategies, Interpretability, arXiv, ICLR 2018, Multi-hop QA
924557559087992800,Sun Oct 29 08:44:34 +0000 2017,RT @hardmaru: A Visual Guide to Evolution Strategies https://t.co/LEfMMLqZOY https://t.co/AvkjBOiWJR
923845690912014300,Fri Oct 27 09:35:51 +0000 2017,@ehfo0 Thanks! I'm glad it's helpful! What's AVEB? :)
923825238961516500,Fri Oct 27 08:14:35 +0000 2017,Learning a hierarchical policy to solve mazes by @OpenAI - interesting approach; first author is a high schooler! https://t.co/peVDYhw2pY
923545407577362400,Thu Oct 26 13:42:38 +0000 2017,@mohitban47 @UNC @unccs @DARPA Great! Looking forward to it! :)
923530167548563500,Thu Oct 26 12:42:04 +0000 2017,RT @_aylien: All PhDs interested in an industry-based postdoc: the @scienceirel Industry Fellowship deadline is Nov 23 https://t.co/0fdkIjI‚Ä¶
923530075299041300,Thu Oct 26 12:41:42 +0000 2017,@UNC @mohitban47 @unccs @DARPA Awesome work @mohitban47 and well deserved! I'm really interested in your life-long‚Ä¶ https://t.co/Lck721FuJp
923452270271975400,Thu Oct 26 07:32:32 +0000 2017,@deliprao @r7dotai @bcmcmahan Congrats and good luck!!
923128014686380000,Wed Oct 25 10:04:04 +0000 2017,@nishantiam @_rockt Well, you would like to have a webpage that feels as native as possible, i.e. it should be beautiful and responsive.
923111161742200800,Wed Oct 25 08:57:06 +0000 2017,@_rockt Yeah, some papers also fail to render for me.
923104938217140200,Wed Oct 25 08:32:22 +0000 2017,For instance, it turns this https://t.co/w0pquLkgH2 into this https://t.co/Fwtbxh1cdd
923104755169275900,Wed Oct 25 08:31:38 +0000 2017,Arxiv Vanity: open-source LaTeX to HTML converter that generates gorgeous webpages from arXiv papers https://t.co/PHeaqbyVJL
922947796554088400,Tue Oct 24 22:07:56 +0000 2017,@_ahani_ Haha :) It seems you're already doing a great job on your own blog! üëè Just keep going, write where you're‚Ä¶ https://t.co/gaJguuDnOJ
922925188294967300,Tue Oct 24 20:38:06 +0000 2017,Colaboratory: Google's collaboration tool based on Jupyter notebooks, with Drive integration &amp; live collaboration https://t.co/OQiRPn7O8P
922923948286738400,Tue Oct 24 20:33:11 +0000 2017,@TrailofPapers Hey, yep. The cover image of the post is from that paper (see bottom of the post for attribution). I‚Ä¶ https://t.co/8z1fqjAsTh
922913544009539600,Tue Oct 24 19:51:50 +0000 2017,"I interviewed at 5 top companies in 5 days, and got 5 job offers" - honest article about interviews &amp; preparation https://t.co/kzKbxE9zH0
922811575664365600,Tue Oct 24 13:06:39 +0000 2017,@m4rkmc That's awesome! Really appreciate it. :)
922775009843695600,Tue Oct 24 10:41:21 +0000 2017,@m4rkmc Thanks, Mark! Glad you like it! :)
922743534444269600,Tue Oct 24 08:36:17 +0000 2017,@r_speer @LuminosoInsight Thanks for the reference! I've added a mention of your AAAI paper to the post. :)
922471278908526600,Mon Oct 23 14:34:26 +0000 2017,@crocodoyle @emdupre_ Glad you found it helpful, Andrew! :)
922427208441958400,Mon Oct 23 11:39:19 +0000 2017,RT @dennybritz: The Wild Week in AI - AlphaGo Zero; Uber's DL framework; Intel releases new DL chips; and more https://t.co/KIT46qDtBh
922416302605721600,Mon Oct 23 10:55:58 +0000 2017,@fblain They seem to be working for me (clicking on the headlines). What's the issue you're experiencing?
922403278771707900,Mon Oct 23 10:04:13 +0000 2017,@surafelml Quite a few people seem to be working on GANs and VAEs for NLP, but I'm not aware of too many success stories yet.
922375480283750400,Mon Oct 23 08:13:46 +0000 2017,@SiddKotwal Yep! Nice, versatile method! Someone on HN also mentioned it. Have now added it to the post. :)
922172227931566100,Sun Oct 22 18:46:06 +0000 2017,@alexissmirnov Thanks, Alexis! :)
922081884380827600,Sun Oct 22 12:47:07 +0000 2017,@SiddKotwal Good timing then! Hope it helped! üòä
922051617834373100,Sun Oct 22 10:46:51 +0000 2017,Generative Adversarial Networks: An Overview - succinct review of GAN architectures and discussion of challenges https://t.co/GQWnApGWI5
922047906030473200,Sun Oct 22 10:32:06 +0000 2017,@r_speer For KB completion, or a la retro-fitting, or were you thinking of something else? Do you have some references for me? :)
921741399258816500,Sat Oct 21 14:14:09 +0000 2017,@AndrewLBeam Thanks for the pointer! Definitely worth mentioning! Added the reference now. :)
921737227419439100,Sat Oct 21 13:57:34 +0000 2017,@ssuty Thanks Taeyeong! Glad you liked it. :)
921736985160699900,Sat Oct 21 13:56:36 +0000 2017,New blog post: Word embeddings in 2017 - Trends and future directions https://t.co/BiFMn3zAzI
921489687839170600,Fri Oct 20 21:33:56 +0000 2017,@rbhar90 @deep_chem Awesome! üëè
921280410423881700,Fri Oct 20 07:42:21 +0000 2017,RT @IAugenstein: 10 days until deadline for #nips2017 #lld2017 workshop! 5 pages excl references, cross-submissions allowed. #ML https://t.‚Ä¶
921048763975852000,Thu Oct 19 16:21:52 +0000 2017,@Rajesh23MD @jeremyphoward At the moment, the best you can do is to use pre-trained word embeddings. There is work‚Ä¶ https://t.co/Ij6R9CdCbp
920979250999627800,Thu Oct 19 11:45:39 +0000 2017,@pymnpsbn @soegaarducph Thanks for the note. We'll try to include it in the revised version. :)
920962019225940000,Thu Oct 19 10:37:10 +0000 2017,New survey w/ Ivan Vuliƒá,  @soegaarducph: Everything you need to know for learning word embeddings across languages https://t.co/Ivhq5689VR
920761543381352400,Wed Oct 18 21:20:33 +0000 2017,@jeremyphoward I'd love to! :)
920750325795115000,Wed Oct 18 20:35:59 +0000 2017,@jeremyphoward Yes, I'm pretty confident that with better/easier ways to pre-train NLP models, we'll be able to make many more advances.
920743171017109500,Wed Oct 18 20:07:33 +0000 2017,@jeremyphoward Yes, totally. ATM, NLP is lagging behind CV re core innovations in DL. I'd assume because data is sm‚Ä¶ https://t.co/CBY2DWHQ7D
920738980836016100,Wed Oct 18 19:50:54 +0000 2017,@jeremyphoward Haha, thanks for raising awareness of these low-hanging fruit and making NLP more accessible! We nee‚Ä¶ https://t.co/JldPt8UgT1
920727126793482200,Wed Oct 18 19:03:47 +0000 2017,@jeremyphoward Cool job, Jeremy! Glad to see you delving more and more into NLP! :)
920726561569083400,Wed Oct 18 19:01:33 +0000 2017,AlphaGo Zero is another striking example of the huge potential of learning through self-play for #machinelearning https://t.co/To8tyFwOhh
920579718969163800,Wed Oct 18 09:18:03 +0000 2017,RT @_aylien: Our founder &amp; CEO @parsaghaffari is speaking at an NLP-oriented #machnelearning Meetup next Monday - https://t.co/wTvCHm6GbA
919990440878837800,Mon Oct 16 18:16:28 +0000 2017,RT @mohitban47: We are hiring for several exciting tt faculty positions in ML, NLP, robotics, cyber-phys sys! Spread the word and ping me f‚Ä¶
919926942790504400,Mon Oct 16 14:04:09 +0000 2017,@_rockt @UniofOxford Cool stuff! Congrats, Tim!
919921688510435300,Mon Oct 16 13:43:16 +0000 2017,New NLP Newsletter - PyTorch DeepMoji, AutoML, GANs, DisSent, and DilatedRNN https://t.co/NNRL4Hh84I
919549055260340200,Sun Oct 15 13:02:33 +0000 2017,@GianniBarlacchi @UniTrento @FBKcom @kdd_lab @Unipisa @FIGC @mesosbrodleto @lucpappalard @denadai2 @rossi_ale Congrats! üéâ
919248761146273800,Sat Oct 14 17:09:18 +0000 2017,RT @NLPDublin: Join us for our next event together with the Dublin Spark meetup with talks on IoT and discourse at @NitroHQ https://t.co/sL‚Ä¶
918508817675116500,Thu Oct 12 16:09:01 +0000 2017,RT @_aylien: Londoners! Our research scientist @seb_ruder is talking today at 6 in the @turinginst about Transfer Learning &amp; NLP https://t.‚Ä¶
918370185257549800,Thu Oct 12 06:58:09 +0000 2017,Building an AI that understands the world through video: Cool demos of learning to predict motions by @twentybn https://t.co/MfsQCnFrjv
918170252835541000,Wed Oct 11 17:43:41 +0000 2017,@tallinzen Cool! Congrats! :)
918165046177525800,Wed Oct 11 17:23:00 +0000 2017,Incorporating prior information is super important for generalizing better. This looks like a great step towards that!
918164577858224100,Wed Oct 11 17:21:08 +0000 2017,TensorFlow Lattice: Making ML models generalize better by incorporating prior knowledge re monotonicity https://t.co/jOHysLqt00
918161822275993600,Wed Oct 11 17:10:11 +0000 2017,Competitive Self-Play: Self-play allows agents to discover a wide range of skills and learn transferable policies https://t.co/X6UNTlIXll
918135563814948900,Wed Oct 11 15:25:51 +0000 2017,@radekosmulski :) Glad you're finding it helpful.
917380708221497300,Mon Oct 09 13:26:19 +0000 2017,RT @dublinAI: Next DUBLIN AI, 1st Nov at Dogpatch!  Talks inc Google acquisition of GreenParrotPictures (+ Voice, NLP)  RSVP&gt; https://t.co/‚Ä¶
917361322412707800,Mon Oct 09 12:09:17 +0000 2017,@alxcnwy Not sure. I'll share the links if they are; otherwise I'll share the slides.
917336773998587900,Mon Oct 09 10:31:44 +0000 2017,I'll be giving talks on transfer learning for NLP on Thurs &amp; Fri this week at UCL &amp; Cambridge. Come say hi! Details: https://t.co/UHUkkDGixy
915530181959409700,Wed Oct 04 10:52:59 +0000 2017,@freddylecue @Accenture I'm getting "page not found" on both links. :o
915498024327053300,Wed Oct 04 08:45:12 +0000 2017,Teachable Machine -- great to see #machinelearning be made more accessible &amp; fun! That's how we get people excited! https://t.co/eZoto36pEc
914815236544372700,Mon Oct 02 11:32:03 +0000 2017,@nlothian Really appreciate the thanks, Nick! üòä
914800190263058400,Mon Oct 02 10:32:16 +0000 2017,New NLP Newsletter -- LaTeX in FB, ML glossary, Language of Hip Hop, VI reference, Faster LSTMs, Lego sets! #NLProc https://t.co/T5b4xmBThJ
914457028466217000,Sun Oct 01 11:48:40 +0000 2017,Variational Inference &amp; Deep Learning: A New Synthesis -- @dpkingma thesis w/ insights on VI, generative modeling https://t.co/dmO49jdvsL
914060555866034200,Sat Sep 30 09:33:13 +0000 2017,Floating-point arithmetic and evolution strategies allow for non-linear computation in deep linear neural networks https://t.co/shGxxMTVai
912802822181224400,Tue Sep 26 22:15:26 +0000 2017,@radekosmulski @jeremyphoward Thank you, Radek! üòä
911951391987585000,Sun Sep 24 13:52:09 +0000 2017,New blog post: Multi-Task Learning Objectives for #NLProc -- a collection of the most useful auxiliary tasks for NLP https://t.co/YurKAWVjUB
910823220366004200,Thu Sep 21 11:09:12 +0000 2017,@pymnpsbn @tarfandy @LiuQunMTtoDeath @AdaptCentre @DublinCityUni üéâ Woohoo! Way to go, Peyman! Congrats!
910126034808426500,Tue Sep 19 12:58:50 +0000 2017,RT @dirk_hovy: New blog post. Some thoughts on the future of #NLProc conferences given their rapid growth https://t.co/hWRvNETDl2
910115256055353300,Tue Sep 19 12:16:00 +0000 2017,A Tutorial on Deep Learning for Music Information Retrieval by @keunwoochoi @kchonyc https://t.co/oc9egIz6ue
910093167785136100,Tue Sep 19 10:48:14 +0000 2017,RT @BarackObama: Coding is important ‚Äì and fun. @CSforAll, thanks for your work to make sure every kid can compete in a high-tech, global e‚Ä¶
909800704449146900,Mon Sep 18 15:26:05 +0000 2017,@jimmycallin It's fairly recent. ;) Thanks for the appreciation! Glad you like it! :)
909795856081145900,Mon Sep 18 15:06:49 +0000 2017,New NLP News - Review of EMNLP 2017, Analyzing Bias, Google Brain AMA, DRAGNN, and AllenNLP https://t.co/78OKJPdbKh
909791609549725700,Mon Sep 18 14:49:57 +0000 2017,Did you miss #emnlp2017 last week? Check out my highlights of the conference including some cool new datasets, clus‚Ä¶ https://t.co/oqWLPEbUMJ
909693577953005600,Mon Sep 18 08:20:24 +0000 2017,Vincent van Houcke on state of unsupervised learning (Google Brain AMA): Predicting causal future works, e.g. LM https://t.co/u2vhxvS84r
908326958349602800,Thu Sep 14 13:49:57 +0000 2017,RT @_aylien: There's a prize for anyone who guesses what letter we're putting in our new office... https://t.co/IdknMzigNB
908302310853496800,Thu Sep 14 12:12:01 +0000 2017,MILABOT: A state-of-the-art chatbot that selects an appropriate response from an ensemble using RL https://t.co/s5nY4G0WM9
908254494873538600,Thu Sep 14 09:02:00 +0000 2017,A (206 pages) Brief Introduction to Machine Learning for Engineers https://t.co/bdXVzdPegx
908102240866410500,Wed Sep 13 22:57:00 +0000 2017,An Introduction to Learning to Optimize using Reinforcement Learning by @berkeley_ai https://t.co/5Ygkq34Pft
908082593752633300,Wed Sep 13 21:38:56 +0000 2017,@eriklindernoren Thanks for the cool repo! Great job! :)
908081506496761900,Wed Sep 13 21:34:37 +0000 2017,A repository of bare-bones Python implementations: From clustering &amp; logreg to GANs &amp; RL by @eriklindernoren https://t.co/zpCrGZMdyR
908074550688174100,Wed Sep 13 21:06:58 +0000 2017,RT @parsaghaffari: Hello world, we have a new website (and a lot of good news to follow!): https://t.co/SmIIeRu9YJ ‚Äì feedback is appreciate‚Ä¶
908046393335533600,Wed Sep 13 19:15:05 +0000 2017,RT @carlosgr_nlp: Last days to apply to PhD student vacancy at ERC-funded #nlproc project FASTPARSE. https://t.co/Em2bFGfDUj . Deadline Sep‚Ä¶
907904121780543500,Wed Sep 13 09:49:45 +0000 2017,Both events are open for the public. Say hi if you're around. :)
907904032945078300,Wed Sep 13 09:49:24 +0000 2017,I'll be giving talks on Transfer Learning for NLP at UCL &amp; Cambridge on October 12 &amp; 13  https://t.co/BM68xoWbSO https://t.co/9J94X4S3iV
907542607416479700,Tue Sep 12 09:53:13 +0000 2017,@lucie_nlp I'd love to read a summary post with some more elaborations. üòä
907277694873763800,Mon Sep 11 16:20:33 +0000 2017,RT @emnlp2017: #emnlp2017 closing ceremony.  Thank you all so much for coming and making it a great event! https://t.co/O5q6uH3Dux
907277390832980000,Mon Sep 11 16:19:21 +0000 2017,RT @SaschaTrippe: Very important reminder. #academia #arXiv https://t.co/c8n6dZL2it
906551116556628000,Sat Sep 09 16:13:23 +0000 2017,RT @gosainnn: Sorry for the mix up everyone! We'll be ready by 6:30 pm! https://t.co/T4mMVjWxTc
906547991149981700,Sat Sep 09 16:00:58 +0000 2017,@gosainnn @emnlp2017 It says 6 pm on the web page, so some people might be confused. :)
906523397617840100,Sat Sep 09 14:23:15 +0000 2017,RT @_aylien: Our third scientist presenting at #emnlp2017 this weekend! üôå @seb_ruder on Transfer Learning with @barbara_plank - https://t.c‚Ä¶
906430435282100200,Sat Sep 09 08:13:51 +0000 2017,No free lunch theorem in #nlproc in action at #emnlp2017 https://t.co/KU4vto6smp
906428272258506800,Sat Sep 09 08:05:15 +0000 2017,@NandoDF on learning to learn, learning to experiment, and learning from simulations at his #emnlp2017 keynote https://t.co/I9dojUL2uz
906409055165583400,Sat Sep 09 06:48:53 +0000 2017,Opening remarks #emnlp2017 stats: Rising numbers of submissions, Deep Learning again most popular keyword, most sub‚Ä¶ https://t.co/vaLt3VAnxk
906143986041147400,Fri Sep 08 13:15:36 +0000 2017,@soegaarducph on examples and objective functions for different types of cross-lingual embeddings at cross-lingual‚Ä¶ https://t.co/UL4fgDEzYa
906129843603066900,Fri Sep 08 12:19:24 +0000 2017,Ivan Vulic on goals of the cross-lingual word representations tutorial at #emnlp2017 https://t.co/Bu5pzNmrcf
905789534272704500,Thu Sep 07 13:47:08 +0000 2017,RT @_aylien: If you're at the New Frontiers in Summarization workshop, come talk to our research intern Demian at the poster session! (on n‚Ä¶
905364454656880600,Wed Sep 06 09:38:01 +0000 2017,@Tbeltramelli @emnlp2017 üëç
905344986132205600,Wed Sep 06 08:20:39 +0000 2017,Heading to @emnlp2017 today üõ´ Come say hi if you're in town.
904976654686187500,Tue Sep 05 07:57:02 +0000 2017,@_rockt Awesome, congrats!
904841458062155800,Mon Sep 04 22:59:49 +0000 2017,@ChrisLovesAI Awesome! Thanks for translating it. :)
904815131456147500,Mon Sep 04 21:15:12 +0000 2017,New NLP Newsletter - Resources for learning NLP, advances in automatic speech recognition, language modelling and MT https://t.co/d5ZpNF1LPI
904784575196483600,Mon Sep 04 19:13:47 +0000 2017,@ChrisLovesAI Awesome! Just linked to it. :)
904710945494233100,Mon Sep 04 14:21:12 +0000 2017,RT @_aylien: Using Neural Machine Translation to unify two #NLProc tasks, by our own @Mystical_Wiz #DeepLearning  #EMNLP2017 https://t.co/D‚Ä¶
904052461396250600,Sat Sep 02 18:44:37 +0000 2017,Lots of resources for learning about #NLProc in the next NLP Newsletter. Subscribe to get it in your inbox on Monday https://t.co/0h8zWma2dl
904043057615646700,Sat Sep 02 18:07:15 +0000 2017,@abhishek3195 Probably the best thing is to train a retrieval model on SO data to recommend relevant threads/responses.
904042838815592400,Sat Sep 02 18:06:23 +0000 2017,@abhishek3195 If you want something that really works that's way harder and I can't think of a current approach that comes close.
904042547793813500,Sat Sep 02 18:05:14 +0000 2017,@abhishek3195 Depends on your goal. If you want to build something for fun or to show off, train seq2seq on stack overflow data.
904005755635949600,Sat Sep 02 15:39:02 +0000 2017,@ChrisLovesAI Hey Chris, sure. Feel free. Could you let me know if you've finished a translation? Would love to link to it. üòä
903619901251870700,Fri Sep 01 14:05:47 +0000 2017,@microth Haha, the part about using AI to fight against Aliens probably just didn't make it into the video.
903619460757639200,Fri Sep 01 14:04:02 +0000 2017,@tarantulae Totally! There's so much meme material in here.
903619360769630200,Fri Sep 01 14:03:38 +0000 2017,@joabingel Haha, better jump on the bandwagon now before all stock video sites will have SGD optimizations.
903613293457154000,Fri Sep 01 13:39:32 +0000 2017,@joabingel Haha, the SGD part of the video reminded me of the promo video where they filmed your screen. üòÅ
903598678111395800,Fri Sep 01 12:41:27 +0000 2017,Yes, this is totally how the life of an AI researcher looks like. https://t.co/YpeBlpTUQb https://t.co/TIHNVMgFNs
903560061104115700,Fri Sep 01 10:08:00 +0000 2017,Neural Nets for Generating Music: Fantastic review of the history of Deep Learning for Music Generation by @kcimc  https://t.co/B7RI6tsd3i
903529117836042200,Fri Sep 01 08:05:03 +0000 2017,@jeremyphoward @deliprao @MarekRei With hard parameter sharing, it is often hard to know whether an aux NLP task wi‚Ä¶ https://t.co/h03FgCh8lM
903528263401824300,Fri Sep 01 08:01:39 +0000 2017,@jeremyphoward @deliprao To add to the discussion, @MarekRei proposed exactly this (an auxiliary LM objective) in a‚Ä¶ https://t.co/axEP89d81G
903390695406280700,Thu Aug 31 22:55:00 +0000 2017,How I replicated an $86 million project in 57 lines of code: DIY license plate scanning with Deep Learning https://t.co/71i08HfZXz
903376655049261000,Thu Aug 31 21:59:13 +0000 2017,Do We Need a Speedometer for Artificial Intelligence? Informative @WIRED article on the AI Index project‚Ä¶ https://t.co/S0grAXz31g
902996940341874700,Wed Aug 30 20:50:22 +0000 2017,Background removal with Deep Learning: A detailed walkthrough of tackling challenges of a new use case with DL‚Ä¶ https://t.co/B6bAQNNkU3
902526393090805800,Tue Aug 29 13:40:34 +0000 2017,@elikiper @yoavgo Just sent you an email. :)
902521434429411300,Tue Aug 29 13:20:52 +0000 2017,@yoavgo Hey Yoav, I was wondering if you're able to share the Stanford dependencies version of PTB you used for https://t.co/F2W89BrI0S ?
902496823889326100,Tue Aug 29 11:43:05 +0000 2017,Deep Learning for Video Game Playing: Nice review of recent techniques by @risi1979 @togelius‚Ä¶ https://t.co/4tIU5eAf4y
902193774432051200,Mon Aug 28 15:38:52 +0000 2017,@ireneyu05 Let's talk more via PM. I've followed you. You also have to follow me for us to talk.
902193512392863700,Mon Aug 28 15:37:49 +0000 2017,@Tbeltramelli @teamrework üòÄ I'll also be in Copenhagen in September (for EMNLP), remember? üòâ
902178631337861100,Mon Aug 28 14:38:42 +0000 2017,@Tbeltramelli @teamrework Awesome! Congrats, Tony!
902172269950197800,Mon Aug 28 14:13:25 +0000 2017,RT @_aylien: New research: our research intern Demian's #EMNLP workshop paper on multi-document summarization #NLProc https://t.co/MuYEdRE7‚Ä¶
900474730997837800,Wed Aug 23 21:48:00 +0000 2017,@irenetrampoline Thanks, Irene!
900285700444221400,Wed Aug 23 09:16:52 +0000 2017,@elikiper @yoavgo I'd like to run your BIST parser https://t.co/F2W89BrI0S on PTB (SD format) but couldn't find it. Is it possible to share?
899928284556927000,Tue Aug 22 09:36:37 +0000 2017,@iamtrask üëçüèª
899923749784490000,Tue Aug 22 09:18:36 +0000 2017,RT @_aylien: The latest bi-monthly summary of #NLProc news &amp; research by @seb_ruder was published last night - take a look! https://t.co/i3‚Ä¶
899923602912555000,Tue Aug 22 09:18:01 +0000 2017,@iamtrask Awesome! Congrats! Still doing a PhD at the same time?
899915215269240800,Tue Aug 22 08:44:41 +0000 2017,@dennybritz @jackclarkSF @Smerity I also enjoy surveying stuff, so let me know if you're up to something in this space.
899762856362598400,Mon Aug 21 22:39:16 +0000 2017,NLP News #4 - Data selection, ML &amp; NLP in Esports, VQA, bias &amp; lyric annotations https://t.co/HiVOHPXjA4
898989439590035500,Sat Aug 19 19:25:59 +0000 2017,RT @_aylien: Chat with 3 #chatbots from the 1960s to 2016 and see how #AI has progressed. https://t.co/H3ZiycJDgB https://t.co/e4MQm9PYQB
897820628014817300,Wed Aug 16 14:01:33 +0000 2017,@yoavgo üëçüèª
897818225332940800,Wed Aug 16 13:52:00 +0000 2017,@yoavgo Thanks for the prompt response and the pointers! I'll try to copy-paste the C++ code then. :)
897813401988206600,Wed Aug 16 13:32:50 +0000 2017,@yoavgo Hey Yoav, is it possible to modify the LSTM *cell* in DyNet, e.g. adding new connections in Python w/o delving into the C++ code?
897082039916253200,Mon Aug 14 13:06:39 +0000 2017,@iiacobac https://t.co/y6GldoqPRx https://t.co/cuvnbO5mEU
895559113073205200,Thu Aug 10 08:15:05 +0000 2017,@EmilStenstrom Thanks for the pointer, I'll try to have a look.
895544496800370700,Thu Aug 10 07:17:01 +0000 2017,@yaringal @UniofOxford @CompSciOxford Congrats, Yarin!
895396914245636100,Wed Aug 09 21:30:34 +0000 2017,@TerryUm_ML @UWaterloo Congrats!
895396001808306200,Wed Aug 09 21:26:57 +0000 2017,@EmilStenstrom Thanks!
895390226444628000,Wed Aug 09 21:04:00 +0000 2017,@EmilStenstrom Yep, I'll have a look. Any pointers what's the best way to fix that?
895387031559393300,Wed Aug 09 20:51:18 +0000 2017,@EmilStenstrom Are you using Firefox? Someone using Firefox said the same thing before. It's looking ok on Chrome for me.. üòì
895320666387365900,Wed Aug 09 16:27:35 +0000 2017,RT @_aylien: A summary of 'Learning to Select Data for Transfer Learning' by our research scientist, @seb_ruder #machinelearning https://t.‚Ä¶
894694617286930400,Mon Aug 07 22:59:54 +0000 2017,The 3rd edition of NLP News is out w/ a look at Pytorch, awesome posters, cool implementations, NLP competitions etc https://t.co/Yd2AMLLkaI
893503288779440100,Fri Aug 04 16:05:59 +0000 2017,RT @explosion_ai: Our annotation tool is finally here! ‚ú®   Introducing Prodigy ‚Äì a new tool for radically efficient machine teaching. https‚Ä¶
892844958817935400,Wed Aug 02 20:30:01 +0000 2017,On Chomsky and the Two Cultures of Statistical Learning: An illuminating post from 2011 by Peter Norvig https://t.co/ZJgN793DHf
892807220852904000,Wed Aug 02 18:00:03 +0000 2017,A 2017 Guide to Semantic Segmentation with Deep Learning - a nice overview of semantic segmentation by @qure_ai https://t.co/bc5M0npYha
892706256670085100,Wed Aug 02 11:18:51 +0000 2017,RT @_aylien: The newsletter curated by @seb_ruder is well worth signing up for - the best #NLProc articles &amp; papers every 2 weeks https://t‚Ä¶
891219362425843700,Sat Jul 29 08:50:28 +0000 2017,@RadimRehurek Where are you moving to, Radim?
890641743401500700,Thu Jul 27 18:35:13 +0000 2017,@patrickthealba Yep, it's a pretty good analysis. :) Cool, so I know now how to get your retweets. "bias", "ai", "racist", "jupyter" ;)
890641291054161900,Thu Jul 27 18:33:25 +0000 2017,@ehfo0 Hey Hoda, thanks! Adam should look pretty similar to RMSprop for this example.
890600859259633700,Thu Jul 27 15:52:46 +0000 2017,RT @harvardnlp: New work (@emnlp2017) on difficulty of neural document generation https://t.co/mN2xTK5WUB. Dataset of basketball stats-game‚Ä¶
890597243874660400,Thu Jul 27 15:38:24 +0000 2017,How to make a racist AI without even trying: A cool Jupyter notebook on understanding biases in our models https://t.co/ascABuuaPz
890499366879981600,Thu Jul 27 09:09:28 +0000 2017,@UKPLab Thanks! Cool paper! Looking forward to seeing some of you at @emnlp2017! :)
890351888843100200,Wed Jul 26 23:23:26 +0000 2017,"Reading Wikipedia to Answer Open-Domain Questions" -- Paper &amp; implementation by @danqi_chen https://t.co/bBBiZ7t3RS https://t.co/e0QumWgMRB
890258642485641200,Wed Jul 26 17:12:55 +0000 2017,@jrbtaylor Thank you, Jason! I appreciate it!
890201622445514800,Wed Jul 26 13:26:20 +0000 2017,@eoinhurrell @eoinbrazil Thanks, Eoin! :)
890191693399691300,Wed Jul 26 12:46:53 +0000 2017,@parklize Cool! :)
890164639279009800,Wed Jul 26 10:59:23 +0000 2017,@stevelizcano Haha, I'll hope we get there sooner or later. A collection of plug &amp; play NLP best practices &amp; design patterns. :)
890145881277726700,Wed Jul 26 09:44:50 +0000 2017,@balajinix üëçüèª Let me know your thoughts when you're done. :)
890145753154408400,Wed Jul 26 09:44:20 +0000 2017,@mgalle @RadimRehurek Thanks for the pointer, Matthias! :)
890132023574241300,Wed Jul 26 08:49:46 +0000 2017,@mgalle @RadimRehurek Good point. I'll add QA over the next days. To me, it seemed memory is mostly used for QA or‚Ä¶ https://t.co/qvTSALcUPN
890122458950402000,Wed Jul 26 08:11:46 +0000 2017,@stevelizcano You're totally right. I confused the two. Fixed now. :)
890122313991163900,Wed Jul 26 08:11:11 +0000 2017,RT @dennybritz: Deep Learning for NLP Best Practices https://t.co/dgN9GgpSgI  Excellent guide by @seb_ruder!
890119922025672700,Wed Jul 26 08:01:41 +0000 2017,@alexott_en Hey Alex! Thanks for the note! It's not empty for me: https://t.co/Wuz7s79K2g What am I missing?
890110048298557400,Wed Jul 26 07:22:27 +0000 2017,RT @RadimRehurek: Collection of "Deep Learning for #NLP Best Practices" by @sebastianruder: https://t.co/fWk4tdPLXy  I like its mission sta‚Ä¶
889982012811096000,Tue Jul 25 22:53:41 +0000 2017,New blog post: Deep Learning for #NLProc Best Practices -- a collection of best practices for applying NNs to NLP https://t.co/4TSQkBydSK
889868770298261500,Tue Jul 25 15:23:42 +0000 2017,RT @PMinervini: Our @uai2017 paper on Adversarial Sets for injecting prior knowledge in neural link predictors https://t.co/djUnW6u7Z2 @_ro‚Ä¶
889817611806134300,Tue Jul 25 12:00:25 +0000 2017,RT @TsukAfonso: Join me in following this newsletter? üëÄ NLP News by @seb_ruder https://t.co/tyf1vScXyL via @revue
889762618302484500,Tue Jul 25 08:21:53 +0000 2017,How to improve beam search, censorship, generating recipes from images, grounded representations, and more #NLProc  https://t.co/5S3kBUFJlE
889432475763691500,Mon Jul 24 10:30:01 +0000 2017,Issue #2 of NLP News - NMT, beam search, &amp; food is out! Get it while it's hot! üî• https://t.co/6kLIlLQub6
889419734818713600,Mon Jul 24 09:39:24 +0000 2017,An Interactive History of #chatbots: Learn about (and interact with!) ELIZA, ALICE, and a neural seq2seq model. https://t.co/d4PLkMshSy
888738968199737300,Sat Jul 22 12:34:16 +0000 2017,10,000 Hours with Claude Shannon: 12 Lessons we can learn from a genius on how to think, work, and live https://t.co/sK9A5ybJEr
888709008533618700,Sat Jul 22 10:35:13 +0000 2017,RT @goodfellow_ian: @hugo_larochelle @m_aggrey @ylecun  https://t.co/0jm9IcYtXE
888506328024526800,Fri Jul 21 21:09:50 +0000 2017,Superintelligence - The Idea That Eats Smart People: Such a great analysis of the fear of AI and its pros and cons https://t.co/D8XLvmxgE0
888469520020623400,Fri Jul 21 18:43:35 +0000 2017,@KieranSnyder The following are some interesting approaches: https://t.co/BIyWePyqdn, https://t.co/mgTa1PnYMj, https://t.co/SQyX4pJnBD
888334583946776600,Fri Jul 21 09:47:23 +0000 2017,RT @NLPDublin: Slides for our talks on human evaluation, MT in translation workflows, and universal dependencies are now online https://t.c‚Ä¶
888081551766040600,Thu Jul 20 17:01:56 +0000 2017,RT @dennybritz: Proud to be an advisor at @OpenEdAI's AI for Education HackWeek - $17k in prizes from @gdg @IBM @AWS_edu. Sign up: https://‚Ä¶
887667221929054200,Wed Jul 19 13:35:32 +0000 2017,RT @_aylien: New research from one of our research scientists, @seb_ruder, and @barbara_plank on Transfer Learning for #NLProc https://t.co‚Ä¶
887645895692562400,Wed Jul 19 12:10:47 +0000 2017,RT @NathanBenaich: Just published issue #20 of my AI newsletter. Few words in there on what's happening these days... https://t.co/9SxVEgdT‚Ä¶
887371399781691400,Tue Jul 18 18:00:02 +0000 2017,Code for our multi-task learning model Sluice Networks w/ @joabingel @IAugenstein @soegaarducph is now online‚Ä¶ https://t.co/DGbLNLMmnQ
887222413737459700,Tue Jul 18 08:08:01 +0000 2017,Learning to select data for transfer learning with Bayesian Optimization w/ @barbara_plank @emnlp2017 is now online https://t.co/vrCkHNJmis
887063755653251100,Mon Jul 17 21:37:34 +0000 2017,RT @CParraEsc: Thanks a lot for having us tonight at the @NLPDublin meetup! It was a great evening! https://t.co/IpoQwmFzzJ
887027086338248700,Mon Jul 17 19:11:52 +0000 2017,RT @15Cl9H14C: Such responsibility + passion  required as a meetup organiser #NaturalLanguageProcessing @NLPDublin @Workday @seb_ruder #the‚Ä¶
887026308718506000,Mon Jul 17 19:08:46 +0000 2017,RT @NLPDublin: "True self-control is waiting until the movie starts to eat your popcorn." True self-control is sneaky.. Syntactic ambiguiti‚Ä¶
887014598871834600,Mon Jul 17 18:22:14 +0000 2017,RT @NLPDublin: @CParraEsc talking about how we can use machine translation to improve the workflow of translators at tonight's #NLPDublin m‚Ä¶
887006083428098000,Mon Jul 17 17:48:24 +0000 2017,RT @NLPDublin: Do you want to evaluate your MT system with humans? This flow chart by Sheila Castilho will tell you what you should care ab‚Ä¶
886898538734551000,Mon Jul 17 10:41:04 +0000 2017,RT @NLPDublin: As of today, we have surpassed 500 members in our #NLProc group! Great to have such a vibrant community in Dublin! https://t‚Ä¶
886897886889488400,Mon Jul 17 10:38:28 +0000 2017,RT @NLPDublin: Join us for an awesome #NLProc event tonight with @cigilt, @CParraEsc, and Dr. Sheila Castilho tonight at @Workday https://t‚Ä¶
886209463539073000,Sat Jul 15 13:02:55 +0000 2017,@afshinmeh Cool stuff, Afshin!
885898180705095700,Fri Jul 14 16:26:00 +0000 2017,@joanfihu Yeah, good point. I've taken a look at CF but would like to compare AMT and CF a bit more, particularly s‚Ä¶ https://t.co/OoHq8Gc562
885897761366962200,Fri Jul 14 16:24:20 +0000 2017,@mlagunas_ Thanks for the reference! Will take a look. :)
885844315678945300,Fri Jul 14 12:51:57 +0000 2017,Does anyone have advice / pointers on how to set up qualification tests / other quality control for Amazon Mechanical Turk? #nlproc
885758161466130400,Fri Jul 14 07:09:36 +0000 2017,Awesome professional-looking photos generated from Google StreetView panoramas using GANs https://t.co/Nk75SVAIHt‚Ä¶ https://t.co/qK3gFi0X7R
885600919471231000,Thu Jul 13 20:44:47 +0000 2017,@gneubig Awesome! Are you going to have some at @emnlp2017 too? üòÄ
884860238515621900,Tue Jul 11 19:41:35 +0000 2017,@xxplusdata Thanks! üòä
884771800131010600,Tue Jul 11 13:50:10 +0000 2017,@DashingD3js @revue Ah, thanks for the pointer! Fixed those links now. :)
884765007589888000,Tue Jul 11 13:23:10 +0000 2017,@DashingD3js @revue Thanks for the shout-out! üòç Small correction: The site is https://t.co/0h8zWma2dl.
884765007589888000,Tue Jul 11 13:23:10 +0000 2017,@DashingD3js @revue Thanks for the shout-out! üòç Small correction: The site is https://t.co/0h8zWma2dl.
884699063123300400,Tue Jul 11 09:01:08 +0000 2017,@NeillGernon Thanks! :)
884690475453739000,Tue Jul 11 08:27:00 +0000 2017,@statsforbios üëçüèª I'll put it here on Twitter as well, so you should also be able to see it here then. :)
884687941917913100,Tue Jul 11 08:16:56 +0000 2017,@statsforbios Just working on finalizing the pdf. Should be out at the end of this week / early next week.
884687799802318800,Tue Jul 11 08:16:22 +0000 2017,@statsforbios Thanks a lot! Really glad you enjoyed it. :)
884531005280878600,Mon Jul 10 21:53:20 +0000 2017,RT @NathanBenaich: Terrific resource by @seb_ruder üíØ Thanks for the shout out üôèüèº https://t.co/i47AoZkmfY
884495461960351700,Mon Jul 10 19:32:05 +0000 2017,RT @koepermx: Nice new (first?) newsletter for NLP:  https://t.co/ieOwxdxmwZ  from @seb_ruder, subscribe :)
884490770371293200,Mon Jul 10 19:13:27 +0000 2017,Amazing results of the first Google Brain Residency class: 15 ICLR, 11 ICML, 3 CVPR, 2 EMNLP papers, and much more üëè https://t.co/06iBaAQysn
884488561441079300,Mon Jul 10 19:04:40 +0000 2017,üî• Hot off the press: First edition of ‚ÄúNLP News - NLP for beginners, dialogue &amp; sentence representations‚Äù https://t.co/9DQ8Vk3Bqo
884415171384488000,Mon Jul 10 14:13:03 +0000 2017,RT @iatitov: PhD position in deep learning for NLP (affiliated with Uni Amsterdam but hosted @EdinburghNLP), deadline July 31: https://t.co‚Ä¶
884008270314299400,Sun Jul 09 11:16:10 +0000 2017,RT @honnibal: After yesterday's debate, was asked to answer why I think @coastalcph PhD positions are so good.   https://t.co/gJ84nfxAgu
883326741112131600,Fri Jul 07 14:08:01 +0000 2017,RT @honnibal: Very good #NLProc PhD opening in Copenhagen! Much better offer than U.S. PhDs: shorter program, awesome city, top class resea‚Ä¶
883307933580140500,Fri Jul 07 12:53:17 +0000 2017,@manaalfar @jasonbaldridge Cool #NLProc application! My favourite example: "cocodrile" (from "cocodrilo" in Spanish).
883294571039477800,Fri Jul 07 12:00:11 +0000 2017,Open PhD position in Copenhagen in transfer &amp; multi-task learning, one of the hottest areas of #NLProc/‚Ä¶ https://t.co/7S9Jq7flIp
883220042153947100,Fri Jul 07 07:04:02 +0000 2017,RT @parsaghaffari: By the looks of it we have been working on the wrong NLP all along https://t.co/zpVrf20HAs
883068946072645600,Thu Jul 06 21:03:38 +0000 2017,X Degrees of Separation for Art: Awesome art experiment by @quasimondo and @Google https://t.co/MlqhG3WuZE #machinelearning
883036832283471900,Thu Jul 06 18:56:01 +0000 2017,An interesting study on how Silicon Valley best practices may get lost in translation by @StanfordEng https://t.co/4pdWaoJ25n
882925286127435800,Thu Jul 06 11:32:46 +0000 2017,@PMinervini, just saw your new arXiv paper on link prediction. Looking forward to reading it! :) https://t.co/pwMBqIMZJA
882714785242120200,Wed Jul 05 21:36:19 +0000 2017,Interested in #NLProc or #MachineLearning research? You now have the chance to join one of the most awesome #NLProc‚Ä¶ https://t.co/FDtOGRdxZX
882663368913498100,Wed Jul 05 18:12:00 +0000 2017,Enlightening slides on word2vec by @yoavgo: Focus on a specific task, inputs &gt;&gt; fancy math https://t.co/U80BPfqnz0 #nlproc
882377734432927700,Tue Jul 04 23:17:00 +0000 2017,@kshitij_sri Thanks! Really appreciate it! üòä
882336465165754400,Tue Jul 04 20:33:00 +0000 2017,Interpretability in Neural Networks: Interpreting neurons in an LSTM used for transliteration https://t.co/wRwHma125G #NLProc
882311551755182100,Tue Jul 04 18:54:01 +0000 2017,Talk on Rules of #MachineLearning: Getting Big Practical Gains by Avoiding Common Pitfalls via @rzykov‚Ä¶ https://t.co/gCGmOlJxWO
882170115420291100,Tue Jul 04 09:32:00 +0000 2017,@LeonDerczynski @soegaarducph Cool! Hope to play around with the data in the future! :)
882139642325327900,Tue Jul 04 07:30:54 +0000 2017,@lmthang @MelodyGuan @dennybritz Cool, congrats!
882000754483114000,Mon Jul 03 22:19:01 +0000 2017,Tackling new problems, eg multi-task/transfer/zero-shot learning &amp; unintuitive properties of NNs by @hugo_larochelle https://t.co/E5G9bHhiZw
881963001934340100,Mon Jul 03 19:49:00 +0000 2017,Good resource with pros and cons of doing a #MachineLearning PhD by @80000Hours https://t.co/1UhHkp2BGd
881682656089391100,Mon Jul 03 01:15:00 +0000 2017,Hardware motivates software: Today's AI chips require us to take our algos from exploiting parallelism to locality https://t.co/KUIz09VXa2
881611687895236600,Sun Jul 02 20:33:00 +0000 2017,Performance RNN by @googleresearch Magenta generates expressive timing &amp; dynamics for music pieces https://t.co/kXlljNfXm4
881580964471869400,Sun Jul 02 18:30:55 +0000 2017,RT @_aylien: Our research scientist @seb_ruder on word embeddings and their connection to distributional semantic models https://t.co/hKc8I‚Ä¶
881571424548393000,Sun Jul 02 17:53:01 +0000 2017,Beautiful article on A Path Less Taken to the Peak of the Math World https://t.co/UuVCfpb9ZJ
881219364325863400,Sat Jul 01 18:34:03 +0000 2017,@PMinervini @barbara_plank @emnlp2017 Both go in a similar direction. We try to learn what data is relevant and thu‚Ä¶ https://t.co/ldRbuOMwhs
881178836074876900,Sat Jul 01 15:53:00 +0000 2017,Happy to announce my paper w/ @barbara_plank on Learning to select data for transfer learning got accepted to @emnlp2017. Camera-ready soon.
881087970048127000,Sat Jul 01 09:51:56 +0000 2017,RT @barbara_plank: NLP challenge: #variability #pydataberlin https://t.co/pmpY5yuEvf
880817820128469000,Fri Jun 30 15:58:27 +0000 2017,RT @CParraEsc: Looking forward to talk about human factors in Machine Translation at the next #NLPDublin meetup on July 17 with awesome @ci‚Ä¶
880771160241295400,Fri Jun 30 12:53:03 +0000 2017,Join us for an awesome event on July 17 with 3 amazing speakers on human evaluation, linguistics, and human-compute‚Ä¶ https://t.co/FLfahb50OF
880734685227348000,Fri Jun 30 10:28:06 +0000 2017,@headlighty @AdaptCentre @EDGEFellowships Congrats Annalina!
880517471173840900,Thu Jun 29 20:04:59 +0000 2017,@nsaphra Congrats, Naomi!
880076317487747100,Wed Jun 28 14:51:59 +0000 2017,RT @_aylien: Blog: ‚ÄòModeling Documents with Generative Adversarial Networks‚Äô by our VP of Science @anotherjohng #machinelearning https://t.‚Ä¶
879738402056400900,Tue Jun 27 16:29:14 +0000 2017,@nlpmattg Hey Matt, thanks for adding #nlphighlights to @Stitcher. Your podcast will be binge listened to over the next couple of days. üòÅ
879379004893134800,Mon Jun 26 16:41:07 +0000 2017,The stack and architecture of @SiliconHBO's Not Hotdog app by @timanglade: mobile TF, Keras,React Native https://t.co/2sSO1I7BkO @timanglade
879325103770742800,Mon Jun 26 13:06:56 +0000 2017,RT @insight_centre: New Scientist spoke to @seb_ruder about Google's latest breakthrough https://t.co/O5cYteLPvO @ucddublin @DublinCityUni‚Ä¶
878192762335703000,Fri Jun 23 10:07:25 +0000 2017,RT @NLPDublin: Slides of last night's talks are now online! Thanks to our speakers @heerme and @dublin_io for the brilliant talks! https://‚Ä¶
877961551197556700,Thu Jun 22 18:48:40 +0000 2017,RT @NLPDublin: Adrian Mihai of Opening talking about how to leverage #NLProc for #HR at tonight's #NLPDublin meetup hosted by @NitroHQ #HRt‚Ä¶
877949401754480600,Thu Jun 22 18:00:23 +0000 2017,RT @NLPDublin: Georgiana Ifrim talking about real-time tagging of streaming news at the 7th #NLPDublin meetup hosted by @NitroHQ #NLProc ht‚Ä¶
877883231458140200,Thu Jun 22 13:37:27 +0000 2017,RT @NLPDublin: Join us for our 7th #NLPDublin meetup tonight at @NitroHQ with talks by @heerme and @amorroxic https://t.co/CoGI53qF60 https‚Ä¶
877545818936496100,Wed Jun 21 15:16:41 +0000 2017,@cigilt üòÄüéâ
877545049743126500,Wed Jun 21 15:13:38 +0000 2017,@cigilt Awesome! Will start using this! üòÅ
877544796071616500,Wed Jun 21 15:12:38 +0000 2017,RT @iamaidang: Learning across domains: MultiModel and the significance of multi-task learning: https://t.co/NF5qXlMLRO
877535307700240400,Wed Jun 21 14:34:55 +0000 2017,Two new large-scale video datasets w/ 250k+ labeled videos of basic actions and dynamic hand gestures by @twentybn‚Ä¶ https://t.co/2chsbIdyV8
877176990960431100,Tue Jun 20 14:51:06 +0000 2017,RT @IAugenstein: Hiring: postdoc in #NLProc #ML @uni_copenhagen @lowlandscph for 1+ years, project-independent, flexible topic &amp; start date‚Ä¶
876931644434690000,Mon Jun 19 22:36:11 +0000 2017,Google open-sources Tensor2Tensor library, library for running many SotA models. Supports modular multitask learning https://t.co/77MEUfhIoP
876826501743874000,Mon Jun 19 15:38:23 +0000 2017,@IAugenstein @uni_copenhagen @lowlandscph Congrats, Isabelle! Well deserved!
876826397150502900,Mon Jun 19 15:37:58 +0000 2017,RT @IAugenstein: I'm starting as tenure-track assistant prof @uni_copenhagen - absolutely thrilled to join @lowlandscph! Visit me at #emnlp‚Ä¶
876806687897813000,Mon Jun 19 14:19:39 +0000 2017,RT @joabingel: Heads up #ml and #nlproc folks, join our #summerschool on graphical models on south Swedish island mid-August. https://t.co/‚Ä¶
876750386488635400,Mon Jun 19 10:35:56 +0000 2017,One Model To Learn Them All: Multi-task learning with multimodal tasks. Slight transfer between ImageNet &amp; parsing. https://t.co/zoWTxjpiFN
875819708481753100,Fri Jun 16 20:57:45 +0000 2017,RT @b_niranjan: Super useful survey on cross-lingual embeddings. The original blog post version here: https://t.co/b5kAam6un2. Thanks @seb_‚Ä¶
875686693386559500,Fri Jun 16 12:09:11 +0000 2017,@uclmr Really cool! Congrats! :D
875654538669178900,Fri Jun 16 10:01:25 +0000 2017,@fisadev Thanks, Juan! :)
875457891108495400,Thu Jun 15 21:00:01 +0000 2017,For everyone interested in optimization with SGD, I updated my blog post with descriptions of AdaMax and Nadam https://t.co/FbRUiPZcFl
875418378923458600,Thu Jun 15 18:23:00 +0000 2017,ICML 2017 paper and blog post on Schema Networks, achieves zero-shot transfer on Breakout  https://t.co/l57ZXFya0F‚Ä¶ https://t.co/eWVIQswNNm
874999328087126000,Wed Jun 14 14:37:51 +0000 2017,@Miles_Brundage @dennybritz @80000Hours Very cool report, Miles! Thanks for this! I'm sure it will be helpful to ma‚Ä¶ https://t.co/dkHL5RSfM6
874650199431446500,Tue Jun 13 15:30:32 +0000 2017,@nlpmattg @_rockt @riedelcastro @Stitcher Cool. Thanks a lot, Matt! :)
874647773773213700,Tue Jun 13 15:20:54 +0000 2017,@PMinervini @_rockt @thomeestr @riedelcastro @uai2017 Ditto. :) I'll try to give a talk at the South England NLP me‚Ä¶ https://t.co/w3HeXqNnOQ
874642651089383400,Tue Jun 13 15:00:32 +0000 2017,@_rockt @PMinervini @thomeestr @riedelcastro @uai2017 Unfortunately not. Hope I can visit you at UCL / Oxford sometime, though. :)
874641790036578300,Tue Jun 13 14:57:07 +0000 2017,@_rockt @PMinervini @thomeestr @riedelcastro @uai2017 Cool! Congrats!
874641520011481100,Tue Jun 13 14:56:03 +0000 2017,@nlpmattg @_rockt @riedelcastro @Stitcher Thanks! Would be awesome for us with Android devices.
874568640254836700,Tue Jun 13 10:06:27 +0000 2017,@nlpmattg @_rockt @riedelcastro Hey Matt, awesome podcast! Do you plan to make it available on platforms such as‚Ä¶ https://t.co/XhZ6nXPbWW
874387604753526800,Mon Jun 12 22:07:04 +0000 2017,@deliprao @PyTorch @joostware @OReillyAI üëçüèª
874372296772902900,Mon Jun 12 21:06:15 +0000 2017,@deliprao @PyTorch @joostware @OReillyAI Awesome!! Slides won't be made available I assume? ü§î
872883930114003000,Thu Jun 08 18:32:00 +0000 2017,The Blissful Ignorance of the Narrative Fallacy: how we create narratives to explain our data by @stitchfix_algo https://t.co/PUf23ilXge
872812974796361700,Thu Jun 08 13:50:03 +0000 2017,@gerasimoss Haha :D
872768939453018100,Thu Jun 08 10:55:05 +0000 2017,RT @parsaghaffari: We're looking for an amazing office admin in Dublin to keep our office a fun, positive and productive environment: https‚Ä¶
872701263183577100,Thu Jun 08 06:26:09 +0000 2017,RT @alexip: All you ever wanted to know about word embeddings On word embeddings part 1 by @seb_ruder https://t.co/Pr0RooteU6 https://t.co/‚Ä¶
872559795953336300,Wed Jun 07 21:04:01 +0000 2017,A Year of Google &amp; Apple Maps: Google Maps‚Äôs Quiet Transformation, Apple Maps‚Äòs ______? https://t.co/Td6LfSqMLD
872527835365343200,Wed Jun 07 18:57:01 +0000 2017,Paul Buchheit on Lessons Learned from Investing in 200+ Startups via @ycombinator https://t.co/wxc3BxfVcK
872376577300070400,Wed Jun 07 08:55:58 +0000 2017,@eltimster Cool! What particular NLP applications are you looking at? Biomedical text mining?
872357204254502900,Wed Jun 07 07:38:59 +0000 2017,RT @gcosma1: A must read! An Overview of Multi-Task Learning in Deep Neural Networks  https://t.co/KHZPbJgDGL #DeepLearning  #transfer lear‚Ä¶
872221312260984800,Tue Jun 06 22:39:00 +0000 2017,Great, detailed blog post by @dustinvtran on how to get from research to engineering (and everything in between) https://t.co/0vEdkwVow7
872188094061195300,Tue Jun 06 20:27:00 +0000 2017,Insightful blog post by @ryanmdahl  (@nodejs creator) about his time as Brain resident &amp; lessons learned https://t.co/uxMSp1W1uo
872152864176586800,Tue Jun 06 18:07:01 +0000 2017,ScatterText: scatter plots for text analysis. Nice work by @jasonkessler on visualizing corpora in fun engaging ways https://t.co/3PDRCWTzKu
872096959150084100,Tue Jun 06 14:24:52 +0000 2017,@jsdelfino Thanks, Sebastien! Glad you like it!
870990085357006800,Sat Jun 03 13:06:33 +0000 2017,@williamyi96 Which idea in particular? MTL has been applied to RL in many cases.
870759663062720500,Fri Jun 02 21:50:56 +0000 2017,RT @brandalisms: Come to the next NLP Dublin talk if you want a technical insight into our work:  https://t.co/zCTMCGyadG @dublin_io Adrian‚Ä¶
870559203395588100,Fri Jun 02 08:34:22 +0000 2017,RT @NLPDublin: Join us at 7th #NLP Dublin meetup on June 22 hosted by @NitroHQ! Talks about realtime news tagging &amp; NLP for HR tech https:/‚Ä¶
870292058350735400,Thu Jun 01 14:52:50 +0000 2017,AlphaGo in context -- on the narrowness of AlphaGo and how well the approach generalizes beyond Go by @karpathy https://t.co/9spE3prCk4
869954512664047600,Wed May 31 16:31:33 +0000 2017,RT @lowlandscph: @seb_ruder presenting work on transfer learning in #NLProc https://t.co/1oGxCENpoH
869872875980951600,Wed May 31 11:07:09 +0000 2017,Why Should Drug Discovery Be Open Source? TL;DR so that not only most profitable diseases are researched by @rbhar90 https://t.co/QsoEwWsWJh
869584673818775600,Tue May 30 16:01:56 +0000 2017,RT @lowlandscph: Join us tomorrow for @seb_ruder's talk on transfer learning for #NLProc https://t.co/1ORY2Yfkic
869562821490733000,Tue May 30 14:35:06 +0000 2017,@iskander I'll DM you so we can talk more about this. :)
869562598777380900,Tue May 30 14:34:13 +0000 2017,@rbhar90 @hardmaru Thanks for the link! Will check it out! :)
869441688560861200,Tue May 30 06:33:46 +0000 2017,@YadFaeq @deontologician Cool! Thanks for the pointers! :D
869441270539837400,Tue May 30 06:32:06 +0000 2017,@iskander Interesting. Mini-batches didn't really hurt performance for me as long as distribution of tasks was simi‚Ä¶ https://t.co/MdNQ5082bI
869437785232617500,Tue May 30 06:18:16 +0000 2017,@deontologician @YadFaeq Cool! MTL/Transfer learning with RL is super exciting. Let me know if you'd like to chat sometime.
869304679703957500,Mon May 29 21:29:21 +0000 2017,@arthchan2003 Thanks, Arthur!
869304577992118300,Mon May 29 21:28:56 +0000 2017,@hardmaru Yes! Waxing a car is a very underrated skill for transfer learning. Helps you pay the GPU bill.
869236904641806300,Mon May 29 17:00:02 +0000 2017,New blog post on Multi-Task Learning in Deep Neural Networks https://t.co/DfNDRKYtxr https://t.co/7Zkbww3sWp
869196419063844900,Mon May 29 14:19:09 +0000 2017,@Tbeltramelli Awesome! üëè üéâ
867999989393502200,Fri May 26 07:04:58 +0000 2017,@Smerity These are awesome!!
867319642653675500,Wed May 24 10:01:31 +0000 2017,@barbara_plank @IAugenstein @joabingel @soegaarducph Yes. We'll have to clean it up and document it a bit more, so‚Ä¶ https://t.co/xl43u1YlIt
867312148598468600,Wed May 24 09:31:44 +0000 2017,@delliott @IAugenstein @joabingel @soegaarducph Yes. We haven't looked into what sampling probs for work best for M‚Ä¶ https://t.co/H4fhF2UK60
867307700207702000,Wed May 24 09:14:04 +0000 2017,RT @IAugenstein: New paper on learning what to share in multi-task learning https://t.co/kdTTsj50yp @seb_ruder @joabingel @soegaarducph #dl‚Ä¶
866684691809742800,Mon May 22 15:58:27 +0000 2017,Joel Tetrault from @Grammarly giving a talk on detecting good conversations in comment sections at the Copenhagen N‚Ä¶ https://t.co/rKeCCA48Hi
866025991520612400,Sat May 20 20:21:01 +0000 2017,TMW looking for a color palette for your graph leads you to the xkcd color survey and down the color rabbit hole... https://t.co/2TiatHfXGB
865981632410513400,Sat May 20 17:24:44 +0000 2017,Real-Time Adaptive Image Compression at @icmlconf 2017. Stunning results vs. baselines. Adaptivity FTW! https://t.co/b0TS7abmQg
865667604815335400,Fri May 19 20:36:54 +0000 2017,@pymnpsbn Congrats! Is a link to the paper available?
865521535615815700,Fri May 19 10:56:29 +0000 2017,RT @vlachos_nlp: Come work with us in Sheffield! 5 positions available, 2 of them in machine learning + robotics/neuroscience! https://t.co‚Ä¶
864815687197503500,Wed May 17 12:11:41 +0000 2017,RT @parsaghaffari: SFI Industry Fellowship 2017 is now open! For PhDs/Postdocs in Ireland looking for NLP/Deep Learning collaboration: http‚Ä¶
864137493016346600,Mon May 15 15:16:47 +0000 2017,@tanoross Cool! Enjoy! üòÄ
863341261939499000,Sat May 13 10:32:51 +0000 2017,RT @anotherjohng: So @elonmusk is now into both space flight and advanced drilling? ü§î https://t.co/paJtR6RL0U
863299029819678700,Sat May 13 07:45:02 +0000 2017,Are pop lyrics getting more repetitive? An exercise in language compression. Beautiful visualizations by @puddingviz https://t.co/nbKYE52i4G
862404039807565800,Wed May 10 20:28:40 +0000 2017,@_rockt @CompSciOxford @whi_rl Awesome! Best of luck, Tim!
861905591907876900,Tue May 09 11:28:01 +0000 2017,@MendeleySupport I want to reduce my web space usage. Can you tell me how I can identify the documents with the largest attached pdf files?
860930681270140900,Sat May 06 18:54:04 +0000 2017,@lmthang Darn. Can we get something like this for *ACL conferences? Not much in terms of graphics, but maybe w/ bea‚Ä¶ https://t.co/gBTRuYbDjP
860224607974027300,Thu May 04 20:08:23 +0000 2017,FAIR open-sources ParlAI, framework for dialogue research, supports train+test, MTL (w/ pronunciation hint cf. bAbI) https://t.co/09ww31cS7F
859423204204568600,Tue May 02 15:03:53 +0000 2017,How to get started in #NLProc -- a list of resources and pointers collected by @meltomene https://t.co/iJwRZHSKk7
856455289209532400,Mon Apr 24 10:30:27 +0000 2017,@freddylecue @IJCAI_2017 @jpansw Cool! Is there a link to the paper?
856062898032005100,Sun Apr 23 08:31:14 +0000 2017,@odsc Thanks!
855556127240720400,Fri Apr 21 22:57:30 +0000 2017,@odsc Hey, would be cool if you let me know if you repost one of my blog posts and attribute it to the right author. https://t.co/7IkkA4LZy7
855555771152683000,Fri Apr 21 22:56:05 +0000 2017,@desertnaut Thanks for the note. Yep, that's weird. I'll shoot them a message. :)
854690952820273200,Wed Apr 19 13:39:37 +0000 2017,@barbara_plank Awesome! Thanks! Particularly wanted to look at the MTL for NLP slides. :)
854679385785475100,Wed Apr 19 12:53:39 +0000 2017,@barbara_plank Cool new website! :) Seems like the links to your Geneva presentations don't work yet. ;)
854616192031567900,Wed Apr 19 08:42:32 +0000 2017,@yagwar183 @lmthang @_rockt @_rockt knows more about RTE, but sure. :)
854277240368115700,Tue Apr 18 10:15:40 +0000 2017,@bernardt_d Awesome! What's your use case?
854215648431530000,Tue Apr 18 06:10:55 +0000 2017,@bernardt_d Glad you like it! Wow, that's a bit too big of a compliment! üòÅ
854058796976570400,Mon Apr 17 19:47:39 +0000 2017,RT @kdnuggets: Transfer Learning - #MachineLearning's Next Frontier https://t.co/kD5j4cmM7y @seb_ruder https://t.co/fT5SSSV4Ww
853240028058853400,Sat Apr 15 13:34:09 +0000 2017,@tallinzen @barbara_plank Yeah, don't think it's as general as that. LAMBADA is a good example of human-level LM performance! üëç
853195542477852700,Sat Apr 15 10:37:23 +0000 2017,I'm visiting @lowlandscph and @soegaarducph in Copenhagen until June. Let me know if you're in Copenhagen and would like to chat.
853149380303573000,Sat Apr 15 07:33:57 +0000 2017,@tallinzen @barbara_plank Jozefowicz et al. (2016) achieve 24.2 per-word perplexity, i.e. 4.6 bits per word. What a‚Ä¶ https://t.co/GfxuOM9NWS
853148991286128600,Sat Apr 15 07:32:24 +0000 2017,@tallinzen @barbara_plank Thanks! I was mostly comparing this to Shannon (1951), where he says that for humans, an‚Ä¶ https://t.co/hvBKHomIat
852986099408543700,Fri Apr 14 20:45:08 +0000 2017,@theronwu7 Great! Would love to learn more about your use cases! :)
852981648807403500,Fri Apr 14 20:27:27 +0000 2017,@jasonbaldridge @manaalfar @Google Oh, wow! That's awesome, Jason! Congrats! :)
852847810030514200,Fri Apr 14 11:35:37 +0000 2017,@AjitJaokar Me too! Good to stay in touch this way!
852845604103499800,Fri Apr 14 11:26:51 +0000 2017,@AjitJaokar Yep. üòÄ
852843870924812300,Fri Apr 14 11:19:58 +0000 2017,RT @AjitJaokar: excellent post by @seb_ruder on transfer learning https://t.co/Foj0EvXIOA
852655522725724200,Thu Apr 13 22:51:32 +0000 2017,RT @kaggle: New to transfer learning? Start here. Learn about methods, applications, related research and lots more. https://t.co/Ap8P0wvDp‚Ä¶
852595159539871700,Thu Apr 13 18:51:41 +0000 2017,RT @_aylien: Flappy Bird and evolution strategies: an experiment #AI #MachineLearning https://t.co/duxQbzMGiz https://t.co/QZErTcnlb2
851847847343521800,Tue Apr 11 17:22:08 +0000 2017,RT @BananaData: #TransferLearning is #MachineLearning's next big frontier https://t.co/CvFzbDtMcb #ml #datascience by @seb_ruder https://t.‚Ä¶
850080789211557900,Thu Apr 06 20:20:28 +0000 2017,RT @yonatanzunger: Some excellent advice here for anyone who wants to use ML in, well, anything. https://t.co/Q9dVqSt140
849735727051100200,Wed Apr 05 21:29:19 +0000 2017,@EugenBurianov Hey Eugen, thank you! Really appreciate that you're finding it helpful. :)
849327112578269200,Tue Apr 04 18:25:38 +0000 2017,RT @joabingel: When and why does multitask learning work in #NLProc? See @soegaarducph and my poster at the Wednesday evening poster sessio‚Ä¶
849218454251089900,Tue Apr 04 11:13:51 +0000 2017,RT @NLPDublin: Slides for our last event Interacting with Machines by @eoinhurrell, @alfredomg, Emer Gilmartin are now online https://t.co/‚Ä¶
848422538661306400,Sun Apr 02 06:31:10 +0000 2017,RT @tonyojeda3: Transfer Learning - Machine Learning's Next Frontier https://t.co/CI9UBmj7yu #DeepLearning #NLProc #MachineLearning by @seb‚Ä¶
847553095819296800,Thu Mar 30 20:56:19 +0000 2017,RT @jimmfleming: Transfer Learning‚ÄîMachine Learning's Next Frontier from @seb_ruder https://t.co/BkkvLXePl7 #machinelearning https://t.co/4‚Ä¶
847525907589996500,Thu Mar 30 19:08:17 +0000 2017,RT @NLPDublin: .@eoinhurrell talks about what it takes to understand user queries at the 6th #NLPDublin meetup hosted by @ZalandoTech https‚Ä¶
847512999673843700,Thu Mar 30 18:16:59 +0000 2017,RT @cigilt: @alfredomg of @AdaptCentre making sense of word sense disambiguation at #NLPDublin in the colourful @ZalandoTech https://t.co/b‚Ä¶
847512950973849600,Thu Mar 30 18:16:48 +0000 2017,RT @NLPDublin: Sense and sensibility: @alfredomg on the challenges and advances in word sense disambiguation https://t.co/udC0tnX0k8
847512930862161900,Thu Mar 30 18:16:43 +0000 2017,RT @NLPDublin: Full house at the 6th #NLPDublin meetup: Interacting with Machines hosted by @ZalandoTech https://t.co/vVjGN3jUXV
847488401142038500,Thu Mar 30 16:39:15 +0000 2017,RT @RaphaelWouters: Tonight is the 6th @NLPDublin meetup at @Zalando office at Grand Canal Quay https://t.co/iIs45u4Abe #NLP #AI https://t.‚Ä¶
847366008427888600,Thu Mar 30 08:32:54 +0000 2017,@verhoevenben haha, 'well, we won't be able to give a definitive answer to this' didn't fit in my remaining characters. ;-)
847110114683301900,Wed Mar 29 15:36:04 +0000 2017,RT @NLPDublin: Many constructive conversations at the #machinelearning communities day in London hosted by @Google @seldon_io #MLCUK https:‚Ä¶
847014014106423300,Wed Mar 29 09:14:12 +0000 2017,RT @mckibben23: Machine learning community day hosted by google London. #mldublin with @seb_ruder  and @NeillGernon https://t.co/8YRn29dtRW
846947808234082300,Wed Mar 29 04:51:07 +0000 2017,Does author response make a difference? The answer is "yes" according to data from ACL review process https://t.co/zKtoqOkT8H
846925125874274300,Wed Mar 29 03:20:59 +0000 2017,RT @tdietterich: These rules are full of wisdom. Every university should offer classes in DevOps for #MachineLearning using them as the tex‚Ä¶
846784961386549200,Tue Mar 28 18:04:01 +0000 2017,Rules of #MachineLearning: Best practices for ML Engineering. A 43-rule playbook by a Google Research scientist. https://t.co/5EYk6pCumw
846741924287533000,Tue Mar 28 15:13:01 +0000 2017,The Problem(s) with Neural Chatbots - slides by @ryan_t_lowe https://t.co/IYAoA6GuBj #machinelearning #chatbots
846673382418583600,Tue Mar 28 10:40:39 +0000 2017,RT @bentolmachoff: This little girl thought a broken water heater is a real life robot. It's just not fair how cute it is https://t.co/TLbu‚Ä¶
846423964339572700,Mon Mar 27 18:09:33 +0000 2017,Overfitting and dropout at the #machinelearning #dublinml meetup hosted by @Accenture @DublinML https://t.co/Ysh3qgsMCC
845425600374419500,Sat Mar 25 00:02:25 +0000 2017,@derekgreene @IgorBrigadir @insight_centre woohoo, congrats igor!
845322239260155900,Fri Mar 24 17:11:41 +0000 2017,Insightful blog post on Evolution Strategies as a scalable Alternative to Reinforcement Learning by @OpenAI https://t.co/5Mm3gaV0O2
844990057031307300,Thu Mar 23 19:11:43 +0000 2017,RT @AlexRoseJo: Transfer Learning allows ML to transfer knowledge learned between tasks. Great post by @seb_ruder https://t.co/BtJXNWHJ5W h‚Ä¶
844978006397472800,Thu Mar 23 18:23:50 +0000 2017,RT @_aylien: Transfer Learning - Machine Learning's Next Frontier https://t.co/C1f0XVkxXi #deeplearning #NLProc #transferlearning https://t‚Ä¶
844513264264532000,Wed Mar 22 11:37:07 +0000 2017,RT @parsaghaffari: We're looking for a DevOps eng/SRE @_aylien to help build the next generation of our Text Analysis/#nlproc platform: htt‚Ä¶
844262334524645400,Tue Mar 21 19:00:00 +0000 2017,Just published my latest blog post: Transfer Learning - #MachineLearning's Next Frontier https://t.co/EWVJxcobWB https://t.co/WmZjxr9d44
843885148672331800,Mon Mar 20 18:01:12 +0000 2017,After several awesome articles &amp; visualizations, @distillpub launches officially as a journal! üëè @ch402, @shancarter https://t.co/BPvsDKwEFs
843819180793192400,Mon Mar 20 13:39:04 +0000 2017,RT @dennybritz: The Wild Week in AI - Y Combinator AI, Keras 2, Evolutional Strategies vs. RL, Learning to Communicate https://t.co/y9ktTLy‚Ä¶
843178696475365400,Sat Mar 18 19:14:01 +0000 2017,Learning to communicate: How @OpenAI trains agents to invent a language through cooperation. https://t.co/xBUB3FnuxE #machinelearning
843032597655703600,Sat Mar 18 09:33:28 +0000 2017,How Experiment, Kickstarter for #Science, got through @ycombinator. Great story &amp; advice! https://t.co/jkLl1icO6q https://t.co/fIlgSRnEVj
842444091203936300,Thu Mar 16 18:34:57 +0000 2017,RT @NLPDublin: RSVPs are now open for our next event Interacting with Machines! We are hoping to see all of you at @ZalandoTech! https://t.‚Ä¶
842178611083898900,Thu Mar 16 01:00:02 +0000 2017,ParseySaurus, SyntaxNet's character-based successor is released by @googleresearch. Rawr! https://t.co/U72TAz4kfA https://t.co/txLOKTmNQC
842155216392884200,Wed Mar 15 23:27:04 +0000 2017,Learning when to skim (w/ BOW model) and when to read (w/ LSTM) for sentiment analysis. Detailed post by @MetaMindIO https://t.co/MN9pLb1o6j
841746255642521600,Tue Mar 14 20:22:00 +0000 2017,Spending your time reading/analyzing reviews? This infographic nicely visualizes how they can make or break a biz. https://t.co/0kpBj0H3Ar
841702213604978700,Tue Mar 14 17:27:00 +0000 2017,The classic house prize intro to #machinelearning beautifully visualized by @r2d3us https://t.co/C8y0nC0ziI
841694039862841300,Tue Mar 14 16:54:31 +0000 2017,@ejlbell @edersantana Also, softmax seems to be working pretty well (e.g. PixelRNN family). Love to see more work on categorical dists tho.
841693407777046500,Tue Mar 14 16:52:00 +0000 2017,@nerdherdempire @HrSaghir @dennybritz @hardmaru &lt;3 the discussion. Curiosity != objective? Let me know if we're making philosophical headway
841605423023153200,Tue Mar 14 11:02:23 +0000 2017,@IntuitMachine Hey Carlos, saw you're interested in domain adaptation/transfer learning in your Feb 8 blog post. Would be cool to connect.
841604234818797600,Tue Mar 14 10:57:40 +0000 2017,ICLR 2017 reviews vs wisdom of the crowd aka arxiv-sanity. Nice comparison on paper acceptance by @karpathy. https://t.co/irnkgYDXyX
841568686871871500,Tue Mar 14 08:36:25 +0000 2017,RT @deliprao: Brilliant! Can we have one for #nlproc? https://t.co/aTH1SWMY8l
841568659751473200,Tue Mar 14 08:36:18 +0000 2017,@deliprao +1 to this!
841425565370482700,Mon Mar 13 23:07:42 +0000 2017,RT @jekbradbury: A good list, and my colleagues are #1 :) | ‚ÄúTen Deserving Deep Learning Papers that were Rejected at ICLR 2017‚Äù https://t.‚Ä¶
841413061198172200,Mon Mar 13 22:18:01 +0000 2017,A beautiful example of data story telling and gorgeous visualizations of applied #machinelearning by @stitchfix_algo https://t.co/1aQoD9k6W6
841362981053165600,Mon Mar 13 18:59:01 +0000 2017,How advances in generative modelling can lead to the spread of misinformation by Paul Soulos https://t.co/CD20f4P1Hy #machinelearning
841328754811002900,Mon Mar 13 16:43:00 +0000 2017,Have you ever woken up and wondered what optimizer works best for your style transfer model? Find the answer here: https://t.co/p8iTxHw4eL
841259312995532800,Mon Mar 13 12:07:04 +0000 2017,Check out @dennybritz's new seq2seq framework in @tensorflow. Awesome work! Easy to use and extendable! https://t.co/G5UBulHOiU
839947909768556500,Thu Mar 09 21:16:01 +0000 2017,@PyImageSearch Thanks for asking! That's no problem. For the images 5 or 6 just make sure to attribute them to @AlecRad.
839946994152009700,Thu Mar 09 21:12:23 +0000 2017,@PyImageSearch thanks a lot! Really appreciate it! üòÄ
838705183618138100,Mon Mar 06 10:57:52 +0000 2017,@anujgupta82 Glad you like it. :)
837709341465718800,Fri Mar 03 17:00:45 +0000 2017,RT @barbara_plank: Job: We are looking for a lecturer in Inf. Science (2 years, w/ possibility for tenure!) #NLProc - apply by April 9! htt‚Ä¶
837673706558206000,Fri Mar 03 14:39:09 +0000 2017,RT @freddylecue: @seb_ruder presenting #TransferLearning @AccentureDock @accenturelabs @Accenture_Irl https://t.co/cmpiQVtFUa
837440536239833100,Thu Mar 02 23:12:37 +0000 2017,I recommend registering for the Lisbon #MachineLearning Summer School 2017. I went last year and it was a blast!  https://t.co/WRl5bLXfFh
836493988635029500,Tue Feb 28 08:31:22 +0000 2017,@deliprao @hugo_larochelle Could you share the link?
836349415984095200,Mon Feb 27 22:56:54 +0000 2017,@StartupLJackson You are back!!
836299603125366800,Mon Feb 27 19:38:57 +0000 2017,.@Mystical_Wiz talking about constraining search for universal sequence-to-sequence models at @DublinML https://t.co/PsocvHCfQ1
836290403372531700,Mon Feb 27 19:02:24 +0000 2017,RT @WiNLPWorkshop: This is the account for the 1st Women in NLP workshop (WiNLP) @acl2017 #NLProc FB: https://t.co/MsSAzGILrW www: https://‚Ä¶
836284966615007200,Mon Feb 27 18:40:48 +0000 2017,Kicking off the @DublinML meetup by turning photographs into paintings #DublinML #machinelearning https://t.co/MCk4a4QlcW
836165610530824200,Mon Feb 27 10:46:31 +0000 2017,RT @NLPDublin: Join us for our 6th NLP Dublin Meetup with talks by @alfredomg, @eoinhurrell on March 30 at @ZalandoTech! https://t.co/g4yyl‚Ä¶
836165452644692000,Mon Feb 27 10:45:53 +0000 2017,Interested in interacting with machines? We'll have talks on human-machine spoken dialogue, word sense disambiguati‚Ä¶ https://t.co/5r9hFSa5DK
834803769271525400,Thu Feb 23 16:35:03 +0000 2017,Interested in Semi-Supervised Learning? Here's a clearly written (if ~outdated) literature survey: https://t.co/WhqGAeSuRD #machinelearning
834464865020473300,Wed Feb 22 18:08:22 +0000 2017,We had a blast at yesterday's Dublin #deeplearning meetup at @ZalandoTech. Great talks and speakers! Looking forwar‚Ä¶ https://t.co/FfRHwFNh8w
834135374591045600,Tue Feb 21 20:19:05 +0000 2017,Robert Hogan on how to get the most out of transfer learning at the Dublin #deeplearning meetup at @ZalandoTech https://t.co/yUX7RadXhU
834124435103834100,Tue Feb 21 19:35:37 +0000 2017,.@PeleteiroAna talking about RNNs for #NLProc at the first Dublin #deeplearning meetup of 2017 https://t.co/yczruEXlUm
832545000047124500,Fri Feb 17 10:59:30 +0000 2017,RT @NLPDublin: Dublin's AI community is growing and we're happy to be a part of it together with @dublinAI @DublinML @DataSci_Ireland! http‚Ä¶
832286697966678000,Thu Feb 16 17:53:06 +0000 2017,RT @_aylien: Exciting research proposals submitted by our PhD and MSc candidates to the @IrishResearch Council today. Best of luck to all!
830071046715953200,Fri Feb 10 15:08:54 +0000 2017,Can anyone think of #machinelearning transfer learning scenarios mentioned in news, e.g. self-driving cars trained in different cities?
830016209450065900,Fri Feb 10 11:30:59 +0000 2017,@mesch13 You have to follow me back for me to DM you.
830002322717700100,Fri Feb 10 10:35:49 +0000 2017,@mesch13 Hey Ryan, thanks for sponsoring the awesome @dublinAI event last night! Can I talk to you about the @NLPDublin meetup?
829833511242719200,Thu Feb 09 23:25:01 +0000 2017,RT @parsaghaffari: We have some big news to share ‚Äì after weeks of work @_aylien we've managed to model @realDonaldTrump with a yuuge accur‚Ä¶
829800860116979700,Thu Feb 09 21:15:16 +0000 2017,RT @mesch13: Wow. That was an intredible set of speakers tonight @dublinAI! @Artomatixltd @_aylien @getpointy #DublinAI
829280962643161100,Wed Feb 08 10:49:23 +0000 2017,RT @Miles_Brundage: "Knowledge Adaptation: Teaching to Adapt," Ruder et al.: https://t.co/vwkvkvo37y
829075863589310500,Tue Feb 07 21:14:24 +0000 2017,RT @_aylien: AYLIEN founder &amp; CEO @parsaghaffari will be speaking at @dublinAI on Thursday "Using NLP to understand textual content at scal‚Ä¶
829034502718681100,Tue Feb 07 18:30:02 +0000 2017,Great compilation of directions and desiderata for AI control by @OpenAI's @paulfchristiano https://t.co/U9ibJBdlGl #machinelearning
827506532266938400,Fri Feb 03 13:18:26 +0000 2017,Join us at Dublin's first AI hackathon in two weeks run by @Atrovate @neillgernon #dublinAIhack https://t.co/KyHXNcz6Pa #machinelearning
826958309206978600,Thu Feb 02 00:59:59 +0000 2017,@sravsatuluri very cool! Thanks for reading!
826958227812253700,Thu Feb 02 00:59:40 +0000 2017,RT @sravsatuluri: My mindmap of optimizations of Gradient descent after reading through this excellent article by @seb_ruder https://t.co/D‚Ä¶
826910073247121400,Wed Feb 01 21:48:19 +0000 2017,RT @deliprao: After months of hard work, very satisfying to launch Stage 1 of  #FakeNewsChallenge. For dataset &amp; details, see https://t.co/‚Ä¶
826866800738594800,Wed Feb 01 18:56:22 +0000 2017,@MendeleySupport Hello? Is it possible to get a reply from you? Should I rather contact you via email?
826839626858037200,Wed Feb 01 17:08:23 +0000 2017,@lpag_ai Thanks for the mention. :)
826586031524810800,Wed Feb 01 00:20:41 +0000 2017,@hacks thanks for the shout-out! Glad it's helpful. üòÅ
826524268607443000,Tue Jan 31 20:15:16 +0000 2017,@armillspaugh @ycombinator Haha, touch√©! Still, always glad to see access to NLP/AI being made easier. :)
826517283518427100,Tue Jan 31 19:47:30 +0000 2017,How To Get Into Natural Language Processing -- a post by @ycombinator #NLProc https://t.co/TrYkPTd7FC
826514626825293800,Tue Jan 31 19:36:57 +0000 2017,RT @freddylecue: And now Yassine Lassoued on IBM Debating Technologies @NLPDublin @accenturelabs @Accenture_Irl https://t.co/ulGQuZ3cDi
826514592834678800,Tue Jan 31 19:36:49 +0000 2017,RT @NeillGernon: Another great meetup tonight with full house at @NLPDublin #nlp #ai https://t.co/VMCPSNbBXS
826514569984168000,Tue Jan 31 19:36:44 +0000 2017,RT @gevrapatapich: Sara has just started her talk but it's already 10 times more interesting than 4 previous talks together #NLPDublin
826394508413767700,Tue Jan 31 11:39:39 +0000 2017,RT @NeillGernon: Great event last night @DublinML and tonight is @NLPDublin .... Dublins AI community's looking pretty right now! https://t‚Ä¶
826169882962165800,Mon Jan 30 20:47:04 +0000 2017,RT @dermotcasey: First two speakers at #mldublin @dublinml highlighting role of international students in Irish science ecosystem https://t‚Ä¶
826141959714652200,Mon Jan 30 18:56:06 +0000 2017,RT @dublinAI: Irelands First AI Hackathon, this Feb.. by @atrovate https://t.co/QuP3Hlh37f
826133907460468700,Mon Jan 30 18:24:07 +0000 2017,@Atrovate Awesome event! Seems like it's not possible yet to register, though?
826117089278693400,Mon Jan 30 17:17:17 +0000 2017,RT @parsaghaffari: Excited for @NLPDublin meetup tomorrow @Accenture_Irl with speakers @pymnpsbn, @xin_irl &amp; Yassine from @IBMResearch: htt‚Ä¶
826010900662538200,Mon Jan 30 10:15:19 +0000 2017,RT @parsaghaffari: .@shelanzadeh and I will be talking to @RTERadio1's @morningireland tomorrow around 8:20AM about #MuslimBan and how it a‚Ä¶
825325626081042400,Sat Jan 28 12:52:17 +0000 2017,With talks from @xin_irl, @pymnpsbn. Particularly excited to hear about @Accenture's SA and @IBMResearch's debating‚Ä¶ https://t.co/Q0Wxu7kBU7
824609513521184800,Thu Jan 26 13:26:43 +0000 2017,RT @NLPDublin: More than 110 people are signed up to attend our first meetup of the year. Join us for a New Year of #NLProc! https://t.co/0‚Ä¶
823926897365106700,Tue Jan 24 16:14:14 +0000 2017,@MendeleySupport I'm over my personal web space limit. Is there a way to sort docs by file size so that I can remove the biggest files?
823926639331541000,Tue Jan 24 16:13:13 +0000 2017,@PeleteiroAna Won't make it to the NLP meetup unfortunately as I'll still be traveling. I'll see you next time then.
823668167620735000,Mon Jan 23 23:06:08 +0000 2017,@PeleteiroAna Hey Ana, just read your blog post on WiML. Cool that you were also at NIPS. Would love to get coffee sometime to talk abt ML.
821758373867626500,Wed Jan 18 16:37:18 +0000 2017,@MendeleySupport I find that this sometimes happens among documents in the same folder. I haven't been able to reproduce it yet reliably.
821458996364247000,Tue Jan 17 20:47:41 +0000 2017,@Tbeltramelli Thanks for the shout-out, Tony! Btw, I'll be in Copenhagen in April and May if you wanna meet up.
821399102500073500,Tue Jan 17 16:49:41 +0000 2017,RT @NLPDublin: Join us for A New Year of NLP: RSVP's for our first event of 2017 hosted by @Accenture are now open. https://t.co/0vrJCs6Vgk‚Ä¶
821035119343497200,Mon Jan 16 16:43:21 +0000 2017,RT @_aylien: We have a number of openings in different teams at AYLIEN. Check them out here: https://t.co/O0118X4dY8 #jobfairy #Startup #Jo‚Ä¶
819914187816267800,Fri Jan 13 14:29:10 +0000 2017,@betterexplained You're welcome. :) Forget to mention you in the tweet. Will do that next time. ;)
819880072685506600,Fri Jan 13 12:13:36 +0000 2017,RT @_aylien: Our very own @afshinmeh talks us through the why and how of his #opensource journey with #introjs https://t.co/EtU6FOaFaS #jav‚Ä¶
819733529265340400,Fri Jan 13 02:31:17 +0000 2017,The best explanation of the Fourier Transform I've read. Great way to interactively understand it! https://t.co/US0T07uHjp
818428990797021200,Mon Jan 09 12:07:31 +0000 2017,@erfannoury @Smerity has also tweeted about this https://t.co/WfxDCXy4KR
818054418692276200,Sun Jan 08 11:19:06 +0000 2017,RT @parsaghaffari: Looking to start a PhD or MSc in Deep Learning or NLP in 2017? We're offering fully-funded positions @_aylien. More: htt‚Ä¶
817109091415982100,Thu Jan 05 20:42:43 +0000 2017,@KLM Worst customer service in my life. No help at all. Booked new ticket now with @British_Airways. Still no money returned from KLM.
817093159758991400,Thu Jan 05 19:39:24 +0000 2017,@KLM We are both in shock and my girlfriend is crying.
817087672049168400,Thu Jan 05 19:17:36 +0000 2017,@KLM because payment didn't go through -- which it did. No response from customer service on Twitter and on hold online. WTF.
817087397611667500,Thu Jan 05 19:16:30 +0000 2017,Gf &amp; I booked flight to Brazil 5 months ago via @KLM. Money was deducted. Flight is tmrw &amp; they're now telling me that tickets got cancelled
817009031025983500,Thu Jan 05 14:05:06 +0000 2017,@KLM I just tried to check-in online with my booking code and flight no for a Dublin-Brazil flight early tomorrow and it won't let me. Help?
816271539830190100,Tue Jan 03 13:14:35 +0000 2017,@MendeleySupport I love your product, but notes saved in 'General Notes' are switched between papers or lost with alarming regularity. Help?
815899317517041700,Mon Jan 02 12:35:30 +0000 2017,@erfannoury Seems like a good paper for an in-depth reading group session then. ;)
814143361485602800,Wed Dec 28 16:17:58 +0000 2016,RT @keydaliu: Really cool visualisation of a GAN being trained https://t.co/5JzG1VjGZa, great blog post @_aylien on GAN + TF.
813392063932801000,Mon Dec 26 14:32:34 +0000 2016,The Wild Week in AI weekly newsletter by @dennybritz featuring NIPS summaries, OpenNMT release, ICLR paper analysis https://t.co/zd6JTATS8w
813300478603882500,Mon Dec 26 08:28:39 +0000 2016,RT @GenevieveCerf: Trying to sort this out... AI moving much faster now than it did when I was doing this research in 1983 - 1988 or so! No‚Ä¶
812028721452503000,Thu Dec 22 20:15:08 +0000 2016,@chris_brockett Haha, would definitely love to see a more judicious variant of that response. ;)
812028413732995100,Thu Dec 22 20:13:55 +0000 2016,@joelgrus @Adversarial_L Awesome! I'll check it out! :D
812024376036048900,Thu Dec 22 19:57:52 +0000 2016,@Adversarial_L @joelgrus Cool. Didn't know about the podcast before. Are you on Stitcher?
812023878579056600,Thu Dec 22 19:55:53 +0000 2016,@chris_brockett Ah, wasn't sure how to interpret it. It was my first time at NIPS, so I guess I just expected to see more NLP papers. :)
811999605244198900,Thu Dec 22 18:19:26 +0000 2016,RT @dennybritz: Highlights of NIPS 2016: Adversarial Learning, Meta-learning and more - Nice summary by @seb_ruder https://t.co/pDd3nWL8sS
811984615468662800,Thu Dec 22 17:19:52 +0000 2016,RT @parsaghaffari: Missed NIPS? Check out @seb_ruder's detailed write up on the latest in deep learning research #deeplearning #nlproc http‚Ä¶
811855226630246400,Thu Dec 22 08:45:44 +0000 2016,RT @_aylien: Check out our review of #nips2016 featuring GANs, RNNs, and learning-to-learn. #NLProc #deeplearning https://t.co/GF9xWHwyfb
811650488630464500,Wed Dec 21 19:12:10 +0000 2016,My highlights of #nips2016 with thoughts on adversarial learning, meta-learning, reinforcement learning, and more. https://t.co/25aCRCQrvx
811610672765276200,Wed Dec 21 16:33:58 +0000 2016,Be sure to mark Jan 31 2017 in your calendars and join us at the first #NLPDublin meetup of the new year! #NLProc https://t.co/CQ89WPBz4W
811515806525825000,Wed Dec 21 10:17:00 +0000 2016,@rdviii @AlecRad Looks awesome! :)
809343883071066100,Thu Dec 15 10:26:33 +0000 2016,@eoinhurrell No worries. Let me know if you'd like to chat sometime. I'd love to know more about those hairy NLP problems. :)
809040493275480000,Wed Dec 14 14:20:59 +0000 2016,@PMinervini @NLPDublin Another option might be the Concrete Distribution. 2017 should be the year of GANs for NLP. https://t.co/THRVRnFfLu
808983478951956500,Wed Dec 14 10:34:26 +0000 2016,RT @NLPDublin: Photos and slides of last night's #NLPDublin meetup are now available online. Thanks for coming, everyone! https://t.co/PL9G‚Ä¶
808976649094103000,Wed Dec 14 10:07:17 +0000 2016,Here are the slides of my review of #nips2016 featuring GANs, RNNs, RL &amp; #rocketai at last night's @NLPDublin meetup https://t.co/HldUzxW9bN
808815355493752800,Tue Dec 13 23:26:22 +0000 2016,@freddylecue @NLPDublin @NipsConference Definitely. Still early days, though. Haven't seen too many applications of this to NLP yet, though.
808786680769613800,Tue Dec 13 21:32:25 +0000 2016,RT @freddylecue: @NipsConference 2016 report from @seb_ruder @NLPDublin https://t.co/772SkqB5sr
808784310258061300,Tue Dec 13 21:23:00 +0000 2016,RT @0xh3x: Awesome talk by @seb_ruder at #NLPDublin https://t.co/ppjW5NVp8m
808757909085683700,Tue Dec 13 19:38:06 +0000 2016,RT @NLPDublin: Generative adversarial document representations on 20newsgroups by @anotherjohng https://t.co/CO14HAxzrs
808754477109018600,Tue Dec 13 19:24:27 +0000 2016,Haven't had enough of GANs after #nips2016 yet? John talks about some of out recent research using GANs for #nlproc‚Ä¶ https://t.co/imCDCpULCs
808745664133951500,Tue Dec 13 18:49:26 +0000 2016,RT @NLPDublin: First talk on multimodal machine translation by Iacer Calixto at 4th #NLPDublin meetup hosted by @workday https://t.co/QzkUU‚Ä¶
808627962358677500,Tue Dec 13 11:01:44 +0000 2016,I'll be giving a review of #nips2016 at the @NLPDublin meetup tonight hosted by @Workday. https://t.co/pYfvchDhuH #NLProc #NLPDublin
808614437368242200,Tue Dec 13 10:07:59 +0000 2016,RT @anotherjohng: I'll be talking about Generative Adversarial Networks and representation learning at the @NLPDublin meetup tonight
808407735473733600,Mon Dec 12 20:26:38 +0000 2016,@MrChrisJohnson Thanks a lot for the shout-out, Chris! :)
808373924954341400,Mon Dec 12 18:12:17 +0000 2016,@lmthang @chrmanning Congrats, Thang! Great work! +1 for the Babel fish citation to start off the intro. ;)
808284776872476700,Mon Dec 12 12:18:02 +0000 2016,@DrewPurves Awesome! Looking forward to it! :)
808284207441203200,Mon Dec 12 12:15:46 +0000 2016,@DrewPurves Hey Drew, are the slides from your keynote talk at NIPS by any chance available somewhere?
807308810331291600,Fri Dec 09 19:39:54 +0000 2016,RT @NLPDublin: Join us on Dec 13 to talk about multi-modal MT, GANs, RL &amp; more! Note: attendees need to fill out this form. https://t.co/LR‚Ä¶
807230826597466100,Fri Dec 09 14:30:01 +0000 2016,Meta-learning has been an emerging topic at #nips2016; this blog post presents #iclr2017 meta-learning submissions https://t.co/uKw7uQ08eX
807214645333397500,Fri Dec 09 13:25:43 +0000 2016,@tecdr @dublinAI @Irish_TechNews @NeillGernon @NLPDublin That's cool, thanks! ‚ò∫Ô∏è
807211864723693600,Fri Dec 09 13:14:40 +0000 2016,Brenden Lake presenting the Character and Frostbite Challenges and discussing how to build machines that learn and‚Ä¶ https://t.co/5jQV0o2FIy
807181626253971500,Fri Dec 09 11:14:31 +0000 2016,RT @anotherjohng: I'm presenting a poster on adversarial document models at the #nips2016 adversarial training workshop today https://t.co/‚Ä¶
807170102286254100,Fri Dec 09 10:28:43 +0000 2016,@tecdr @dublinAI @Irish_TechNews @NeillGernon not true. There's the #MLDublin meetup, the @NLPDublin meetup, all dealing with AI.
807160666004058100,Fri Dec 09 09:51:13 +0000 2016,@Smerity so sorry, man. Let me know if you need any help.
807154399437004800,Fri Dec 09 09:26:19 +0000 2016,Playing curling with an AI in the learning, inference and control of multi-agent systems workshop at #nips2016 https://t.co/JFxyIpU2m7
806947048683634700,Thu Dec 08 19:42:23 +0000 2016,Insightful #nips2016 edition of @jackclarkSF's newsletter with news about DeepMind, Salesforce, DL, RL &amp; more https://t.co/lqHWGE5j9Q
806867633186414600,Thu Dec 08 14:26:49 +0000 2016,The panel on unsupervised learning kicks off at the Deep Learning symposium at #nips2016 with @ylecun talking about‚Ä¶ https://t.co/twF9NoWnoK
806773049672335400,Thu Dec 08 08:10:58 +0000 2016,@joabingel @OpenAI @ProjectJupyter Yeah, live coding would be cool, but depending on the approach might be hard to follow. Code is nice tho.
806570923134427100,Wed Dec 07 18:47:48 +0000 2016,Interactive music improv with @googleresearch's Magenta at #nips2016 https://t.co/FwHZUi5RJc
806543999305707500,Wed Dec 07 17:00:48 +0000 2016,Actionable presentation on weight normalization by @OpenAI's Tim Salimans with @tensorflow/keras implementation on‚Ä¶ https://t.co/6OUjdyR9Px
806287170822672400,Wed Dec 07 00:00:16 +0000 2016,@iamaidang @jackclarkSF yessss, more apple research papers!!
805902448497987600,Mon Dec 05 22:31:31 +0000 2016,@RichardSocher Sounds good. Do you want to ping me when you're available?
805901063484276700,Mon Dec 05 22:26:01 +0000 2016,RT @NLPDublin: Want to hear about cutting-edge research trend such as GANs and highlights from #nips2016? Join us on December 13!  https://‚Ä¶
805882373040640000,Mon Dec 05 21:11:44 +0000 2016,Cool tutorial on large-scale optimization by Francis Bach at #nips2016. Even mentioned my SGD blog post üò≤  Slides: https://t.co/b0DKKArRgw
805872766658506800,Mon Dec 05 20:33:34 +0000 2016,@RichardSocher Hey Richard, I'd love to meet up. Let me know when suits you.
805776786839715800,Mon Dec 05 14:12:11 +0000 2016,Full house at the Generative Adversarial Networks tutorial by @goodfellow_ian at #nips2016 https://t.co/Bqs8fksRZU
805739019342647300,Mon Dec 05 11:42:06 +0000 2016,Transfer learning is the second biggest driver after supervised learning for future #machinelearning success accord‚Ä¶ https://t.co/0sAPpLSNCS
805718051396210700,Mon Dec 05 10:18:47 +0000 2016,.@AndrewYNg on the most important aspects of building real-world #machinelearning applications at the nuts &amp; bolts‚Ä¶ https://t.co/FijVxZBAOh
805716779276398600,Mon Dec 05 10:13:44 +0000 2016,RT @nikoSuenderhauf: Can we transfer the knowledge an agent learned in computer games to a mobile robot in the real world? Anyone looking f‚Ä¶
805710078426935300,Mon Dec 05 09:47:06 +0000 2016,Universe, @OpenAI's new platform allows to train agents on 1,000s of games incl. Portal, Fable. Let the games begin! https://t.co/I8sQMhdPWB
805692751534784500,Mon Dec 05 08:38:15 +0000 2016,RT @parsaghaffari: At #nips2016 and interested in learning more about #NLProc research @_aylien? Talk to @anotherjohng or @seb_ruder
805377700928045000,Sun Dec 04 11:46:21 +0000 2016,Are you already excited for @emnlp2017? The website is up and deadlines are announced. Hope to see you all there! https://t.co/75SiBmePpZ
805025612725424100,Sat Dec 03 12:27:17 +0000 2016,@barbara_plank Looking forward to seeing you in Barcelona!
804108864652279800,Wed Nov 30 23:44:27 +0000 2016,RT @RLRudge: This is best piece of Superman art I've seen in a long time, made by Brakken https://t.co/4BAJvGS2Vx https://t.co/CgJnCiGfwL
804092370530746400,Wed Nov 30 22:38:55 +0000 2016,Some to-the point notes of the Senate Subcommittee Dawn of #AI  Q&amp;A by @stephenroller https://t.co/XG1gV31ROl
804002926440185900,Wed Nov 30 16:43:29 +0000 2016,@lmthang @dennybritz More than the usual reports about some clever system that can fool half the judges?
803962593757040600,Wed Nov 30 14:03:13 +0000 2016,Great, interactive exploration of the inner workings of thought vectors by @gabeeegoooh https://t.co/eFMC981O6d #deeplearning
803729474235691000,Tue Nov 29 22:36:53 +0000 2016,Tensorboard can now visualize embeddings. Inspecting your model just got a lot more visually appealing! @tensorflow https://t.co/GAgKWYCgvr
803670936843001900,Tue Nov 29 18:44:17 +0000 2016,Check out our in-depth analysis of #websummit2016 tweets and insights re events &amp; speakers! https://t.co/IBvVpXnRef @_aylien @WebSummit
803667390164856800,Tue Nov 29 18:30:11 +0000 2016,Are you in Dublin on Dec 13 and interested in #NLProc? Then join us at the 4th #NLPDublin meetup! https://t.co/DwYPw3qnFh
803309768710516700,Mon Nov 28 18:49:08 +0000 2016,Andrew Brock from Edinburgh talks about neural photo editing at the #MLDublin meetup https://t.co/2FUyVFhUAI
803281169840304100,Mon Nov 28 16:55:29 +0000 2016,@microth @haldaume3 UB was a great experience. RepL4NLP@ACL also had great speakers. Hope to see more NIPS-style workshops at ACL events.
803214363771490300,Mon Nov 28 12:30:01 +0000 2016,Ever wanted to train word embeddings across languages? Check out my new post on a survey of cross-lingual models! https://t.co/EfE9y2nUrL
803213046512500700,Mon Nov 28 12:24:47 +0000 2016,Check out @dennybritz's new edition of The Wild Week in AI newsletter #machinelearning #DeepLearning https://t.co/RVxC4HMky3
803196952825462800,Mon Nov 28 11:20:50 +0000 2016,I'll be at #nips2016 next week. Let me know if you'd like to chat abt domain adapt, X-lingual, transfer learning or generally abt #NLProc.
802498713897799700,Sat Nov 26 13:06:17 +0000 2016,RT @_aylien: Fancy joining a fast-growing startup in Dublin? AYLIEN is looking for a Site Reliability Engineer (SRE). https://t.co/6hPgbeAs‚Ä¶
802288767570612200,Fri Nov 25 23:12:02 +0000 2016,@benwilson_ml @Smerity @NipsConference @uselateral This is awesome! Thanks a lot for this!
801871008332451800,Thu Nov 24 19:32:01 +0000 2016,Disseminate information to your team through a week-in-review rather than a meeting https://t.co/SUOdfAWRRK #productivity
801821273928192000,Thu Nov 24 16:14:23 +0000 2016,"Are we solving AI or accelerating progress towards the next AI winter? Is there a difference?" asks  @fhuszar https://t.co/ERqAmmMdXO
801742300137324500,Thu Nov 24 11:00:34 +0000 2016,@yoavgo https://t.co/FtSpRS3GAL
801158516849442800,Tue Nov 22 20:20:49 +0000 2016,@iamaidang @jackclarkSF @rsalakhu I spent an embarrassingly long time pondering why Apple would spend time researching fruit detection.. üò∞
801155165546041300,Tue Nov 22 20:07:30 +0000 2016,@jackclarkSF @iamaidang I've been tricked! üòµ
801136917395992600,Tue Nov 22 18:55:00 +0000 2016,Kick-off at the inaugural @dublinAI event #dublinai https://t.co/XhPAnoCVEd
801015736265220100,Tue Nov 22 10:53:28 +0000 2016,@charhkcer Well spotted! Sure, go ahead. :)
801002658920329200,Tue Nov 22 10:01:30 +0000 2016,@SashaVNovikov @Smerity @deliprao  Should be able to narrow choices or even cross the tree greedily. Not sure if done in practice. Thoughts?
801002306187759600,Tue Nov 22 10:00:06 +0000 2016,@SashaVNovikov @Smerity @deliprao My intuition was that you still need to calculate probs for all words to know most likely one.
801000683424071700,Tue Nov 22 09:53:39 +0000 2016,@charhkcer Hey Andy, thanks a lot for the feedback. Would you mind pointing me to the blog post and the line where you spotted the error?
800823376780599300,Mon Nov 21 22:09:06 +0000 2016,@hugo_larochelle Hey Hugo, that's awesome news! Congrats!
800617140546109400,Mon Nov 21 08:29:35 +0000 2016,@deliprao @Smerity thanks for the shout out. Glad about all feedback!
798872016002707500,Wed Nov 16 12:55:05 +0000 2016,RT @parsaghaffari: Hello world. We're looking for a Site Reliability Engineer (#SRE) @_aylien and would appreciate intros: https://t.co/PhA‚Ä¶
798853731680780300,Wed Nov 16 11:42:26 +0000 2016,RT @NLPDublin: The 4th #NLPDublin meetup takes place on Dec 13. Join us there to talk about multi-modal MT &amp; other #NLProc things! https://‚Ä¶
798666563889012700,Tue Nov 15 23:18:41 +0000 2016,RT @filar: The @_aylien blog continues to crush it. This time w/ "Highlights of EMNLP 2016: Dialogue, deep learning, and more" https://t.co‚Ä¶
798311303920152600,Mon Nov 14 23:47:01 +0000 2016,Didn't make it to @emnlp2016? Check out my collection of highlights of the conference:  https://t.co/2SuSGvs3rw @_aylien #NLProc
798251654445305900,Mon Nov 14 19:49:59 +0000 2016,RT @parsaghaffari: Great writeup on @emnlp2016 by @seb_ruder for those of us who couldn't attend: https://t.co/Cz4kchyMMh #deeplearning #nl‚Ä¶
796032844678234100,Tue Nov 08 16:53:14 +0000 2016,@NeillGernon @AndySmyth6 @willmcq @polomuiriu Cool initiative re @dublinAI. Looking forward to the first event!
796015916693721100,Tue Nov 08 15:45:58 +0000 2016,RT @_aylien: Hello Lisbon! We've arrived at #WebSummit. If you're attending &amp; interested in learning more about what we do, DM us &amp; we'll h‚Ä¶
795962307692101600,Tue Nov 08 12:12:57 +0000 2016,@adgaudio Thanks, Alex! Really appreciate your message. :)
795265523378425900,Sun Nov 06 14:04:10 +0000 2016,RT @microth: Such feedback definitely makes us want to try and repeat! Maybe as a workshop in 2018? We are going to discuss and will see! :‚Ä¶
795259306329440300,Sun Nov 06 13:39:28 +0000 2016,RT @KirkDBorne: Overview of gradient descent optimization algorithms: https://t.co/Wj41fMlYc7 #BigData #DataScience #MachineLearning #Tenso‚Ä¶
794988431717912600,Sat Nov 05 19:43:06 +0000 2016,David DeVault shows how to negotiate with a dialogue agent at the Uphill Battles workshop at #emnlp2016 https://t.co/ef7D4TXlXm
794918323100192800,Sat Nov 05 15:04:31 +0000 2016,Star-studded panel on text understanding at the uphill battles in language processing workshop at #emnlp2016 https://t.co/HrcocMHMt1
794587776305938400,Fri Nov 04 17:11:03 +0000 2016,@fhuszar ICLR 2017, right? üòâ
794549117225750500,Fri Nov 04 14:37:26 +0000 2016,RT @revue: @dennybritz The Wild Week In AI is truly a great digest so we had to feature it üòä https://t.co/Nb5z4iiJOP
794274369795035100,Thu Nov 03 20:25:41 +0000 2016,Check out our @iclr2017 submission on a teacher-student model for domain adaptation https://t.co/uC8FAttmo7 cc @johnbreslin @parsaghaffari
793889247778308100,Wed Nov 02 18:55:21 +0000 2016,RT @parsaghaffari: Looking to do an industry-based MSc/PhD in #NLProc and #DeepLearning in 2017? We've launched a joint program with @Irish‚Ä¶
793209531882803200,Mon Oct 31 21:54:24 +0000 2016,The student volunteers after preparing 700 attendee bags &amp; badges for @emnlp2016! Great work, guys! #NLProc https://t.co/k8D4o7nnHg
792807637603668000,Sun Oct 30 19:17:25 +0000 2016,@jonathonmorgan @partiallyd It was great meeting you too, Jonathon! Hope to stay in contact!
792802477146079200,Sun Oct 30 18:56:54 +0000 2016,Austin craft beers and muffuletta with @jonathonmorgan @partiallyd https://t.co/iOx0CEXYje
791999087012876300,Fri Oct 28 13:44:31 +0000 2016,RT @_aylien: Going to Lisbon for #websummit2016? Drop by our stand, say hello, and see what we're up to in the world of #NLProc &amp; #MachineL‚Ä¶
791759187705524200,Thu Oct 27 21:51:15 +0000 2016,RT @NLPDublin: The slides of last event's talks on dep-based MT &amp; cross-lingual doc alignment are now up on the event page! https://t.co/lb‚Ä¶
791755064905990100,Thu Oct 27 21:34:52 +0000 2016,I'll be at @emnlp2016 in Austin next week. I'd love to meet up and chat about research! Let me know if you're interested. #NLProc
791751936496267300,Thu Oct 27 21:22:26 +0000 2016,RT @NLPDublin: Join us for the 4th #NLPDublin meetup on November 28! Stay tuned for the topic's of the talks! https://t.co/o6AVWD5KSh #NLPr‚Ä¶
791413886268432400,Wed Oct 26 22:59:08 +0000 2016,RT @AfliHaithem: First @AdaptCentre presentation at #NLPmeetup by Prof. @LiuQunMTtoDeath and Liangyou Li https://t.co/LbMXArprUC
791318128278069200,Wed Oct 26 16:38:38 +0000 2016,RT @_aylien: We're looking for collaborators from academia as part of the SFI Industry Fellowship programme (Dublin, Ireland) https://t.co/‚Ä¶
791015752539246600,Tue Oct 25 20:37:06 +0000 2016,RT @RaphaelWouters: Heading to 3rd @NLPDublin meetup at @intercom regarding "Dependency Relations" https://t.co/i97Yc3O7v2 #nlproc #nlpdubl‚Ä¶
791014553815490600,Tue Oct 25 20:32:20 +0000 2016,RT @NLPDublin: Pintu Lohar talks about cross-lingual document alignment for the 2nd talk of #nlpdublin meetup hosted by @intercom https://t‚Ä¶
791014473645555700,Tue Oct 25 20:32:01 +0000 2016,RT @NLPDublin: .@LiuQunMTtoDeath and Liangyou Li talk about dependency based SMT for the 1st talk of the event hosted by @intercom https://‚Ä¶
790929277785104400,Tue Oct 25 14:53:29 +0000 2016,@manaalfar @jasonbaldridge @eponvert @stephenroller @josephreisinger Thanks for the tip, guys!
790851310765674500,Tue Oct 25 09:43:40 +0000 2016,Can anyone recommend what's the best spot to print posters in Austin? @jasonbaldridge @eponvert @stephenroller @josephreisinger
789860737527255000,Sat Oct 22 16:07:29 +0000 2016,@erfannoury Argh!
789844678766161900,Sat Oct 22 15:03:40 +0000 2016,@erfannoury That's annoying! Can you use a VPN or something?
789437869668569100,Fri Oct 21 12:07:09 +0000 2016,@nlothian @paulg Good point. I've always thought of Sapir-Whorf as only concerning language; SW pertains to thought -- is writing different?
789427945244880900,Fri Oct 21 11:27:43 +0000 2016,@paulg Oh, this is interesting. Is anyone aware if people have looked at what impact changing one's style might have?
788465069101289500,Tue Oct 18 19:41:36 +0000 2016,Great initiative to learn, collaborate and conduct research in the open. Can't wait to learn &amp; work with awesome pe‚Ä¶ https://t.co/kVrCD5JhY0
786950783027798000,Fri Oct 14 15:24:22 +0000 2016,Very cool post that provides intuitions for understanding t-SNE with lots of neat visualizations https://t.co/lU8i2iHBR6 #dataviz
786559487125688300,Thu Oct 13 13:29:29 +0000 2016,RT @NLPDublin: Only 10 spots left for our next #NLProc meetup on October 25! Join us for exciting talks by @LiuQunMTtoDeath et al. https://‚Ä¶
786121215509405700,Wed Oct 12 08:27:57 +0000 2016,RT @NLPDublin: RSVPs for our next #NLProc meetup on October 25 hosted by @intercom are now open! See you all there! https://t.co/lb4QmpkOEd
783924950767902700,Thu Oct 06 07:00:47 +0000 2016,@robert_winslow On the backlog. Just haven't gotten to it yet. üòä
783383152417005600,Tue Oct 04 19:07:52 +0000 2016,Working in academia in Ireland and curious about industry? Join us for 6-12 months with an SFI Fellowship! https://t.co/0oYopmHl4p
783328999544131600,Tue Oct 04 15:32:41 +0000 2016,RT @NLPDublin: We've just announced our 3rd #NLProc meetup on October 25 hosted by @intercom! Make sure to save the date! https://t.co/lb4Q‚Ä¶
783207024033042400,Tue Oct 04 07:28:00 +0000 2016,Why #DeepLearning is suddenly changing your life via @FortuneMagazine https://t.co/qfiNUFSE4u
783001854984777700,Mon Oct 03 17:52:44 +0000 2016,RT @DevilleSy: Your paper, after reviewer 3 asked for improvements to the manuscript. http://t.co/YoNFvgc8Aa
782940519861071900,Mon Oct 03 13:49:00 +0000 2016,Cool blog post on binary stochastic neurons, w/ implementation in @TensorFlo and extensive experiments https://t.co/kCOOoKAEXn #deeplearning
782887921120673800,Mon Oct 03 10:20:00 +0000 2016,The Self Learning Quant -- teaching a RL model to buy and sell stocks https://t.co/9l9JeIqBNO #deeplearning
782162392214011900,Sat Oct 01 10:17:00 +0000 2016,Hierarchical Multiscale Recurrent Neural Networks, SoTA on PTB; notes by @jimmfleming https://t.co/ER6zSAn2Pu #deeplearning
781848578151022600,Fri Sep 30 13:30:01 +0000 2016,The 7,000$ CS Master's Degree at @GeorgiaTech. Awesome! via @udacity @SebastianThrun https://t.co/uGsQTI0G5s #edutech
781771313815949300,Fri Sep 30 08:23:00 +0000 2016,Google's NMT system is Neural Machine Translation at scale, reduces translation errors by 55%-85%‚Ä¶ https://t.co/MuIXEDfNpf
781766634465787900,Fri Sep 30 08:04:24 +0000 2016,RT @zeigenvector: Who called it Tinder and not Naive Baes Classifier?
781442363214430200,Thu Sep 29 10:35:52 +0000 2016,RT @_aylien: Another great turnout and more fantastic speakers at the second @NLPDublin meetup last night! https://t.co/lOHAGcTAxZ #NLProc‚Ä¶
781243846562877400,Wed Sep 28 21:27:02 +0000 2016,Amazon, Google, IBM, FB, &amp; MS announce non-profit Partnership on AI to advance understanding, set best practices https://t.co/0JBL7vbYoU
781200378704658400,Wed Sep 28 18:34:18 +0000 2016,RT @NLPDublin: Piyush Arora talks about estimating the quality of @StackOverflow questions for our last talk of the evening https://t.co/dD‚Ä¶
781199892177883100,Wed Sep 28 18:32:22 +0000 2016,RT @ParisNLP: #UnitedMeetups The first Paris and Dublin NLP Meetups on the same day. Let's create a huge European NLP community! https://t.‚Ä¶
781193816258805800,Wed Sep 28 18:08:14 +0000 2016,RT @NLPDublin: .@derekgreene talks about dynamic modeling at the second #NLProc Dublin meetup hosted by @ZalandoTech https://t.co/ik1n2tqo3s
781186115017515000,Wed Sep 28 17:37:38 +0000 2016,RT @NLPDublin: .@giancds talks about idiom token classification to a full audience at the second #NLProc meetup. https://t.co/4ktMXdUnpq
780843200319721500,Tue Sep 27 18:55:00 +0000 2016,DeepBench -- a new #deeplearning benchmarking tool by @BaiduResearch https://t.co/tSPirJntEP
780813756012060700,Tue Sep 27 16:58:00 +0000 2016,CommAI-env -- @fbOpenSource's communication-based environment to pave the way to a general AI https://t.co/34ZfnuhJCu #MachineLearning
780767450669256700,Tue Sep 27 13:54:00 +0000 2016,Pointer Sentinel Mixture Models w/ pointer + softmax back-off by @Smerity @RichardSocher https://t.co/negiECFt5a #deeplearning
780731269852368900,Tue Sep 27 11:30:14 +0000 2016,RT @NLPDublin: Are you ready for tomorrow's #NLProc meetup on idiom classification, dynamic topic modelling, &amp; StackOverflow QA? https://t.‚Ä¶
780685647526977500,Tue Sep 27 08:28:57 +0000 2016,Paper on Google's Neural MT system: Bridging the Gap between Human and Machine Translation https://t.co/yhbGSlxQKu #MachineLearning
780461797447008300,Mon Sep 26 17:39:27 +0000 2016,The #MLDublin meetup kicks off with an application of word embeddings for IR hosted by @websummit https://t.co/m25FmPNfe8
780438048299364400,Mon Sep 26 16:05:05 +0000 2016,cleverhans, a library by @goodfellow_ian to benchmark #MachineLearning models' against adversarial examples https://t.co/EogatELb8q
780407943888011300,Mon Sep 26 14:05:27 +0000 2016,@krishpopp Thanks! üòä
780382142073307100,Mon Sep 26 12:22:56 +0000 2016,RT @anotherjohng: Nice talk on Adversarial Networks by @soumithchintala: https://t.co/Mg3hi4aYKM
780315730159939600,Mon Sep 26 07:59:02 +0000 2016,#MachineLearning with Weak Supervision blog post on #nips2016 paper &amp; open-source framework Snorkel by @HazyResearch https://t.co/PzuXBwZLXE
779814434436943900,Sat Sep 24 22:47:04 +0000 2016,@karpathy This is so awesome! Thanks for the link!
779783060065755100,Sat Sep 24 20:42:23 +0000 2016,Nice blog post on language modelling on the 1B Word Benchmark &amp; when (not) to use LSTMs by @deliprao https://t.co/Lo5z8HdjYB #deeplearning
779751563455787000,Sat Sep 24 18:37:14 +0000 2016,RT @withfries2: @karpathy is live now at Bay Area Deep Learning School starting off with history of neural nets https://t.co/j7GYsgb5fC
779724199246037000,Sat Sep 24 16:48:30 +0000 2016,The secret ingredients of word2vec -- new blog post on word2vec's success factors and its link to dist semantics https://t.co/Iw0zVv1Qsv
779724199246037000,Sat Sep 24 16:48:30 +0000 2016,The secret ingredients of word2vec -- new blog post on word2vec's success factors and its link to dist semantics https://t.co/Iw0zVv1Qsv
779587931249872900,Sat Sep 24 07:47:01 +0000 2016,Where Will Artificial Intelligence come from? -- interesting blog post by @sebnowozin https://t.co/YbTsQcmE2O
779376791714566100,Fri Sep 23 17:48:01 +0000 2016,Supervision by constraining the output space, e.g. to the laws of physics https://t.co/2dmj1P06kd #MachineLearning #deeplearning
779330503602085900,Fri Sep 23 14:44:05 +0000 2016,Facebook and Intel win in Doom AI deathmatch competition https://t.co/mn6fQjBO29 #deeplearning
779214505507168300,Fri Sep 23 07:03:09 +0000 2016,#AINow White House Report on the social &amp; economic implications of #MachineLearning on healthcare, labor, inequality https://t.co/yZr07xJueO
779033954749939700,Thu Sep 22 19:05:43 +0000 2016,@AnaPopescu_SV Haha, I can certainly identify with that. :)
779033519439904800,Thu Sep 22 19:03:59 +0000 2016,@AnaPopescu_SV Indeed. Lots of cool things going on and many things to try. :) Are you excited to start again?
779032510038700000,Thu Sep 22 18:59:58 +0000 2016,@AnaPopescu_SV Sure. :) What are you using in your own work?
779032197630156800,Thu Sep 22 18:58:44 +0000 2016,@AnaPopescu_SV Mostly tensorflow.
779031133342294000,Thu Sep 22 18:54:30 +0000 2016,@AnaPopescu_SV Thanks. :)
779030527500283900,Thu Sep 22 18:52:06 +0000 2016,Check out a new blog post on our recent @emnlp2016 paper on aspect-based sentiment analysis @_aylien https://t.co/99RswTVUEn #deeplearning
778648130757812200,Wed Sep 21 17:32:35 +0000 2016,@TheShubhanshu None I'm aware of. Most of it seems to be unwritten consensus, tips &amp; tricks. Best thing is prob to explore the kaggle forums
778646483922919400,Wed Sep 21 17:26:02 +0000 2016,Need to maintain code in multiple repos? Copybara, a new tool by @google allows you to move code between repos. https://t.co/nkdRzBs7uk
778637158588510200,Wed Sep 21 16:48:59 +0000 2016,@TheShubhanshu Instead of using 0/1, you use density/frequency/likelihood of the feature for encode. See also here: https://t.co/tuEDzbnCqX
778615783886696400,Wed Sep 21 15:24:03 +0000 2016,Cool, extensive presentation about tips &amp; tricks to win at @kaggle competitions https://t.co/vX9bEnDCUB #MachineLearning #DataScience
778559260993216500,Wed Sep 21 11:39:27 +0000 2016,On-demand Buddhist priests are now a thing in Japan https://t.co/IDG8eOHLUE #sharingeconomy
778509574718361600,Wed Sep 21 08:22:01 +0000 2016,One of the classics: Adaptive Mixtures of Local Experts (Jacobs et al., 1991) https://t.co/snI2sIG9Qa #MachineLearning
778247605562159100,Tue Sep 20 15:01:02 +0000 2016,Want your models to be leaner? Romero et al. (2015) show how to train thin, deep nets https://t.co/0IguTwsELq #deeplearning
778186966990925800,Tue Sep 20 11:00:05 +0000 2016,Knowledge distillation allows transferring knowledge from a large, cumbersome model to a small one https://t.co/NdoEqpehFy #deeplearning
778165549968793600,Tue Sep 20 09:34:59 +0000 2016,@cigilt @AdaptCentre @tarfandy Congrats! üëè
777908821373124600,Mon Sep 19 16:34:50 +0000 2016,Today is International Talk Like a Pirate Day. Is this also an official holiday of the R community? #DataScience
777885145172959200,Mon Sep 19 15:00:45 +0000 2016,For the buccaneering subgroup of data scientists, there is no question whether to use Python or R. #DataScience https://t.co/hOykz8loMA
777822814858387500,Mon Sep 19 10:53:04 +0000 2016,Energy functions make a comeback with the Energy-based Generative Adversarial Network https://t.co/mUIBMrqowF #deeplearning
777459660139835400,Sun Sep 18 10:50:02 +0000 2016,The Neural Network Zoo is getting crowded via @asimovinstitute https://t.co/66oucroyrB #deeplearning
777419064549539800,Sun Sep 18 08:08:43 +0000 2016,@fchollet @dennybritz and I have been thinking about this before. Would love to participate.
777324035692634100,Sun Sep 18 01:51:06 +0000 2016,Think your model is confidential because people can only access it through an API? Think again. https://t.co/SuYCMzgH5J #MachineLearning
777189376266674200,Sat Sep 17 16:56:01 +0000 2016,New Reinforcement Learning framework from @TwitterEng on @TorchML -- with @OpenAI gym integration and gif creation! https://t.co/tXG1oO3K4x
777156929843966000,Sat Sep 17 14:47:05 +0000 2016,Is #DeepLearning the New 42? -- Great panel with @pmddomingos, @NandoDF et al. at #KDD2016 https://t.co/5uM4SCiUbG
777090978721173500,Sat Sep 17 10:25:01 +0000 2016,Great interview with Elon Musk about the things that will have the biggest future impact by @sama https://t.co/NQF8DJBTld
776718492569567200,Fri Sep 16 09:44:54 +0000 2016,The next #NLProc Dublin meetup is almost booked out! Join us for an evening of NLP research as long as you can! https://t.co/7ULGn4f99g
776064763960885200,Wed Sep 14 14:27:12 +0000 2016,RT @NLPDublin: RSVPs to our second #NLProc meetup are now open! See you all on September 28! https://t.co/yrm8O0DSoA
775333505614094300,Mon Sep 12 14:01:27 +0000 2016,Our @emnlp2016 paper on aspect-based sentiment analysis is now on arXiv @parsaghaffari @johnbreslin https://t.co/Oqa26tvzz6
774643996966711300,Sat Sep 10 16:21:35 +0000 2016,RT @fastml_extra: An introduction to Generative Adversarial Networks (with code in TensorFlow): https://t.co/sAYqeoOzzP #TensorFlow #GAN‚Ä¶
774249059779420200,Fri Sep 09 14:12:15 +0000 2016,Awesome post with amazing, interactive figures on Attention and Augmented RNNs by @ch402 @shancarter https://t.co/CsF2rEz7H9 #deeplearning
774219578612211700,Fri Sep 09 12:15:06 +0000 2016,Ask a Female Engineer -- honest answers from female engineers about inclusion and diversity via @ycombinator https://t.co/BHDhcfTLJe
774210843848351700,Fri Sep 09 11:40:23 +0000 2016,Extremely looking forward to discussing uphill battles, early goals, and challenges in dialogue and language understanding.
774210308168712200,Fri Sep 09 11:38:16 +0000 2016,My abstract on modelling NL domains has been accepted to the Uphill Battles in NL workshop at @emnlp2016!  https://t.co/T6CuAYrh8t
774133540006932500,Fri Sep 09 06:33:13 +0000 2016,More unspoken rules of English from the author of the viral English-adjective-order paragraph https://t.co/CDWJhYr1h1 #linguistics
774022109446213600,Thu Sep 08 23:10:26 +0000 2016,Check out @karpathy's Quora session for insightful answers about #AI, @OpenAI, and #MachineLearning https://t.co/KYVN4F3bwc
774002238637375500,Thu Sep 08 21:51:28 +0000 2016,@sap_negi Thanks!
773955575185014800,Thu Sep 08 18:46:03 +0000 2016,Awesome work by @avdnoord @sedielem! WaveNet - Generating human-like voices and music as raw waveforms https://t.co/cj3KYpzZi6 #deeplearning
773918861175423000,Thu Sep 08 16:20:09 +0000 2016,@haldaume3 "beer or your preferred beverage" did not fit on the original tweet unfortunately üòÖ
773916808600510500,Thu Sep 08 16:12:00 +0000 2016,@micaleel @DatSciAwards Thanks! :D
773902333918969900,Thu Sep 08 15:14:29 +0000 2016,Stoked to be a finalist for the 2016 DatSci Student of the Year Award! @DatSciAwards #DataScience https://t.co/P5itqnqtyx
773876174615437300,Thu Sep 08 13:30:32 +0000 2016,RT @_aylien: Check out @mSchmitz_ #DataScience reading list. This week features our research scientist @seb_ruder's blog - https://t.co/PXi‚Ä¶
773813319698358300,Thu Sep 08 09:20:46 +0000 2016,@lucie_nlp @karpathy Totally agree! For me, the first stage was "Too awed to approach Prof. X".
773807519353438200,Thu Sep 08 08:57:43 +0000 2016,3 stages of a PhD: 1. Haven't read most related work 2. Recognize all related work 3. Shared a beer with 1st authors of all papers @karpathy
773796030231871500,Thu Sep 08 08:12:04 +0000 2016,The 9 #DeepLearning Papers You Need To Know About -- with a focus on CNNs by @aditdeshpande3 https://t.co/y8tmYVMbyI
773735140740567000,Thu Sep 08 04:10:07 +0000 2016,The importance of benchmarks in #MachineLearning &amp; a walk-through of the mother of benchmarks, MNIST by @djkust  https://t.co/tyBBYxkhr9
773630580315545600,Wed Sep 07 21:14:38 +0000 2016,Awesome survival guide to a PhD detailing how to choose an adviser, find a topic, write good papers etc by @karpathy https://t.co/7xjMtBYDet
773284277748273200,Tue Sep 06 22:18:33 +0000 2016,The Evolution of #DataScience at @Airbnb via @kaggle https://t.co/2fB8ycV5RT
773257353688932400,Tue Sep 06 20:31:34 +0000 2016,Fairness in #MachineLearning -- and why democratic parity is not the way to go by @mrtz https://t.co/C4BZN3PiMa
773181045927579600,Tue Sep 06 15:28:21 +0000 2016,Great summary of the #DeepLearning Summer School 2016 in Montreal https://t.co/1P9D8fbOm8
773143991500013600,Tue Sep 06 13:01:06 +0000 2016,Science in the age of selfies: how cultural shift may lead to fewer fundamental breakthroughs https://t.co/GzKPELHnAV
772904671417606100,Mon Sep 05 21:10:08 +0000 2016,@cosmosquared Nice one! Enjoy!
772807252990451700,Mon Sep 05 14:43:01 +0000 2016,Great, actionable list on how to create more inclusivity in #computerscience classes! https://t.co/HdwPsyfh3K
771776978340892700,Fri Sep 02 18:29:05 +0000 2016,#ICML2016 Tutorial on Deep Reinforcement Learning by David Silver https://t.co/Zkoia51h03 #deeplearning
771735001721409500,Fri Sep 02 15:42:17 +0000 2016,@naveenaly @nednadima Thanks for the shout-out, Naveen! :D
771730667285557200,Fri Sep 02 15:25:03 +0000 2016,Stuart Russell leads new Center for Human-Compatible #ArtificialIntelligence at @UCBerkeley https://t.co/EYoqv0rnJd
771707555500396500,Fri Sep 02 13:53:13 +0000 2016,@natebrix @DeepMindAI @maxjaderberg That's the core of it AFAIK.
771690404525727700,Fri Sep 02 12:45:04 +0000 2016,Clear blog post explaining the motivation behind Synthetic Gradients by @DeepMindAI's @maxjaderberg https://t.co/EYoqv0rnJd #deeplearning
771628495726268400,Fri Sep 02 08:39:04 +0000 2016,The best research / PhD tumblr I've seen in a while: situations in the life of a grad student re-enacted with lego https://t.co/3CcZlmawwI
771390853680070700,Thu Sep 01 16:54:45 +0000 2016,RT @_aylien: We're delighted to introduce our new advisors. Welcome to the team @barrysmyth &amp; @JimiShan! https://t.co/scgxHaSJQG https://t.‚Ä¶
771384142298611700,Thu Sep 01 16:28:05 +0000 2016,Detailed analysis (with p-values!) of how the CTO of @overleaf spent his time over one year https://t.co/vdkDHS886k #technology
771345906041917400,Thu Sep 01 13:56:09 +0000 2016,.@BaiduResearch joins the #DeepLearning library landscape with Paddle https://t.co/cjdnih8Dze
771287525168074800,Thu Sep 01 10:04:10 +0000 2016,Great, clear post on Source Code Classification using #DeepLearning by @montenegrodr via @_aylien https://t.co/I9G68YIUTr
771245235984748500,Thu Sep 01 07:16:07 +0000 2016,.@OpenAI's Infrastructure for #DeepLearning -- with Kubernetes Auto-scaling on AWS EC2 at its heart https://t.co/kVaismPku1
771068448558485500,Wed Aug 31 19:33:38 +0000 2016,A Letter to My Younger Self (and all us PhD students out there) About Dealing with Rejection in Academia by @scyrusk https://t.co/4jy8bkJHID
769224637867851800,Fri Aug 26 17:26:59 +0000 2016,Create refined segmentations with FB's #computervision trifecta: DeepMask, SharpMask, MultiPathNet https://t.co/uTI8sbVbSk #deeplearning
769099902400008200,Fri Aug 26 09:11:20 +0000 2016,Highlights of the #DeepLearning Summer School in Montreal: cross-modal learning, VAE &amp; RNN variations by @vkrakovna https://t.co/E2LSlUvOqz
768833420696911900,Thu Aug 25 15:32:26 +0000 2016,An Introduction to Generative Adversarial Networks (with @TensorFlo code) by @anotherjohng https://t.co/SF82T1Lqgq @_aylien #deeplearning
768797861282668500,Thu Aug 25 13:11:08 +0000 2016,The key to excelling at duplicate ads detection: XGBoost and lots of feature engineering via @kaggle https://t.co/30G3rLMcO8
768754566779154400,Thu Aug 25 10:19:06 +0000 2016,The lectures of the #DeepLearning Summer School 2016 in Montreal are online. Amazing material by inspiring speakers! https://t.co/cHaoCvHqEs
768733973413556200,Thu Aug 25 08:57:16 +0000 2016,Debugging #MachineLearning how to identify different types of errors in your ML approach via @haldaume3 https://t.co/Tec423uK3D #NLProc
768533586190463000,Wed Aug 24 19:41:00 +0000 2016,@micaleel Haha, well played! üòÇ
768532780716294100,Wed Aug 24 19:37:48 +0000 2016,@micaleel Definitely talk to @snneeggii if you're interested in suggestion mining.
768506454399680500,Wed Aug 24 17:53:11 +0000 2016,RT @_aylien: An Introduction to Generative Adversarial Networks (with code in TensorFlow) https://t.co/gTK7ez2AA3 #deeplearning https://t.c‚Ä¶
768470649035690000,Wed Aug 24 15:30:54 +0000 2016,@IAugenstein @haldaume3 @DCNLP @uclmr You guys are awesome! Would love to fly over &amp; attend every meetup :) Have you considered periscoping?
768468840246284300,Wed Aug 24 15:23:43 +0000 2016,RT @haldaume3: Super active meetup on the other side of the pond in DC too (@DCNLP) if you're ever in our neck of the woods :) https://t.co‚Ä¶
768440989123948500,Wed Aug 24 13:33:03 +0000 2016,The #NLProc meetup scene is heating up: NLP meetups in both Paris &amp; Dublin on September 28. Join us if you're around https://t.co/mF7lvv8AqM
768435382715293700,Wed Aug 24 13:10:46 +0000 2016,RT @unlp_insight: A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis by @seb_ruder from @insight_centre @nuigalway #signlp‚Ä¶
768345064720437200,Wed Aug 24 07:11:53 +0000 2016,RT @NLPDublin: Great to see @ParisNLP have their first meetup on the day of our second event. Have a blast, guys! https://t.co/yrm8O0DSoA #‚Ä¶
768051480662122500,Tue Aug 23 11:45:17 +0000 2016,Great tutorial of Deep Deterministic Policy Gradients in @TensorFlo by @PatrickOmid https://t.co/UayMEVkSrP #deeplearning
767731075980234800,Mon Aug 22 14:32:06 +0000 2016,Delve into the undocumented features of @TensorFlo RNNs with @dennybritz's new blog post https://t.co/NIwNZfrDrm #deeplearning
767675201223921700,Mon Aug 22 10:50:05 +0000 2016,A great exploration on what would it mean if English used not words but characters as in Chinese https://t.co/I0LESI7ABP #linguistics
767667160432906200,Mon Aug 22 10:18:08 +0000 2016,RT @kdnuggets: Lisbon #MachineLearning Summer School Highlights - @_Aylien https://t.co/mI6pMji9O2 https://t.co/QISQM0f2jf
767662257950101500,Mon Aug 22 09:58:39 +0000 2016,RT @NLPDublin: The full agenda for our next event on September 28 is now online. See you all there! https://t.co/yrm8O0DSoA #NLProc #deeple‚Ä¶
767645844313149400,Mon Aug 22 08:53:26 +0000 2016,End-to-end Deep Learning for Self-Driving Cars blog post and paper by @nvidia https://t.co/p2XLF3VdK9 https://t.co/P0UxteTM4W #deeplearning
767424556139249700,Sun Aug 21 18:14:06 +0000 2016,RT @dennybritz: New Post: RNNs in Tensorflow, a Practical Guide and Undocumented Features https://t.co/Pwdz0XcyJl
767026947775950800,Sat Aug 20 15:54:09 +0000 2016,@haldaume3 üòÅ
767026204620755000,Sat Aug 20 15:51:12 +0000 2016,@haldaume3 Oh, even better! Do you have a link?
767025904363143200,Sat Aug 20 15:50:00 +0000 2016,@haldaume3 Will that be a lecture in your CMSC 726 ML course? Any chance that the slides/lecture notes will be put online?
766916902883041300,Sat Aug 20 08:36:52 +0000 2016,RT @jefkine: @seb_ruder post on mathematics behind weight initialization in DNN: https://t.co/sj0ovtQqSN
766916867499974700,Sat Aug 20 08:36:44 +0000 2016,@jefkine Thanks for the link! Awesome post! üòÅ
766665077059625000,Fri Aug 19 15:56:12 +0000 2016,@bluemonk482 @socialsemantics @emnlp2016 @parsaghaffari @johnbreslin Should be released soon.
766618936959991800,Fri Aug 19 12:52:52 +0000 2016,RT @socialsemantics: Accepted for @emnlp2016! "A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis" by @seb_ruder @parsagha‚Ä¶
766605395871883300,Fri Aug 19 11:59:03 +0000 2016,Exciting new @DeepMindAI paper goes beyond backprop by decoupling NN layers, predicting synthetic gradients  https://t.co/3r0KBlvAjl #dlearn
766340500995600400,Thu Aug 18 18:26:28 +0000 2016,RT @gdb: Conferences drive a lot of progress in machine learning. We think it's worth experimenting with different formats: https://t.co/9U‚Ä¶
765604209261703200,Tue Aug 16 17:40:42 +0000 2016,@haldaume3 @OpenAI good point... üòû
765591607936741400,Tue Aug 16 16:50:37 +0000 2016,.@OpenAI adds more prolific researchers and software engineers to its team https://t.co/zD3ih9wkaL #deeplearning
765559319890985000,Tue Aug 16 14:42:19 +0000 2016,@partiallyd Hey my fav podcasters &amp; data scientists, I'll be in Austin Oct 29 - Nov 6 attending EMNLP 2016. Care to meet up?
765196356868829200,Mon Aug 15 14:40:02 +0000 2016,RT @dennybritz: The Wild Week in AI #22 - Intel + Nervana, ACL 2016, Image Completion in TF, WikiReading data + paper https://t.co/GYsCp1EO‚Ä¶
765166229229142000,Mon Aug 15 12:40:19 +0000 2016,An intuitive definition of the Label Bias Problem in structured prediction by @earnmyturns: https://t.co/ILtWZ8CdLe #NLProc
765116523375685600,Mon Aug 15 09:22:49 +0000 2016,.@haldaume3's favourite papers at #acl2016berlin https://t.co/y26UDBd5Ac #NLProc
764844282008043500,Sun Aug 14 15:21:01 +0000 2016,Another insightful summary by @karpathy, this time on Value Iteration Networks for RL https://t.co/A4KB4Uck6h #deeplearning
764808828584157200,Sun Aug 14 13:00:08 +0000 2016,RT @lmthang: Very thorough survey on scaling softmax! we missed this ref when discuss abt vocab prob in NMT! @chrmanning @kchonyc https://t‚Ä¶
764741313942224900,Sun Aug 14 08:31:52 +0000 2016,@yoavgo @haldaume3 @AnaPopescu_SV Could you elaborate a bit more on the advantages of PyCNN over other libraries?
764739260306092000,Sun Aug 14 08:23:42 +0000 2016,Very clear, intuitive summary of Matching Networks for One Shot Learning (Vinyals et al.) by @karpathy https://t.co/ZgiXbQeesO #deeplearning
764542140189999100,Sat Aug 13 19:20:25 +0000 2016,RT @dennybritz: Lisbon Machine Learning Summer School Highlights by @seb_ruder https://t.co/9J17okjLel - Great writeup!
764200949434974200,Fri Aug 12 20:44:39 +0000 2016,RT @NLPDublin: Join us at our second meetup on September 28 to talk about cutting-edge #NLProc research! https://t.co/yrm8O0DSoA
764200305458237400,Fri Aug 12 20:42:05 +0000 2016,My first #acl2016berlin is over -- I'm feeling very lucky to be able to do research in this awesome field w/ these inspiring people! #NLProc
764184462338891800,Fri Aug 12 19:39:08 +0000 2016,Answers by the Google Brain team to their #RedditAMA are now online. https://t.co/myQR9jNYvq
764159951421988900,Fri Aug 12 18:01:44 +0000 2016,Check out my blog post on the highlights of the Lisbon #MachineLearning Summer School @_aylien https://t.co/9ATa0xKwDP
764112020488777700,Fri Aug 12 14:51:16 +0000 2016,Nice compilation of some nuggets of #acl2016berlin and why they're worth reading.  https://t.co/ZhJYBIT3bO
763744576096067600,Thu Aug 11 14:31:11 +0000 2016,Such a great panel at #Repl4NLP with @kchonyc @AnimaAnandkumar @haldaume3 @redpony, Raia Hassell, Katrin Erk https://t.co/OFQRCI9TRM
763701775757115400,Thu Aug 11 11:41:06 +0000 2016,.@haldaume3 on representation learning (notably not imitation learning!) at #repl4nlp #ACL2016 https://t.co/9oW6HPlAvX
763678838471680000,Thu Aug 11 10:09:58 +0000 2016,#ACL Rep4NLP best papers: mapping unseen words, making sense of embeddings, domain adaptation w/ param augmentation https://t.co/5szD9XOAQV
763674132168974300,Thu Aug 11 09:51:16 +0000 2016,@haldaume3 @PaulMineiro Is there already more information available re submission besides what is posted in https://t.co/uOkUJUpoCx ?
763052738691366900,Tue Aug 09 16:42:04 +0000 2016,Uphill battles in #NLProc at @emnlp2016 -- such a cool workshop. Need more events that encourage pushing the limits. https://t.co/T6CuAYrh8t
762944314649481200,Tue Aug 09 09:31:14 +0000 2016,SyntaxNet goes multilingual -- with support for 40 languages, text segmentation &amp; morphological analysis https://t.co/ittnb3LftK #NLProc
761966834924957700,Sat Aug 06 16:47:04 +0000 2016,Slides of this year's #deeplearning summer school in Montreal are now available! https://t.co/DYwiJfEG3R
761921289783119900,Sat Aug 06 13:46:05 +0000 2016,Another #MachineLearning startup gets acquired. Congrats to the folks at @turiinc! https://t.co/jB6oyKFM2r
761885599255105500,Sat Aug 06 11:24:16 +0000 2016,@JayStalker1 Hey! Kudos for already wanting to learn about DL. It's never too early. I would recommend starting with https://t.co/kqoJxSkJ1V
761885187655491600,Sat Aug 06 11:22:38 +0000 2016,ICLR comes to Europe (specifically: Toulon, FR) in 2017! Psyched!! https://t.co/tgPPj48HmW #deeplearning
761563110314479600,Fri Aug 05 14:02:49 +0000 2016,RT @NLPDublin: Here are all the slides of all talks of this week's #NLProc meetup: https://t.co/sWOO6PK1yT https://t.co/79s1iP6CLm https://‚Ä¶
761251880676626400,Thu Aug 04 17:26:06 +0000 2016,Everything you need to about LSTMs &amp; how models like highway &amp; ResNets, NTMs can be derived from them https://t.co/urI8eSCqWB #deeplearning
761197278824570900,Thu Aug 04 13:49:08 +0000 2016,@ilyasut's slides on recent progress in generative modeling at #scaledmlconf https://t.co/cGNwyFbpNV #deeplearning
760908471931924500,Wed Aug 03 18:41:31 +0000 2016,RT @NLPDublin: Jing Su talks about topic modeling for speech processing during our final talk of the evening #nlproc https://t.co/aJnfrpNIom
760908421562568700,Wed Aug 03 18:41:19 +0000 2016,RT @parsaghaffari: For the second talk @seb_ruder is going to speak about softmax approximation #NLProc #NLPDublin https://t.co/M58NjGna86
760897016906059800,Wed Aug 03 17:56:00 +0000 2016,RT @NLPDublin: Duck punching, i.e. transforming a task into classification with @Mystical_Wiz #nlproc https://t.co/4TUjWtZoQj
760893653040689200,Wed Aug 03 17:42:38 +0000 2016,RT @NLPDublin: Our first talk: Task based learning for #nlproc with @Mystical_Wiz https://t.co/WSoFNkyM4B
760892085222465500,Wed Aug 03 17:36:24 +0000 2016,RT @NLPDublin: The NLP meetup kicks off! https://t.co/CchbkojiRB
760884121023447000,Wed Aug 03 17:04:45 +0000 2016,RT @NLPDublin: Our first meetup kicks off! #nlproc https://t.co/Avz6UcB3Eq
760870860043395100,Wed Aug 03 16:12:03 +0000 2016,Interesting @acl2016 paper on treating dialogue as a game, finding Nash equilibrium between mixed motives https://t.co/NAFSAEXDuh #NLProc
760838391994851300,Wed Aug 03 14:03:03 +0000 2016,Tutorial on neural machine translation with tips to improve accuracy by @gneubig https://t.co/EFPJO25vXO #deeplearning
760807694722752500,Wed Aug 03 12:01:04 +0000 2016,Innovative @acl2016 paper puts numbers in perspective by generating descriptions by @stanfordnlp https://t.co/wFwx5eySlc #NLProc
760745275979812900,Wed Aug 03 07:53:02 +0000 2016,.@acl2016 proceedings are now online! Can't wait to dive in! https://t.co/KrPPbHRPt2 #NLProc
760684627254054900,Wed Aug 03 03:52:02 +0000 2016,.@sleepinyourhat's PhD thesis on modelling natural language semantics with learned representations. Congrats! üëè  https://t.co/JIZqWlHZbR
760610148058488800,Tue Aug 02 22:56:05 +0000 2016,The 20th Wild Week in AI newsletter w/ Yann LeCun QA, OpenAI projects, Inverse RL by @dennybritz https://t.co/ZVFRa4zYqk #deeplearning
760603183014117400,Tue Aug 02 22:28:24 +0000 2016,Predicting check-ins in @fb_engineering's 5th @kaggle competition with semi-supervised learning https://t.co/PSIfAa4Mp8 #MachineLearning
760497980269035500,Tue Aug 02 15:30:22 +0000 2016,@mSchmitz_ @_aylien The algorithm behind the endpoint is similar, but optimized to also do well on single sentences w/o the review context.
760497148786008000,Tue Aug 02 15:27:04 +0000 2016,@svenvanpoucke @_aylien No journal, but the EMNLP conference. https://t.co/B7ayigZZRh
759179714837741600,Sat Jul 30 00:12:03 +0000 2016,@shriphani will post as soon as the camera-ready version is done. ‚ò∫Ô∏è
759176376750633000,Fri Jul 29 23:58:47 +0000 2016,Yay! Got my first paper accepted to #emnlp2016! A hierarchical model of reviews for aspect-based sentiment analysis. #nlproc #DeepLearning
758984297818705900,Fri Jul 29 11:15:32 +0000 2016,Intuitive, interactive visualizations &amp; explanation of Gradient Boosting trees https://t.co/drsVjNe5TI #MachineLearning
758721155217973200,Thu Jul 28 17:49:54 +0000 2016,RT @OpenAI: We're looking for machine learning experts interested in leading some important new projects: https://t.co/aGb0wCRs8P.
758673627323650000,Thu Jul 28 14:41:02 +0000 2016,Cool blog post on using reinforcement learning to tackle the Traveling Salesman problem via @stitchfix https://t.co/GX4BXkflVD #deeplearning
758655252249784300,Thu Jul 28 13:28:01 +0000 2016,Blog post on training a language model on the 1 Billion Word dataset with @TorchML https://t.co/yQB0QWw4hn #MachineLearning #deeplearning
758618505780596700,Thu Jul 28 11:02:00 +0000 2016,@erfannoury black are hidden states / memories. Blue are inputs that can be concatenated or gated with the hidden states.
758606735967973400,Thu Jul 28 10:15:14 +0000 2016,.@redpony talks about sequence models, tricks of the trade for using LSTMs at #LxMLS 2016 #machinelearning #dlearn https://t.co/cO7Kg3HMj9
758368124148211700,Wed Jul 27 18:27:05 +0000 2016,Blog post on what representations, e.g. POS tags, embeddings actually encode by @haldaume3 https://t.co/XY2iVmTglw #deeplearning
758354064262455300,Wed Jul 27 17:31:13 +0000 2016,.@OriolVinyalsML goes beyond seq2seq, talks about Alex Grave's Adaptive Computation Time at #LxMLS #DeepLearning https://t.co/BP6vCElC3c
758346922428162000,Wed Jul 27 17:02:50 +0000 2016,. @OriolVinyalsML highlights @IAugenstein's tweet as an example of the popularity of seq2seq models #DeepLearning https://t.co/MvZphBKNPA
758331344019607600,Wed Jul 27 16:00:56 +0000 2016,More highlights of #icml2016 with a focus on RL and #deeplearning https://t.co/EWdUQm8sdX #MachineLearning
758248792042930200,Wed Jul 27 10:32:54 +0000 2016,.@DeepMindAI's Wang Ling talks about character-based word representations at #LxMLS 2016 #machinelearning #dlearn https://t.co/j1Qy4OUMKD
758223661425975300,Wed Jul 27 08:53:02 +0000 2016,@deepnarainsingh Not yet unfortunately. It's still on my reading stack.
758221161398403100,Wed Jul 27 08:43:06 +0000 2016,Deep Fashion, a large-scale clothes dataset with comprehensive annotations at #cvpr2016 https://t.co/Jub7W87VEs #dlearn #MachineLearning
757921891667546100,Tue Jul 26 12:53:55 +0000 2016,@zkajdan Yes. You can find them here: https://t.co/17ID0t50LA
757871190627250200,Tue Jul 26 09:32:27 +0000 2016,Slav Petrov talks about generative and discriminative parsing models at #LxMLS 2016 #machinelearning #NLProc https://t.co/usudjmXz8r
756076021263179800,Thu Jul 21 10:39:05 +0000 2016,Does crime-predicting software bias judges? https://t.co/9snSwr893G #MachineLearning
755804731646193700,Wed Jul 20 16:41:04 +0000 2016,Top #icml2016 topics: ResNets, Memory Networks, and Non-Convex Optimization https://t.co/gZ1J0VBspx #MachineLearning #deeplearning
755777938654658600,Wed Jul 20 14:54:36 +0000 2016,@DataSci_Ireland @NLPDublin That would be awesome! Thanks a lot, guys!
755774763105157100,Wed Jul 20 14:41:59 +0000 2016,.@DeepMindAI uses #DeepLearning to reduce the amount of energy used for cooling Google's servers by 40% https://t.co/hhSMU5XGyp
755734791694188500,Wed Jul 20 12:03:09 +0000 2016,We've announced a new NLP meetup group @NLPDublin. 1st event is on August 3. Looking forward to seeing you all! https://t.co/cZslOYWH2g
755734241586061300,Wed Jul 20 12:00:58 +0000 2016,RT @NLPDublin: Join us at our first NLP meetup featuring great speakers and interesting presentations! RSVP's are now open! https://t.co/fh‚Ä¶
755486622888538100,Tue Jul 19 19:37:01 +0000 2016,Is language arbitrary? An automatic evaluation of phonesthemes, i.e. phonemes that are said to carry meaning https://t.co/xP2MsheW1I #nlproc
755472295863255000,Tue Jul 19 18:40:05 +0000 2016,10 highlight papers from #icml2016 and #cvpr2016 https://t.co/eeqq59dHQf #deeplearning #MachineLearning #computervision
755441583663611900,Tue Jul 19 16:38:03 +0000 2016,Blog post on constructing a similarity-based, global tree of languages using only 100 words https://t.co/QtiMXFBuze #NLProc #linguistics
755431020753805300,Tue Jul 19 15:56:05 +0000 2016,.@chrmanning's #SIGIR2016 slides towards a closer collaboration between #NLProc and #IR. https://t.co/E0dXrZodyE
754085665424412700,Fri Jul 15 22:50:07 +0000 2016,Get some inspiration for your next DL paper with the 2016 #DeepLearning for #NLProc Stanford course reports https://t.co/WR1AyddeSM
753693947143356400,Thu Jul 14 20:53:34 +0000 2016,RT @_aylien: Leveraging Deep Learning for Multilingual #SentimentAnalysis - #DeepLearning #NLProc https://t.co/vXfJemEuTw https://t.co/5XhL‚Ä¶
753638978499207200,Thu Jul 14 17:15:09 +0000 2016,The Winograd Schema Challenge - a better Turing test? https://t.co/6HUYrPPqpp #MachineLearning #NLProc
753289900363874300,Wed Jul 13 18:08:02 +0000 2016,Very cool, extensive blog post about how dropout captures uncertainty in #deeplearning by @yaringal https://t.co/nVbtOQkGw1
753253946668429300,Wed Jul 13 15:45:10 +0000 2016,Beautiful recollections of his time with Richard Feynman by @stephen_wolfram https://t.co/vtZSoJaJ8R
753202084942536700,Wed Jul 13 12:19:05 +0000 2016,Nice overview of #deeplearning courses provided by Stanford, Oxford, Montreal, &amp; Google https://t.co/sal6bHRoXn
753149919796617200,Wed Jul 13 08:51:48 +0000 2016,Some paper highlights from @naacl2016 by @haldaume3 https://t.co/rO5dBim6jU #deeplearning
752512774945865700,Mon Jul 11 14:40:01 +0000 2016,@egrefen https://t.co/uxKxWz6lAA
752152924252352500,Sun Jul 10 14:50:06 +0000 2016,Low-level CNN filters form opposite-phase pairs; concatenated ReLUs capture both phases in one filter https://t.co/IP1cHAnVol #deeplearning
752116922804674600,Sun Jul 10 12:27:02 +0000 2016,Architecture of multi-task NNs is task-dependent; cross-stitch NNs learn optimal combination of params https://t.co/4FjqqfqbN0 #deeplearning
751792303077924900,Sat Jul 09 14:57:07 +0000 2016,Paper on the impact of 2018 EU regulations on algorithmic decision-making &amp; right to explanation https://t.co/EPJu1FWtzV #MachineLearning
751780958513172500,Sat Jul 09 14:12:02 +0000 2016,Intro, paper, &amp; code about inference with generative adversarial networks https://t.co/XpeJFeGl53 #deeplearning
751750519073284100,Sat Jul 09 12:11:05 +0000 2016,Clear post on what we need to do: democratizing AI so that it can be used by everyone by @fchollet https://t.co/pTcmuQW2nO #deeplearning
751714356463894500,Sat Jul 09 09:47:23 +0000 2016,Good post on reducing the hype around AI and being more inquisitive towards results by @Smerity https://t.co/qh9cBIQZfz #MachineLearning
751558493815992300,Fri Jul 08 23:28:02 +0000 2016,Cool podcast on the human side of AI by @a16z with @drfeifei https://t.co/7TU5XJkjS2 #deeplearning #MachineLearning
751548031766593500,Fri Jul 08 22:46:28 +0000 2016,New ggplot version introduces theming, richer expressions, better geoms, &amp; more. Awesome! https://t.co/7TU5XJ2J0u #dataviz #MachineLearning
751444310239879200,Fri Jul 08 15:54:19 +0000 2016,Learning disentangled representations: slides (w/ animations) &amp; paper by @DeepMindAI https://t.co/dGq1MLmykk https://t.co/WMCUhVdcxc #dlearn
751411735978991600,Fri Jul 08 13:44:53 +0000 2016,Bored of simplistic RL environments? @MSFTResearch's Project Malm√∂ now lets you use @Minecraft for #MachineLearning https://t.co/hVgC3KKpp9
751072069547352000,Thu Jul 07 15:15:10 +0000 2016,Design Patterns for #MachineLearning: The Hulk, the Terminator, the Sentinel... Slides from #icml2016 via @netflix https://t.co/oebbAQWjOn
750804782479134700,Wed Jul 06 21:33:04 +0000 2016,Char-RNN generates witty, incoherent snippets; this post looks at methods to add structure to novel-lengths texts https://t.co/03aBrvATRS
750783562270511100,Wed Jul 06 20:08:44 +0000 2016,Great, detailed post and code on how to build a retrieval-based chatbot in @TensorFlo by @dennybritz https://t.co/4sDDOpBo5Q #deeplearning
750015828217651200,Mon Jul 04 17:18:02 +0000 2016,Awesome trailer of #SIGGRAPH2016 technical papers. Can we have this for every conference? https://t.co/2FLFc6xh1n #MachineLearning
749917009756950500,Mon Jul 04 10:45:22 +0000 2016,Very cool LSTM visualization library and demo by @harvardnlp https://t.co/ump3pBbPuB #deeplearning #dataviz
748974470467956700,Fri Jul 01 20:20:03 +0000 2016,Forget #chatbots: This Chinese QA app has rocketed to $100M market cap in 24 days by letting users ask celebrities https://t.co/lGee01naKS
748957102740123600,Fri Jul 01 19:11:02 +0000 2016,Nice, clear blog post on how @stanfordnlp's SPINN combines recursion and recurrence by @j_gauthier https://t.co/XmTXPPGrab #NLProc
748924657420513300,Fri Jul 01 17:02:07 +0000 2016,Great comic on #Brexit and its impact on academia by @PHDcomics https://t.co/Ai6dKj1voX
748907530898640900,Fri Jul 01 15:54:04 +0000 2016,Insightful analysis of tactics and tricks-of-the-trade to win @kaggle competitions https://t.co/EUN0lPmZy3 #datascience #MachineLearning
748862557616803800,Fri Jul 01 12:55:21 +0000 2016,We've announced our first @NLPDublin meetup! Sign up to the meetup group here: https://t.co/jrpgBONnQX #NLProc
748860719861370900,Fri Jul 01 12:48:03 +0000 2016,Interesting analysis of #NLProc &amp; #MachineLearning publishing patterns: CMU and @MSFTResearch publish the most. https://t.co/Iy3USRq3Rn
748552778939797500,Thu Jun 30 16:24:24 +0000 2016,Wide or #DeepLearning? https://t.co/Q7FcG1eXvC https://t.co/WLHV0Hv2Yj
748264803622260700,Wed Jun 29 21:20:05 +0000 2016,A sober look at natural language user interfaces by @honnibal: They are just user interfaces. https://t.co/Gh6KWppaHT #chatbots #NLProc
748217995873312800,Wed Jun 29 18:14:06 +0000 2016,Nice article about why orthogonal initialization prevents vanishing/exploding gradients in RNNs by @Smerity https://t.co/OrfBTiZirZ #dlearn
748187293765099500,Wed Jun 29 16:12:06 +0000 2016,Interesting article on how @google manages to store all its code, ~1B files, in one gigantic repo https://t.co/SL7GWrv6xK
748138462058995700,Wed Jun 29 12:58:03 +0000 2016,Article on FB's memory networks in the context of #chatbots, their key-value extension by @Slate https://t.co/UijTn7Szml #deeplearning
747824406655705100,Tue Jun 28 16:10:07 +0000 2016,Nice and clear primer on Conditional Random Fields by @echen https://t.co/cujeH1Cuyp #NLProc
747791944391090200,Tue Jun 28 14:01:07 +0000 2016,Very cool &amp; comprehensive website on exam #QA with papers, data, tools, &amp; state-of-the-art results by @uclmr https://t.co/QNEttbgtHp #dlearn
747759981328379900,Tue Jun 28 11:54:06 +0000 2016,Some ideas on data gathering and evaluation towards acceleration progress in dialogue by @PaulMineiro https://t.co/hybQ0Hejz7 #deeplearning
747708827454283800,Tue Jun 28 08:30:50 +0000 2016,Very cool 3D semantic, hierarchical parsing of buildings by @StanfordCVGL https://t.co/cgnndOslA7 #computervision
747512404947263500,Mon Jun 27 19:30:20 +0000 2016,Article &amp; paper about efficient hand-tracking to allow hand usage in #VR https://t.co/z7tTyyXFnY https://t.co/hi9twpyEOH #MachineLearning
747493308700692500,Mon Jun 27 18:14:27 +0000 2016,RT @IgorBrigadir: FYI #MLDublin: New Natural Language Processing meetup in Dublin https://t.co/kwo8RZyzWZ poke @seb_ruder for more info
747456603134263300,Mon Jun 27 15:48:35 +0000 2016,This looks amazing! Project Bloks, a tangible programming platform to facilitate #CS education by @google https://t.co/cERo1SC9fp #CSEdu
746685643611267100,Sat Jun 25 12:45:04 +0000 2016,Nice blog post about language bias, not taking all word embeddings at face value @haldaume3 https://t.co/RzV2wdM5sY #NLProc #deeplearning
746652784171302900,Sat Jun 25 10:34:30 +0000 2016,Our algos implicitly encode biases in data; this paper reveals the biases of word embeddings  https://t.co/vya2F3WvEU #NLProc #deeplearning
746378122791125000,Fri Jun 24 16:23:06 +0000 2016,RT @johnbreslin: Research Fellowships at @_AYLIEN - @scienceirel Industry Fellowship Programme - https://t.co/NmJrEPNDCA #ai #nlp #sa https‚Ä¶
746154134202527700,Fri Jun 24 01:33:03 +0000 2016,Great interview with @jasonbaldridge on the state of #NLProc, the promise of #deeplearning &amp; need for linguistics https://t.co/vDAFqlGQIs
746134269009899500,Fri Jun 24 00:14:06 +0000 2016,Great tutorial on Variational Autoencoders with intuitions, explanation of maths &amp; empirical behaviour https://t.co/ycwe7zi6Kn #deeplearning
746111010969387000,Thu Jun 23 22:41:41 +0000 2016,@zygmuntzajac Sure. üòÄ
746102621476368400,Thu Jun 23 22:08:21 +0000 2016,@zygmuntzajac Yes! That would be awesome! :D Should I send you a separate email or is the tweet enough?
746098489646551000,Thu Jun 23 21:51:56 +0000 2016,Great article on how @google is remaking itself as a #machinelearning company by @backchnnl https://t.co/o8ZJMHZKmL
745955313115869200,Thu Jun 23 12:23:00 +0000 2016,Interested in conducting cutting-edge #MachineLearning / #NLProc research in industry? We're looking for Postdocs! https://t.co/wsbt6v4oL6
745587275036164100,Wed Jun 22 12:00:33 +0000 2016,Tutorial on using transfer learning for computer vision with #keras by @fchollet https://t.co/T2LWDbDR0j #MachineLearning #deeplearning
745573874927034400,Wed Jun 22 11:07:18 +0000 2016,@joyjeni Hey Jenisha, glad you found it helpful. :)
745564760792432600,Wed Jun 22 10:31:05 +0000 2016,Cool QA dataset with 100k question-answer pairs, including leaderboard by @stanfordnlp https://t.co/BE5pcfK9GE #MachineLearning #nlrpoc
745532842013167600,Wed Jun 22 08:24:15 +0000 2016,Cool discussion of 5 technical #AI safety problems in a post &amp; paper by @ch402 https://t.co/MTrdJojMRJ https://t.co/cK4HaPjApG #deeplearning
745172502708527100,Tue Jun 21 08:32:23 +0000 2016,@westwoodpro As long as you use a NN for ur use case or an algo which has a loss function, you can optimize it with SGD. Hope that helps. :)
743497604835721200,Thu Jun 16 17:36:57 +0000 2016,@venden19 yep. üëç
743478942984704000,Thu Jun 16 16:22:47 +0000 2016,@venden19 The loss function is what you apply to the output of your NN e.g. the cross-entropy or MSE of some probability. Is that clearer?
743438373998702600,Thu Jun 16 13:41:35 +0000 2016,@SelfReflective Thank you! Appreciate it. :)
743050346793144300,Wed Jun 15 11:59:42 +0000 2016,Cool learning to learn paper by @DeepMindAI on learning the optimization algorithm https://t.co/ixA0rOLrux #deeplearning
742795503700611100,Tue Jun 14 19:07:03 +0000 2016,New @OpenAI paper allows Generative Adversarial Networks to disentangle representations by adding MI https://t.co/2FBHP2qM7b #deeplearning
742769595661094900,Tue Jun 14 17:24:06 +0000 2016,@Smerity thanks for the link! Haha, yeah, i realized. Still good to know some of the alternatives as well IMO üòâ
742747190850768900,Tue Jun 14 15:55:04 +0000 2016,New Natural Language Processing meetup in Dublin. Join it here: https://t.co/OYKVRMP2L7 #NLProc #MachineLearning
742726381440782300,Tue Jun 14 14:32:23 +0000 2016,First paper by @OpenAI focuses on improving Generative Adversarial Networks with various techniques https://t.co/aclq0hiSzy #deeplearning
742627714293583900,Tue Jun 14 08:00:19 +0000 2016,On word embeddings - Part 2: Approximating the Softmax; new blog post on mitigating the bottleneck https://t.co/I7lkb5WtlD #deeplearning
742419744637259800,Mon Jun 13 18:13:55 +0000 2016,New blog post giving an overview of softmax approximations for learning better word embeddings https://t.co/I7lkb5ESu5 #deeplearning #NLProc
742249311279456300,Mon Jun 13 06:56:40 +0000 2016,@faizanj thanks!
741977289320239100,Sun Jun 12 12:55:45 +0000 2016,RT @datiobd: #NLP overview: Word embedding models https://t.co/MQZo5MZ7ig #MachineLearning #DeepLearning via @seb_ruder https://t.co/4smAwb‚Ä¶
741765236609429500,Sat Jun 11 22:53:08 +0000 2016,Systematic evaluation of various CNN hyperparameters &amp; architectures on Imagenet, with practical tips https://t.co/zZrAACTltA #deeplearning
741665726214361100,Sat Jun 11 16:17:43 +0000 2016,Extremely cool primer on #AI, #DeepLearning, and #MachineLearning by @a16z's @withfries2 https://t.co/m0BlfbrgLF
741664712560119800,Sat Jun 11 16:13:41 +0000 2016,An interview about the current state of machine listening, processing music with #machinelearning https://t.co/9UnRvh8jul #deeplearning
741292488099532800,Fri Jun 10 15:34:36 +0000 2016,@mattellsworth @andreasklinger @rrhoover aylien does this too. Let me know about your experience if you end up trying it.
740886552981712900,Thu Jun 09 12:41:33 +0000 2016,Always excited about new NLL advances! This #ACL2016 paper from @stanfordnlp takes cues from Wittgenstein &amp; Winograd https://t.co/3rkglQJBDL
740650443030863900,Wed Jun 08 21:03:20 +0000 2016,Requests for Research by @OpenAI - A living #DeepLearning problems collection. Excited how this will shape research! https://t.co/aaJyoG0TRj
740556455586242600,Wed Jun 08 14:49:52 +0000 2016,@thammegowda yep, me too! Cool to read some more details!
740284890470449200,Tue Jun 07 20:50:46 +0000 2016,@eisokant Thanks! Will forward it! Haven't read it myself yet! :)
740284231478349800,Tue Jun 07 20:48:09 +0000 2016,@eisokant In the paper, they sample positive examples based on a relevance measure, use in-class or out-of-class negative samples.
740283848152469500,Tue Jun 07 20:46:37 +0000 2016,@eisokant In the presentation, an image labeled as "similar" was used together with a random negative sample.
740280440314073100,Tue Jun 07 20:33:05 +0000 2016,@eisokant same loss is also used by Collobert et al. (2011) to learn word embeddings https://t.co/Vge5STSlRB
740280180581752800,Tue Jun 07 20:32:03 +0000 2016,@eisokant much simpler. Input: IMG, similar and dissimilar IMG. Objective: min distance IMG-similar IMG, max distance IMG-different IMG.
740277684115284000,Tue Jun 07 20:22:08 +0000 2016,@eisokant thanks for the love! ‚ò∫Ô∏è glad you found it helpful!
740266352645050400,Tue Jun 07 19:37:06 +0000 2016,Great paper on using virtual adversarial training as regularization for semi-supervised text classification https://t.co/akyCt4sPuY #NLProc
740253768701206500,Tue Jun 07 18:47:06 +0000 2016,A history of (convolutional) neural network architectures: From LeNet to ResNet by @culurciello https://t.co/FAtMBY5Im0 #deeplearning
740132549079371800,Tue Jun 07 10:45:25 +0000 2016,Wondering how Google does Smart Reply? They've now released details in a #kdd2016 paper https://t.co/7essub5gkO #machinelearning #NLProc
740093695731630100,Tue Jun 07 08:11:01 +0000 2016,Google's Magenta on GitHub. Will be interesting to see what artists &amp; musicians come up with! https://t.co/fQjpWjgxa6 #deeplearning
739940654864838700,Mon Jun 06 22:02:54 +0000 2016,@deepnarainsingh Glad it helped! Thanks for sharing your results! :)
738456435059630100,Thu Jun 02 19:45:08 +0000 2016,Deep RL: Pong from Pixels - Great intro to Reinforcement Learning with 130 lines Python example by @karpathy https://t.co/keHfRni6zQ
738427865230942200,Thu Jun 02 17:51:37 +0000 2016,Talk about TiefVision, an end-to-end image similarity search engine by Paul Carr√© at Dublin #DeepLearning meetup https://t.co/jT963BzUbs
738418445813841900,Thu Jun 02 17:14:11 +0000 2016,Facebook bets on #DeepLearning for #NLProc, introduces DeepText, a text analysis framework based on Collobert work https://t.co/MvzW6ce8iY
738360420017262600,Thu Jun 02 13:23:36 +0000 2016,New paper on modeling open-domain dialogue gives taste of Kurzweil's work @google, use @reddit dataset https://t.co/accanSf7wV #deeplearning
738292668229423100,Thu Jun 02 08:54:23 +0000 2016,Good summary of some of the highlights of the Boston RE‚Ä¢WORK #DeepLearning conference by @indicoData https://t.co/Si1Wl0VX6P
738077408729784300,Wed Jun 01 18:39:01 +0000 2016,Cool blog post on Facebook Sixth Sense, an extension that captures when friends are writing to you https://t.co/QLHR5p6HXP
738069071997075500,Wed Jun 01 18:05:54 +0000 2016,@dirk_hovy more gpus! üòÄ
738064900258402300,Wed Jun 01 17:49:19 +0000 2016,. @parsaghaffari gives a talk about the pitfalls of embedding models and a byte-based embedding model, byte2vec https://t.co/UE1sbMbgHK
738064326725140500,Wed Jun 01 17:47:02 +0000 2016,Dublin @databythebay event hosted by @ChiefScientist with talks about #nlproc and #DeepLearning https://t.co/FG423edrvR
738020217083822100,Wed Jun 01 14:51:46 +0000 2016,Awesome blog post on the highlights of @iclr2016 by @quantombone https://t.co/tBpC9LPAdD #deeplearning #MachineLearning
737666265528078300,Tue May 31 15:25:17 +0000 2016,Some cool data visualizations by the @UberEng #dataviz team https://t.co/iYGuKbgtu3 #datascience
737258611584438300,Mon May 30 12:25:25 +0000 2016,@m_deff Thanks, Michael! :)
736560423802044400,Sat May 28 14:11:04 +0000 2016,Extensive tutorial on building autoencoders in keras by @fchollet  https://t.co/ov4Q8AMkfh #DeepLearning
736504772803579900,Sat May 28 10:29:56 +0000 2016,Ray Kurzweil is working on #chatbots at Google that given a sample of your writing, can adopt your style https://t.co/5FqOSeK3A0 #NLProc
736231707213500400,Fri May 27 16:24:52 +0000 2016,.@iclr2016 keynote on NN architectures that are informed by linguistic structure by @redpony https://t.co/uUCRhYHATt #NLProc #DeepLearning
735938333205893100,Thu May 26 20:59:06 +0000 2016,Research on end-to-end conversational agents becomes more goal-oriented, focusing on real world use https://t.co/OQJkADX0kR #DeepLearning
735921274833276900,Thu May 26 19:51:19 +0000 2016,Interesting article providing an overview of advanced #NLProc tools &amp; APIs for #chatbot makers https://t.co/srje68j1xc
735195524869951500,Tue May 24 19:47:27 +0000 2016,@sl8rv Inspiring @Xconomy profile about you. You're not learning German by any chance? ;) We have good poetry. Anyway would love to connect.
735190219842945000,Tue May 24 19:26:22 +0000 2016,@deepnarainsingh default word2vec should be your 1st choice. depending on ur data, expanding corpus is easier than looking for alternatives.
734783563493564400,Mon May 23 16:30:27 +0000 2016,Traditional #nlproc have their merits: SMT outperforms #DeepLearning-based approach on grammatical error correction https://t.co/Oum87qte9h
734687761731596300,Mon May 23 10:09:46 +0000 2016,Residual NNs may not actually be very deep, but ensembles of relatively shallow NNs https://t.co/MXeTGSwBdV #deeplearning #machinelearning
733621304683823100,Fri May 20 11:32:03 +0000 2016,New @DeepMindAI paper showing that memory-augmented NNs e.g. NTMs can perform one-shot learning https://t.co/dirWyRARTg #deeplearning
733281325000450000,Thu May 19 13:01:06 +0000 2016,Large-scale graph-based semi-supervised learning by @google as used in #Allo https://t.co/ecdzPVoWuy #machinelearning
733262673903792100,Thu May 19 11:46:59 +0000 2016,@snneeggii Well, but are we now not going to be able to use, say, a seq2seq NMT model just because the underlying tech is patented?
733219430650523600,Thu May 19 08:55:09 +0000 2016,Details about the seq2seq-powered Smart Reply that is front and center in @google's #Allo https://t.co/bUMnsmW1Kc #machinelearning
732625817642098700,Tue May 17 17:36:21 +0000 2016,RT @_aylien: We are delighted to pick up the @LTInnovate Award today, distinguishing the "Best in Language Technology" üòÉüèÜ #LTI16 https://t.‚Ä¶
732624291246669800,Tue May 17 17:30:17 +0000 2016,Looks like Google is starting to patent its ACL papers üòÆ https://t.co/wCbPxwCvZQ https://t.co/LrzAshyaoM #nlproc #machinelearning
732615456905781200,Tue May 17 16:55:10 +0000 2016,Notes on @iclr2016 by @indicoData with a focus on adversarial learning and optimization https://t.co/2w0W9pBuN2
732276448711589900,Mon May 16 18:28:05 +0000 2016,@Lefiish Thanks for the mention, Thibault. It's not really complete, though. üòâ  There's so much more out there.
732276091612725200,Mon May 16 18:26:39 +0000 2016,RT @Lefiish: Complete review of word embeddings aka transforming words to vectors by @seb_ruder https://t.co/sU2Z9GKabj
731788017250209800,Sun May 15 10:07:13 +0000 2016,Want to stay on top of #nlproc conference deadlines? Here's the new unofficially official conference calendar https://t.co/T6mqeXoXhP
731786477542252500,Sun May 15 10:01:06 +0000 2016,Hal Daum√© on why current practices in e.g. training NNs depend on the optimizer *not* working https://t.co/NvNuWY4dtm #DeepLearning
731617145281519600,Sat May 14 22:48:14 +0000 2016,RT @jonhurlock: A pretty nice blog post on gradient descent algorithms by @seb_ruder over at https://t.co/wqdDAmlu6u
731617128017711100,Sat May 14 22:48:10 +0000 2016,@jonhurlock thanks for the mention, Jon! Glad you like it!
731227892705792000,Fri May 13 21:01:29 +0000 2016,The days are long but the decades are short - clear lessons that go to the heart, by Sam Altman @sama https://t.co/bQizwopiDz
731077912317841400,Fri May 13 11:05:31 +0000 2016,Cool idea: Adversarial training is not only useful for image generation but also for domain adaptation https://t.co/Gr33wbQspc #deeplearning
730857642172788700,Thu May 12 20:30:15 +0000 2016,Google releases @TensorFlo-based SyntaxNet and, ahem, Parsey McParseFace, the best parser in the world https://t.co/MAzrWGxN3A #DeepLearning
730846613208453100,Thu May 12 19:46:25 +0000 2016,RT @AlfredoCanziani: Awesome blog review about optimisations techniques by @seb_ruder https://t.co/gycLZ7Zmc7
730837814091382800,Thu May 12 19:11:27 +0000 2016,RT @_aylien: We added Aspect-Based Sentiment Analysis to our Google Sheets Add-on. You're gonna love it! https://t.co/WR2YViPxzC https://t.‚Ä¶
730804917472989200,Thu May 12 17:00:44 +0000 2016,@stephenjaoates üòÇ
730784025325998100,Thu May 12 15:37:43 +0000 2016,More papers using Bernoulli distributed variables should include coin flips in their images https://t.co/05N64pJ2KG https://t.co/pO3ZzCe8QG
730771437141790700,Thu May 12 14:47:42 +0000 2016,@lawrennd @haldaume3 Thanks for the note, Hal! Both make sense.
730759587679326200,Thu May 12 14:00:37 +0000 2016,@lawrennd Thank you! I love your blog and really enjoyed your recent discussion on @TlkngMchns as well.
730758792565129200,Thu May 12 13:57:27 +0000 2016,RT @lawrennd: Very nice introduction to word embeddings by @seb_ruder https://t.co/MVyZgzxNm2
730503149287518200,Wed May 11 21:01:37 +0000 2016,#dataviz of the day: mapping of German states in @kaggle comp close to actual map positions https://t.co/yDXlLVqiQ6
730441255885414400,Wed May 11 16:55:41 +0000 2016,The Good, the Bad, and the Ugly of @TensorFlo (+ hacks to fix the latter) via @indicoData https://t.co/40HCgpGmGl #DeepLearning #datascience
730374226658635800,Wed May 11 12:29:19 +0000 2016,Thoughtful, illuminating discussion of Nick Bostrom's Superintelligence by @lawrennd https://t.co/kWGzqKSfu1 #machinelearning #datascience
730348224695914500,Wed May 11 10:46:00 +0000 2016,@PMinervini Thanks for the hint!
729685627638263800,Mon May 09 14:53:05 +0000 2016,John Oliver on the oversimplification of science in media; why not to take every study at face value @iamjohnoliver https://t.co/AjZm12SyvB
729649398230175700,Mon May 09 12:29:07 +0000 2016,Cool paper on understanding &amp; predicting visual humor; approach uses w2v-style obj representations ¬†https://t.co/cY4JH5P3S9 #DeepLearning
728942348034510800,Sat May 07 13:39:33 +0000 2016,.@WIRED article about how @facebook automates part of its #machinelearning research, the search for new models https://t.co/FEo9nDa07E
728326350013136900,Thu May 05 20:51:48 +0000 2016,Interesting slides on how @xdotai uses @MongoDB for #machinelearning https://t.co/WohUp3s4Mv
728296822775681000,Thu May 05 18:54:28 +0000 2016,RT @_aylien: We've just released News API SDKs for 7 languages üòÄ  NodeJS, Ruby, PHP, Java, Python,Go &amp; C# https://t.co/PwcyDlltuk https://t‚Ä¶
728133379825111000,Thu May 05 08:05:00 +0000 2016,Exponential linear units (ELUs) work much better than ReLUs with random weights for image textures via @alexjc https://t.co/TNr290aMKv
727975000049918000,Wed May 04 21:35:39 +0000 2016,Quantization will allow us to compress @TensorFlo models up to a 4th of their size via @petewarden https://t.co/LShuYPz1Sv #DeepLearning
727961529463799800,Wed May 04 20:42:08 +0000 2016,@NathanBenaich @bookingcom https://t.co/3GOpneDEwf has a chatbot?
727898903438331900,Wed May 04 16:33:16 +0000 2016,RT @_aylien: Dive deep into customer reviews and feedback with Aspect-based #SentimentAnalysis https://t.co/vx92RmLu6a #CX https://t.co/et1‚Ä¶
727528545010999300,Tue May 03 16:01:36 +0000 2016,Greg Brockman's authentic account of his path from college, to @stripe, to @open_ai. Thanks for sharing! https://t.co/agSfrFYL7Z @gdb
726396685858029600,Sat Apr 30 13:04:00 +0000 2016,RT @alexip: Interesting #NLP project by @_aylien: analysis of News Headline from 2 journalists Here's the #Jupyter notebook https://t.co/wD‚Ä¶
725723494516965400,Thu Apr 28 16:28:59 +0000 2016,Check out our new aspect-based sentiment analysis endpoint using #DeepLearning with semi-supervision https://t.co/lyiTPtiVdn @_aylien
725722699029438500,Thu Apr 28 16:25:49 +0000 2016,@snneeggii üòä
725382104050192400,Wed Apr 27 17:52:25 +0000 2016,.@open_ai releases their first toolkit, an environment for reinforcement learning https://t.co/9I4uTSnzNN #DeepLearning
725241019600650200,Wed Apr 27 08:31:48 +0000 2016,Very cool sketch simplification results with convolutional neural networks https://t.co/ILQy5OjaHH #DeepLearning
725049705504342000,Tue Apr 26 19:51:35 +0000 2016,"Aim for the heart, not the head with your first question" - @calfussman in one of @tferriss' best interviews https://t.co/8BBX76Seue
724705000287547400,Mon Apr 25 21:01:51 +0000 2016,Stuck in a local minima? Check out the #MachineLearning A-Cappella Overfitting Thriller by @udacity https://t.co/pLbIVm4IVG
724668918380531700,Mon Apr 25 18:38:28 +0000 2016,RT @fchollet: I added a tutorial on using Keras as part of a TensorFlow workflow: https://t.co/reoniEndJl https://t.co/NbBAec4VEZ
724665738963898400,Mon Apr 25 18:25:50 +0000 2016,@fchollet that was pretty fast after the results of the survey! Cheers!
724654567158341600,Mon Apr 25 17:41:26 +0000 2016,Talk about using reinforcement learning for travel applications at #MLDublin @boxever #DeepLearning https://t.co/3VfULG9jEd
724612270114897900,Mon Apr 25 14:53:22 +0000 2016,How to write good papers (or Why I gave your paper a Strong Reject) by @mdwelsh https://t.co/GniLiaTWrH #machinelearning
724576006300962800,Mon Apr 25 12:29:16 +0000 2016,Cool text-to-scene generation using graphs capturing physical contact and visual attention https://t.co/C3I5cqpL0H #deeplearning
724509532416782300,Mon Apr 25 08:05:07 +0000 2016,Still generating fake speeches with your LSTM? Some guy is using them to put a stop to patent trolls https://t.co/eqX7bGzlFa #deeplearning
724488355136847900,Mon Apr 25 06:40:58 +0000 2016,RT @albarjip: Very thorough and interesting post by @seb_ruder on SGD optimization methods applied to Deep Learning https://t.co/DeNnwhyVwh
724342454426701800,Sun Apr 24 21:01:13 +0000 2016,.@AndrewYNg's PhD thesis on how to use #deeplearning to fly a helicopter -- back in 2003! https://t.co/Q2kLiNACkb #machinelearning
724249844777377800,Sun Apr 24 14:53:13 +0000 2016,Joan Bruna's Topics Course on #DeepLearning at Berkeley with soo much content, links to relevant papers, etc. https://t.co/n5cuI1Excs
724217119144071200,Sun Apr 24 12:43:11 +0000 2016,@andinfinity_GER Hey Christian, good to see more Germans getting into and being active in ML. :) What ML grad programs are you looking at?
724213560360984600,Sun Apr 24 12:29:02 +0000 2016,.@MSFTResearchCam director Chris Bishop with a reality check on #ArtificialIntelligence https://t.co/KWDXuxnoEO
723825804648570900,Sat Apr 23 10:48:14 +0000 2016,Stylizing pixel art with #deeplearning. Will we soon reach CSI-levels of enhancing? https://t.co/BIEvIkycdp https://t.co/1euDOl8WZ3
723538845560651800,Fri Apr 22 15:47:58 +0000 2016,Think #machinelearning research is not yet awesome enough? You can now do reinforcement learning playing Doom! https://t.co/vO7lqfigPh
723537725140414500,Fri Apr 22 15:43:31 +0000 2016,RT @dennybritz: VizDoom -  Doom-based AI research platform for reinforcement learning from raw visual information https://t.co/tQWpk6F1VL
723255878581911600,Thu Apr 21 21:03:33 +0000 2016,Great new paper by Jason Weston that takes the first step towards more human, dialog-based language learning https://t.co/vMS9yVE5Q8 #nlproc
723162632904167400,Thu Apr 21 14:53:02 +0000 2016,A fascinating, well-written overview of the history of Game #AI until #AlphaGo in Three Parts by Andrey Kurenkov https://t.co/NqQpGEHJGc
723126416909459500,Thu Apr 21 12:29:07 +0000 2016,Great Introduction to @TensorFlo book with lots of details, code examples, etc. by @JordiTorresBCN https://t.co/wke3fEj3vH #DeepLearning
722915752475500500,Wed Apr 20 22:32:01 +0000 2016,@iamaidang great article, Aidan! What's next on your to-write list?
722892890540630000,Wed Apr 20 21:01:10 +0000 2016,Clear numeric example of the computation of a step in the life of an LSTM by Aidan Gomez https://t.co/EgRff0duRH #deeplearning #nlproc
722871875852849200,Wed Apr 20 19:37:40 +0000 2016,Postscript about Bill Campbell at @NewYorker and @bhorowitz's personal account of his friend https://t.co/VxKdNj3Pxb https://t.co/jmd65abjjA
722846801049456600,Wed Apr 20 17:58:01 +0000 2016,RT @socialsemantics: Our researcher @seb_ruder presents paper by Levy, Goldberg, Dagan on word embedding at @insight_centre reading group h‚Ä¶
722843459288043500,Wed Apr 20 17:44:45 +0000 2016,RT @MIT_CSAIL: h/t @peterritchie https://t.co/2YNftzXXAw
722764012048498700,Wed Apr 20 12:29:03 +0000 2016,What differentiates a researcher from a tinkerer: Scholarship, testing, goals, and persistence by @togelius https://t.co/jyW7rJLOzJ
722707824107761700,Wed Apr 20 08:45:47 +0000 2016,@parklize @togelius My bad. https://t.co/FMZxZKlQie
722697619705827300,Wed Apr 20 08:05:14 +0000 2016,Do's and don'ts on how *not* to review a paper by @togelius
722672062041559000,Wed Apr 20 06:23:40 +0000 2016,Great piece by @WIRED on @magicleap, #virtualreality and #mixedreality. https://t.co/1GwvJDnZkm
722038426825003000,Mon Apr 18 12:25:50 +0000 2016,RT @_aylien: Catch our #NLProc smart guys @PeimanBarnaghi  &amp; @seb_ruder  at "Twitter for Research" in @nuigalway this Wednesday https://t.c‚Ä¶
721104586526822400,Fri Apr 15 22:35:05 +0000 2016,RT @hugo_larochelle: This year, for #ICLR2016, we've decided to grand 2 Best Paper Awards: https://t.co/FcPR5zhO91
721049400244596700,Fri Apr 15 18:55:48 +0000 2016,Going to use this for the next conference: Learning to Generate Posters from Scientific Papers https://t.co/iYxQh9Nvmc #machinelearning
720741081390936000,Thu Apr 14 22:30:39 +0000 2016,Blog post and paper about @MSFTResearch's foray in visual storytelling https://t.co/znEUPd9oK3 https://t.co/z4TUc2RCrb #DeepLearning
720693179452944400,Thu Apr 14 19:20:18 +0000 2016,@DheepanRamanan Thanks for the mention, Dheepan! Glad you like it! :)
720345712022523900,Wed Apr 13 20:19:35 +0000 2016,@ericweiskott "Opinions expressed exclusively through puns." That should be the preferred way for most people. üòÅ
720227286226755600,Wed Apr 13 12:29:00 +0000 2016,.@WitNL talks about the trade-off between #machinelearning and rule-based systems in their #chatbots blogpost https://t.co/huShySKzak
720160890759331800,Wed Apr 13 08:05:10 +0000 2016,#naacl2016's best paper beautifully blends #deeplearning and #digitalhumanities üëè https://t.co/zgRQctqewF
720020716180082700,Tue Apr 12 22:48:10 +0000 2016,Beautiful interactive visualization of neural networks in @TensorFlo by @dsmilkov &amp; @shancarter https://t.co/XUanG6sKKI #deeplearning
719854008668659700,Tue Apr 12 11:45:44 +0000 2016,Log reg w/ IR &amp; w2v features gets 3rd place at @allenai_org comp; interesting: 3 classes for 4 Qs https://t.co/XklFS5hGOu #datascience
719629784511356900,Mon Apr 11 20:54:45 +0000 2016,New blog post: On word embeddings, Part 1 - Models &amp; Language Modelling https://t.co/AfIYRehj2R #machinelearning #deeplearning #NLProc
719207212749623300,Sun Apr 10 16:55:36 +0000 2016,Theano's scan for looping inside computation graph, e.g. for RNNs comes to @TensorFlo https://t.co/Uy7RvjJV7g #deeplearning #machinelearning
718738385741996000,Sat Apr 09 09:52:39 +0000 2016,RT @TechCrunch: Aylien launches news analysis API powered by its deep learning tech https://t.co/eoH4jFcCJx https://t.co/qb44uoGY0M
718329537713610800,Fri Apr 08 06:48:02 +0000 2016,@NathanBenaich So far no, but since it's on Friday, I might be able to hop across. Can you give me more info re the event?
718206495717003300,Thu Apr 07 22:39:06 +0000 2016,@NathanBenaich Hey Nathan, thanks for following. Loved reading about your journey from academia to VC in your medium article.
718157286577975300,Thu Apr 07 19:23:34 +0000 2016,An introduction to @TensorFlo by Irene Li at the Dublin #DeepLearning meetup hosted by @Intercom https://t.co/irpmFnAriv
718102685875052500,Thu Apr 07 15:46:36 +0000 2016,.@TechCrunch is covering our new API focused on applying #textanalytics and #deeplearning to news https://t.co/rfxkW3J2SO @_aylien
717975480540987400,Thu Apr 07 07:21:08 +0000 2016,RT @dennybritz: Deep Learning for Chatbots, Part 1 ‚Äì Introduction (new post and new series)  https://t.co/zep7UOzy8m
717364151971143700,Tue Apr 05 14:51:56 +0000 2016,An evaluation of cross-lingual word embeddings https://t.co/hItL0OLPNy #DeepLearning
716607476133716000,Sun Apr 03 12:45:10 +0000 2016,Is Yoshua Bengio trolling or why is his email listed as find-me@the.web in his latest #deeplearning paper? https://t.co/A2tDI6TqfI
715248040022564900,Wed Mar 30 18:43:16 +0000 2016,@AlanZucconi thanks! I appreciate it!
715244568703320000,Wed Mar 30 18:29:28 +0000 2016,Resnet in Resnet? It's ResNets all the way down... https://t.co/5lvKTmgmru #machinelearning #DeepLearning
715158136588525600,Wed Mar 30 12:46:01 +0000 2016,Microsoft bets big on bots. Will future #chatbots be like Tay &amp; Clippy? https://t.co/PdPskEtxI1 #machinelearning
714867406318006300,Tue Mar 29 17:30:45 +0000 2016,RT @_aylien: We're delighted to announce our News API is now live. Check it out and try it for FREE! https://t.co/UYCtlDkDMB https://t.co/w‚Ä¶
714791551159943200,Tue Mar 29 12:29:20 +0000 2016,A contender for shortest #machinelearning paper abstract: "Do Deep ConvNets Need to be Deep (or Convolutional)?" https://t.co/TPU2O7Z6DQ
714782060867686400,Tue Mar 29 11:51:37 +0000 2016,Favorite #machinelearning paper title this week: "Feeling the Bern: Adaptive Estimators for Bernoulli Probabilities" https://t.co/vzx701I3qa
714372272493080600,Mon Mar 28 08:43:16 +0000 2016,Brilliant profile of @google CEO Sundar Pichai https://t.co/vB5e26qSOl
714105766425337900,Sun Mar 27 15:04:16 +0000 2016,Interesting article highlighting the key moves that led to #AlphaGo's victories and demise in game 4 https://t.co/58Bm10CXP1
713653013118849000,Sat Mar 26 09:05:12 +0000 2016,Tons of interesting #deeplearning ideas in Stanford's @cs231n project reports https://t.co/3LKcNtabvR
713485985309438000,Fri Mar 25 22:01:29 +0000 2016,Nice collection of iPython notebooks to prepare you for algos &amp; data structures in next tech interviews https://t.co/2JVHnbLUsv #datascience
712594637215629300,Wed Mar 23 10:59:35 +0000 2016,RT @_aylien: 2 of our #NLP Research Engineers @seb_ruder &amp; @PeimanBarnaghi enjoying the iV&amp;L #DeepLearning Summer School in Malta https://t‚Ä¶
712552327694131200,Wed Mar 23 08:11:28 +0000 2016,.@MetaMindIO's @RichardSocher talks about QA for language &amp; vision at iV&amp;L #DeepLearning Summer School https://t.co/usTvMOMazB
712306785533763600,Tue Mar 22 15:55:46 +0000 2016,Coding a neural language model in @TorchML with @nalkalchbrenner at iV&amp;L #DeepLearning Summer School https://t.co/jgXrURiME9
711311575219572700,Sat Mar 19 22:01:09 +0000 2016,Brilliant interview with #deeplearning godfather Geoff Hinton about past and future of #machinelearning https://t.co/KIcHmVaN8x
711244880891277300,Sat Mar 19 17:36:08 +0000 2016,Awesome &amp; slightly creepy demo of face2face, real-time face capture and video reenactment (CVPR 2016) https://t.co/Yv8uhGcSGm #DeepLearning
710457956098244600,Thu Mar 17 13:29:11 +0000 2016,Upcoming: A legitimate excuse for #machinelearning researchers to craft elaborate Minecraft worlds https://t.co/gheN0kRRln
710391501759651800,Thu Mar 17 09:05:07 +0000 2016,Great article and musings on the fate of #Go via Lucas Banker https://t.co/lz4KCVfuj1 #AlphaGo #machinelearning
710338884463415300,Thu Mar 17 05:36:02 +0000 2016,Superb resource with many interviews with leading #machinelearning practicioners on the future of #AI.  https://t.co/GcJ02Xrq2T
710227944099160000,Wed Mar 16 22:15:11 +0000 2016,There are many ways of learning from one's mistakes; possibly the most brilliant: A wake for Irish startups at #SXSW https://t.co/hqpiKpB0PS
710224520456642600,Wed Mar 16 22:01:35 +0000 2016,Extensive blog post on diagnosing heart diseases with deep neural networks in 2nd @kaggle DSB https://t.co/K4pAcka4BF #machinelearning
710219999990947800,Wed Mar 16 21:43:37 +0000 2016,Following the announcement that @Minecraft will be used for #machinelearning research, here's the first arxiv paper https://t.co/I8wTImF9l2
710218305987022800,Wed Mar 16 21:36:54 +0000 2016,Cool video by @FiveThirtyEight on how to master @marchmadness, tips from UK's top data scientist https://t.co/8IwpQFCXvM #datascience
710169618850189300,Wed Mar 16 18:23:26 +0000 2016,Great article providing an outlook and overview of @DeepMindAI's goals post-#AlphaGo https://t.co/zynd0adfzc #machinelearning
710166063158267900,Wed Mar 16 18:09:18 +0000 2016,Biologists start embracing open-source preprint culture, publish articles on bio version of arXiv https://t.co/M076H9oG80
709896654032347100,Wed Mar 16 00:18:46 +0000 2016,"Have rueful sense of own inevitable obsolence, will travel" - Ken Jenning https://t.co/lDiQ0sCSn6 #AlphaGo #machinelearning
709884846852718600,Tue Mar 15 23:31:51 +0000 2016,RT @_aylien: 10 #DeepLearning terms explained in simple English  https://t.co/g8qgA0ABlh  #MachineLearning #NLP #NLProc https://t.co/aa0x6x‚Ä¶
709653674444918800,Tue Mar 15 08:13:15 +0000 2016,RT @AlecRad: Quick comparison of LR, DNN, CNN with @genekogan visual confusion matrix! https://t.co/H9UflpwjQk
709484605112500200,Mon Mar 14 21:01:26 +0000 2016,Slackbot is not witty enough? Check out this article on how to code a Slack conversational agent #textanalytics  https://t.co/fYlweBCn0k
709053332686508000,Sun Mar 13 16:27:42 +0000 2016,RT @StatsmanBruno: Everyone thinks Lee Sedol won but did you look at the black pieces pattern??? #AlphaGo #AI #Skynet https://t.co/XXzjN5oS‚Ä¶
708821187380559900,Sun Mar 13 01:05:14 +0000 2016,Maximum mutual information objective instead of likelihood leads to more diverse responses by seq2seq dialog systems https://t.co/XzjiuhwfZw
708759132531265500,Sat Mar 12 20:58:39 +0000 2016,@aransena üëçüèª
708755123011452900,Sat Mar 12 20:42:44 +0000 2016,CNNs in continuous feedback loop for more human-like grasping by robots https://t.co/gnPFvU9x0P #DeepLearning
708754738909737000,Sat Mar 12 20:41:12 +0000 2016,@aransena have you read this blog post (+ arxiv paper)? should be of interest to you. https://t.co/gnPFvU9x0P
708703372824731600,Sat Mar 12 17:17:05 +0000 2016,I can't wait until we'll have GANs for generating jokes and we'll have an endless supply of these. #deeplearning https://t.co/R5IVKooQKy
708637189886844900,Sat Mar 12 12:54:06 +0000 2016,This is hilarious and ballsy. Anyone know any shortest abstracts in computer science or #machinelearning? https://t.co/BtHNcvHeJE
708451116732825600,Sat Mar 12 00:34:43 +0000 2016,Great Wired article about the ambivalent feeling wrt AlphaGo's successes, between excitement and sadness https://t.co/6mHrNuQaay #dlearn
707553397952651300,Wed Mar 09 13:07:30 +0000 2016,Inspiring talk by Google director of people operations EMEA Brian Kennan. #StudentTalks16 https://t.co/L87fr6n5iU
705727024866992100,Fri Mar 04 12:10:09 +0000 2016,This is a nice, brief overview of word embeddings in the context of distributional semantics research https://t.co/gDlROnDwp7 #deeplearning
705602715997577200,Fri Mar 04 03:56:11 +0000 2016,Understanding the aesthetics of images using CNNs and a ranking hinge loss https://t.co/oOw9OOkn1r #DeepLearning
705509064294469600,Thu Mar 03 21:44:03 +0000 2016,End-to-end object tracking with RNNs https://t.co/lCDr55DX4z #machinelearning #deeplearning
705306566011002900,Thu Mar 03 08:19:23 +0000 2016,NLP is breaking out via Communications of the ACM https://t.co/NLHzCyIXZN #nlproc #machinlearning
704815091028459500,Tue Mar 01 23:46:27 +0000 2016,Google tells stories from its software engineers at https://t.co/B9XgbRKDyU https://t.co/7ay6I7xnhk #machinelearning
704801793159667700,Tue Mar 01 22:53:36 +0000 2016,Benefits of casting word embedding as a learning to rank problem with WordRank https://t.co/NyaNU0VLKC via @deliprao https://t.co/Wj5nkb5ejR
704618332171976700,Tue Mar 01 10:44:36 +0000 2016,Probably the clearest, most detailed implementation of a #deeplearning research paper I've seen via @iamtrask https://t.co/FWbA1qR85U
704532625420120000,Tue Mar 01 05:04:02 +0000 2016,Autoencoder NNs and XGBoost bring victory in @kaggle Masters Cervical Cancer screening challenge https://t.co/kb0KiBxT99 #machinelearning
704442807331790800,Mon Feb 29 23:07:07 +0000 2016,Excellent review of AlphaGo's win against Fan Hui in light of progress in #AI in general https://t.co/g5o22XH2jM #machinelearning
704433018384072700,Mon Feb 29 22:28:13 +0000 2016,.@egrefen's provocatively titled "How much linguistics in #nlproc?" slides, with details from 2 NIPS 2015 papers https://t.co/0IfEOl67KL
704388859245809700,Mon Feb 29 19:32:45 +0000 2016,RT @_aylien: We collected and analyzed 1.8M tweets from #SuperBowl50 - https://t.co/MJurVW5DzJ #SentimentAnalysis #DataViz https://t.co/NGc‚Ä¶
704383767985651700,Mon Feb 29 19:12:31 +0000 2016,Talk about multilingual multimodal embeddings by Iacer Calixto at #MLDublin hosted by @nitrohq #machinelearning https://t.co/dZxOC5GvNt
704356740331118600,Mon Feb 29 17:25:07 +0000 2016,@egrefen Any chance to tape the talk / make the slides available? Would love to see it. :)
703876237912047600,Sun Feb 28 09:35:47 +0000 2016,Great write-up by @jeanqasaur about the entire journey of her #computerscience PhD, ups and downs, angst, etc. https://t.co/swDQufqVzZ
703554088122912800,Sat Feb 27 12:15:40 +0000 2016,Best. Research. Tumblr. Ever. via @ResearchInP https://t.co/r3w47tbvun #machinelearning #NLProc
703048591087169500,Fri Feb 26 02:47:00 +0000 2016,Excellent article on what it feels like to be on the exponential path to AGI by @waitbutwhy https://t.co/25THRmB8DG #machinelearning
702994987131277300,Thu Feb 25 23:14:00 +0000 2016,Exploring the Intersection of Art and Machine Intelligence. https://t.co/VWKjKO1Ft2 #machinelearning
702948454713184300,Thu Feb 25 20:09:06 +0000 2016,A @reddit irony dataset and paper via @byron_c_wallace https://t.co/902gj6kI5q https://t.co/PsISmwCAks #machinelearning #nlproc
702939527837589500,Thu Feb 25 19:33:38 +0000 2016,Cool article &amp; @TensorFlo implementation of DeepMind's RNN for Image Generation by @ericjang11 https://t.co/q3SePgeKSq #DeepLearning
702939020968501200,Thu Feb 25 19:31:37 +0000 2016,@ericjang11 Hey Eric, nice article and implementation of DRAW! üëç
702855380888657900,Thu Feb 25 13:59:15 +0000 2016,Neural Turing Machines in lasagne via @TristanDeleu https://t.co/anmovHKzXu #DeepLearning #machinelearning
702853588889026600,Thu Feb 25 13:52:08 +0000 2016,Do NNs converge to local optima, saddle points, or something totally different? Aventures in Weight Space. https://t.co/UHg1zF3div #dlearn
702793148158836700,Thu Feb 25 09:51:58 +0000 2016,Generating continuous representations of faces and Atari game frames unsupervisedly. https://t.co/oeBm6YHuDX https://t.co/oeBm6YHuDX #dlearn
702536322460815400,Wed Feb 24 16:51:26 +0000 2016,RT @cocoweixu: EMNLP 2016 website is up https://t.co/J6NtrFeNwQ!  #nlproc #nlp #naacl #naacl2016
702420424576114700,Wed Feb 24 09:10:54 +0000 2016,@richsilvo üëç Cheers! Neural looks cool. Let me know how it's going. :)
702124569394548700,Tue Feb 23 13:35:16 +0000 2016,Lee Sedol vs. AlphaGo press release with details about the upcoming matches, comments, etc. Let the games begin! https://t.co/93EprDL3Al
701898781898444800,Mon Feb 22 22:38:04 +0000 2016,@stephenroller good point and commonly for extractive summarization. Interesting, though more context != better results. Untapped potential.
701824554159763500,Mon Feb 22 17:43:07 +0000 2016,Seq2seq RNNs for text summarization; even only encoding 1st source sentence produces good results https://t.co/OV9PbJ5t00 #DeepLearning
701012970348871700,Sat Feb 20 11:58:10 +0000 2016,WikiTableQuestions, a new dataset for natural language understanding. 21k tables, 22k questions. https://t.co/7QXfpbwlVG #nlproc
700404359243419600,Thu Feb 18 19:39:46 +0000 2016,Heroes and villains of #dlearn at Dublin's first #DeepLearning meetup hosted by @Zalando https://t.co/jPUca0DBjD
699996565868515300,Wed Feb 17 16:39:21 +0000 2016,.@karpathy talk about his RNN research, char-RNNs, and image captioning at the RE‚Ä¢WORK #dlearn summit https://t.co/I5ZCv9mNdt
699926831894569000,Wed Feb 17 12:02:15 +0000 2016,RT @gneubig: Looking forward to doing a tutorial on "Practical Neural Networks for NLP -- ¬≠¬≠ From Theory to Code" at EMNLP with @yoavgo and‚Ä¶
699726084468830200,Tue Feb 16 22:44:33 +0000 2016,NSA's anomaly/terrorist detector ridiculously optimistic, evaluated on unrepresentative population sample https://t.co/DcNSOuOpLj
699291475910271000,Mon Feb 15 17:57:34 +0000 2016,@karangoel üëç
699290038643908600,Mon Feb 15 17:51:52 +0000 2016,@karangoel re that study you can't find: Daniel Kahneman talks about it in Thinking Fast and Slow https://t.co/MKfTpGngzC
698457010401103900,Sat Feb 13 10:41:42 +0000 2016,RT @deliprao: Reviewing Swivel, the new word embedding paper from Google. TLDR: Too many questions. I'm sticking to #word2vec https://t.co/‚Ä¶
697697345987874800,Thu Feb 11 08:23:04 +0000 2016,Lessons learnt by new #1 data scientist on @kaggle: skills, friends, recognition, opportunities, &amp; and collaboration https://t.co/tvyPQNYvpp
697695950396063700,Thu Feb 11 08:17:31 +0000 2016,Swivel - the next generation of word embeddings; from sampling back to count-based methods https://t.co/WECBSsdW7d #DeepLearning
697527089835147300,Wed Feb 10 21:06:32 +0000 2016,Can you guess? Tip: Yoshua Bengio also talked about it on @Quora. https://t.co/siXGGV6SHI #machinelearning  https://t.co/NZnIeTrqHl
697049914879184900,Tue Feb 09 13:30:24 +0000 2016,When #machinelearning metaphors get too elaborate: Indistinguishable Bandits Dueling with Decoys https://t.co/pAOitIVQoi
697024088578138100,Tue Feb 09 11:47:47 +0000 2016,New paper on large-scale language modeling, achieve 30 perplexity (down from 51.3 state-of-the-art), 20x less params https://t.co/hXD8eEmwNL
696802952359649300,Mon Feb 08 21:09:04 +0000 2016,Just came across Michael I. Jordan's @reddit AMA. He is truly the Michael Jordan of #machinelearning. https://t.co/Gj9aMI6gyn
696370918235439100,Sun Feb 07 16:32:19 +0000 2016,Good, brief explanation of LSTM and its motivations vs. RNN in a @reddit comment https://t.co/enotAmIfzp #deeplearning #machinelearning
696308078556377100,Sun Feb 07 12:22:37 +0000 2016,Microsoft has a Chinese chat bot called "Little Bing" that has been talking on Sina Weibo for 18 months https://t.co/AVhBBUtweB
696306197109084200,Sun Feb 07 12:15:08 +0000 2016,Spanish billion word corpus and pre-trained embeddings via @neurotikal https://t.co/m2Ta17oMKc #DeepLearning
696305298462666800,Sun Feb 07 12:11:34 +0000 2016,More vivid neural network dreams with bilateral filters via @mtyka https://t.co/YUc4kPQVsT #DeepLearning
695521606425321500,Fri Feb 05 08:17:27 +0000 2016,RT @demishassabis: Thrilled to officially announce the 5-game challenge match between #AlphaGo and Lee Sedol in Seoul from March 9th-15th f‚Ä¶
695217875586515000,Thu Feb 04 12:10:32 +0000 2016,@nlothian That's a good one. Cheers!
694913323377246200,Wed Feb 03 16:00:21 +0000 2016,I can't believe the richness of annotations in @drfeifei's VisualGenome. Do we have something comparable in #nlproc? https://t.co/UCu32c9DbY
694911028212154400,Wed Feb 03 15:51:14 +0000 2016,@_rockt Great job! Can't wait to read the paper! :)
694911028212154400,Wed Feb 03 15:51:14 +0000 2016,@_rockt Great job! Can't wait to read the paper! :)
